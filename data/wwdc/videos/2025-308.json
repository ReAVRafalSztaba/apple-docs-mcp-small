{
  "id": "308",
  "year": "2025",
  "url": "https://developer.apple.com/videos/play/wwdc2025/308/",
  "title": "Optimize CPU performance with Instruments",
  "speakers": [],
  "duration": "",
  "topics": [
    "Developer Tools",
    "Graphics & Games"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "Hi, I'm Matt and I'm an OS kernel engineer. Today, I'll teach you how to use Instruments to optimize code for Apple silicon CPUs. Using CPU resources efficiently can avoid noticeable wait times when your app needs to process large amounts of data or quickly respond to an interaction. But predicting how software performs is difficult for two reasons.\n\nThe first is the layers of abstraction between your Swift source code and what actually ends up running.\n\nThe source code you write for your app is compiled into machine instructions that eventually execute on a CPU.\n\nBut your code doesn't run in isolation: it's augmented by compiler-generated support code, the Swift runtime, and other system frameworks, all of which may invoke kernel system calls to handle privileged operations on behalf of your app.\n\nThis makes it hard to know the cost of software abstractions your code relies on.\n\nThe second reason that your code's performance is difficult to predict is how the CPU carries out the instructions it's given. Within a single CPU, its functional units work in parallel to execute instructions efficiently.\n\nTo support this, instructions will be executed out-of-order, only giving the appearance of in-order execution. And CPUs also benefit from several layers of memory caches that ensure quick access to data. These characteristics dramatically accelerate common coding patterns, such as linear scans through memory or defensive checks like early exits for rare conditions.\n\nBut some data structures, algorithms, or implementation approaches are difficult for the CPU to execute efficiently without careful optimization or even significant restructuring. We're going to cover the right path to optimizing your code for the CPU. We'll start by reviewing how to approach performance investigations, guided by data to focus on the biggest potential speed-ups first. Then, we'll consider the traditional profiling approaches that are a great first step in identifying excessive CPU usage in your code. To dive deeper and fill in the gaps from profiling, we'll use Processor Trace to record every instruction and measure the costs of software abstractions. And finally, we'll use the improved CPU Counters instrument to analyze CPU bottlenecks and understand how to micro-optimize our algorithm. Let's get started by putting ourselves in the right mindset to approach performance investigations. The first step is to keep an open mind: sources of slowdowns can be surprising and unexpected. Collect data to test assumptions and verify that your mental model for how the code executes is accurate.\n\nFor instance, consider other causes of slowdowns in addition to single-threaded CPU performance. Aside from executing on a CPU, threads and tasks can also spend time blocked, waiting on resources like files or access to shared mutable state.\n\nThe \"Visualize and optimize Swift concurrency\" session covers tools for understanding why your tasks might be off-CPU.\n\nWhen your threads are unblocked and on-CPU, it's possible there's an API that's being misused, like applying the wrong quality of service class to your code or implicitly creating too many threads. Read the documentation on Tuning your code's performance for more details. But if it's efficiency that's the problem, you either need to change the algorithm and its associated data structures or the implementation, which is how the algorithm is expressed in the programming language. Use tools to determine which branches of this tree you should focus on first. Try Xcode's built-in CPU Gauge to detect if CPUs are heavily used while interacting with your app. To analyze the blocking behaviors between threads and which threads will ultimately unblock them, use the System Trace instrument.\n\nAnd for issues impacting the UI or your app's main thread, use the specialized Hangs instrument. The Analyzing hangs with Instruments session contains more details on how to confirm that your app's CPU usage needs to be optimized. But even with guidance from tools, be careful about the kinds of optimizations you implement. Ruthless micro-optimization can make your code more difficult to extend and reason about. And it often relies on compiler optimizations that can be fragile, like auto-vectorization or reference count elision. Before you commit to intrusive micro-optimizations, seek out alternatives that might avoid the slow operations altogether. Consider why the code is running in the first place. Maybe you can avoid doing the work at all and just delete the code.\n\nThanks for watching this session and... just kidding. But seriously, This usually is impossible, but it's a good check on your assumptions about how important the results of the work are.\n\nYou could also try to do the work later outside of the critical path, or only when the results will be visible to someone.\n\nIn the same vein, precomputing values can also hide the time it takes to complete the work. This might even involve baking in values at build time. However, these approaches could consume power unnecessarily or increase the download size of your app. For repeated operations with the same inputs, caching is another solution, but one that often comes with its own set of hard problems, like cache invalidation or dealing with increased memory usage. Once you've exhausted attempts to avoid doing the work when its performance is noticeable, you need the CPU to do the work faster. That's what we'll be focusing on today. Prioritize your optimization efforts on code that will have the largest impact on user experience. Usually, this is code involved in the critical path of someone interacting with your app, where they'll notice performance issues, but can also be longer-running operations that consume power. For this session, we'll focus on searching through prepared lists of integers because it's on my app's critical path.\n\nMy app is already using binary search, a classic algorithm that uses a sorted array to find an element by successively halving the search space. In this example, there are 16 elements in the array and we're searching for the element holding the number 5.\n\n5 is less than the element in the middle of the array, 20, so its element must be in the first half.\n\n5 is also smaller than the element in the middle of the first half, 9, so it must be in the first quarter of the array. We narrow in on the matching element after comparing with 3, in only 4 steps.\n\nThis is the binary search implementation from a framework used by my app. It's a standalone function that uses the metaphor of finding a needle in a haystack to name its parameters. It supports searching for a Comparable needle through the haystack Collection. The algorithm tracks two variables: the start of our current search area in start and the number of elements remaining to search in length. While there's remaining elements to search, it checks the middle value of the search space. If the needle is less than the value, then it just halves the search space, leaving start intact. If the needle is equal, then the element has been found and the middle index is returned. Otherwise, the start position needs to be adjusted to just after the middle element and the search space reduced by half.\n\nWe'll prepare to optimize this algorithm incrementally, confirming at each step that we're making progress by comparing search throughput, or the number of searches the algorithm can complete each second. Don't feel limited to only taking huge leaps forward every time you make a change: some optimizations might be hard to quantify, but small improvements will add up over time.\n\nTo help us optimize continuously, I've written an automated test to measure search throughput. We don't need a particularly robust setup: we just want an estimate of performance. This repeat-while loop calls the search closure until the specified duration has elapsed.\n\nI'm using an OS signpost interval around the calls to the search closure so tools can narrow in on the portion of the test we're optimizing. And I chose the points of interest category since Instruments includes it by default.\n\nThe timing itself uses a ContinuousClock, which unlike Date, can't go backwards and has low-overhead. It's a simple, but effective approach to gathering some rough data on the performance of the algorithm.\n\nMy test is called searchCollection and simulates how my app uses binary search. We'll run the searches for one second with a descriptive name for the signposts, in case we run multiple tests in a single recording.\n\nA loop inside the closure will invoke the binary search function to amortize the cost of checking the time.\n\nLet's run this test under an Instruments profiler to analyze the CPU performance of binary search. There are two CPU-focused profilers to choose from: the Time Profiler and the CPU Profiler.\n\nThe classic Time Profiler instrument periodically samples what's running on the system's CPUs, based on a timer.\n\nIn this example, we have some work happening on two CPUs. At each sample point, Time Profiler captures the user space call stacks of each thread running on the CPU.\n\nInstruments can then visualize those samples as a call tree or flame graph in its detail view to give an approximation of what code is important to optimize for CPU performance. This is helpful for analyzing how work is distributed over time or which threads are active at the same time. But using a timer to sample call stacks suffers from a problem called aliasing. Aliasing is when some periodic work on the system happens at the same cadence as the sampling timer. Here, the blue regions are responsible for most of the CPU time, but the orange functions happen to be running whenever the sampler collects a call stack. This makes orange unfairly over-represented in the Instruments call tree.\n\nTo avoid this problem, we can switch to the CPU Profiler.\n\nIt samples CPUs independently, based on the clock frequency of each CPU. You should prefer CPU Profiler over Time Profiler for CPU optimization because it's more accurate and more fairly weights software consuming CPU resources.\n\nThese bells represent when a CPU's cycle counter samples the running call stack. Apple silicon CPUs are asymmetric and some of them will run at a slower, but more power-efficient clock frequency than others. Individual CPUs that scale up their frequency will be sampled more often, without Time Profiler's bias against faster-running CPUs.\n\nWe'll use CPU Profiler to find out which parts of my binary search function consume the most CPU cycles.\n\nIn Xcode's test navigator you can quickly launch Instruments from a unit test by secondary-clicking on the test name and selecting its Profile item.\n\nIn this case, we'll select Profile searchCollection.\n\nThis opens Instruments and presents the template chooser. I'll choose CPU Profiler.\n\nIn the recorder settings, we'll switch to deferred mode for lower overhead and start recording. The default immediate mode for profilers can be useful to confirm your interactions with an app are captured. But for an automated test on the same machine as Instruments, we want to minimize any overhead that might be added by our tools by waiting until the recording stops to analyze it.\n\nNew documents in Instruments are often daunting. The window is divided into two halves. The top holds the tracks that show activity on the timeline. Each track can contain multiple lanes with charts to indicate levels or regions.\n\nBelow the timeline is the detail view with summary information about the timeline range being inspected. Any extended details are shown on the right side.\n\nTo get oriented, we'll find the region where searches are happening in the Points of Interest track.\n\nSecondary-clicking on the region will offer to set the inspection range, limiting the detail view below to only the data captured in the signpost interval. We can click on the test runner process track to show the CPU profile in the detail view, below the timeline. This view shows a call tree of the functions in the test that were sampled by each CPU's cycle counter. Holding down option and clicking on the chevron next to the first function listed will expand the tree until the first point where the sample counts diverge significantly, which is close to our binary search function. We'll focus on our binary search function by clicking the arrow next to its name and selecting the Focus on Subtree item.\n\nEach function is weighted by the sample count multiplied by the number of cycles between each sample. This call tree is showing a lot of samples taken in functions called by binary search to deal with the Collection type. This protocol witness is showing up in about a quarter of our samples. There are allocations and even Array's checks for Objective-C types. We can avoid the overheads of Array and the generics if we switch to a container type that better matches the kind of data we're searching through. Let's try the new Span type. Span can be used instead of Collection when elements are stored contiguously in memory, which is common for many kinds of data structures. It's effectively a base address and count. But it also prevents escaping, or leaking, the memory reference outside of the functions it's used in. For more details on Span, watch the Improve memory usage and performance with Swift session.\n\nAdopting Span only requires changing the haystack and return types to Span, the algorithm itself is unchanged.\n\nThis small change makes searching four times faster. But this version of binary search is still impacting my app and I'd like to investigate whether Span's bounds checking is contributing to the overhead. To dig in deeper, we'll switch to a new tool called Processor Trace. Starting in Instruments 16.3, Processor Trace can collect a complete trace of all the instructions your app's process executes in user space. This is a fundamental shift in how you can measure software performance: there's no sampling bias and only a negligible 1% impact to your app's performance. Processor Trace requires specialized CPU features that are only available on Mac and iPad Pro with M4 or iPhone with A18.\n\nBefore we jump in, we'll need to set up our device for processor tracing. On a Mac, turn on the setting under Privacy & Security and Developer Tools. For iPhone or iPad, the setting is located in the Developer section. For the best experience with Processor Trace, try to limit your tracing to only a few seconds. Unlike sampling with CPU Profiler, you don't need to batch up your work: even a single instance of the code you want to optimize can be enough. Let's run Processor Trace on the Span version of binary search. Our test now only needs a handful of iterations. To profile this test, I'll secondary-click the test icon in the line number gutter.\n\nThis shows the same menu as the one we used before, but can be more convenient than switching navigators.\n\nI'll select the Processor Trace template and start recording.\n\nProcessor Trace has to deal with a lot of data, so capturing and analyzing it can take some time.\n\nProcessor Trace sets up the CPU to record every branching decision. The cycle count and the current time are also recorded to track how long the CPU spends in each function. Instruments then uses your app's and system framework's executable binaries to reconstruct an execution path and annotate function calls with the cycles elapsed and time durations. We limit the time spent tracing because even though the CPU is recording as little information as possible, it can still be gigabytes of data per second for a multi-threaded application. Now that the document is ready, let's zoom in so we can inspect our binary search function calls.\n\nSearching now only takes up a tiny fraction of the full recording, so we'll find it in the Regions of Interest List in the details view below the timeline and then secondary-click its row and select Set Inspection Range and Zoom. To locate the thread that's executing binary search, we'll secondary-click the Start Thread cell and select Pin Thread in Timeline.\n\nProcessor Trace adds a new function call flame graph to each thread track, so I'll drag the pin's divider up to make space for it.\n\nProcessor Trace shows execution visually as a flame graph. A flame graph is a graphical representation of function costs and relationships: the width of the bars represents how much time the function took to execute and the rows represent nested call stacks. But most flame graphs show data from sampling and their cost is only an estimate based on a sample count.\n\nProcessor Trace's timeline flame graph is different: it shows the calls made over time exactly as they would have been executed on the CPU. The colors of each bar represent the kinds of binaries they're from: brown for system frameworks, magenta for the Swift runtime and standard library, and blue for code compiled into your app's binary or any custom frameworks. The first part of this trace is showing the overhead of emitting the signpost, so let's zoom in further on the binary search code near the end of the range. I'll hold down option, click, and drag across the timeline to zoom in.\n\nI can select any binary search function call from the 10 iterations and set the inspection range and zoom with a secondary click. This is the power of Processor Trace we can see all the calls made by a single function that only runs for a few hundred nanoseconds. We could zoom in even further, but let's use the Function Calls summary below the timeline. This shows the same information as the timeline, but as a table, with the full names of functions being called for short periods of time. I'll sort this table by cycles.\n\nMy initial assumption that bounds checks were causing slowdowns was wrong. This implementation of binary search is still dealing with protocol metadata overheads and can't inline the number comparisons, which ends up being a sizable percentage of the search's total cycle count.\n\nThis is because the generic Comparable parameter is not specialized for the type of elements being used.\n\nSince my code is in a framework linked by my app, the Swift compiler cannot produce a specialized version of binary search, just for the types passed by callers.\n\nWhen this causes overheads in code from a framework, you should add the inlinable annotation to the framework's function to generate specialized implementations within the framework client's binary executables.\n\nBut inlining can make code harder to analyze because it gets mixed with any callers. I'd like to avoid inlining into the test harness, so for this function, I'll just manually specialize it for the Int type used by the app and test, with a new function name.\n\nThe code loses a lot of generality while becoming about 1.7 times faster. We need to keep optimizing because binary search is still contributing to slowdowns in the app. It's pretty odd to spend so much time on optimizing a single function you'll periodically reassess and collect more data to notice a different part of your code responsible for the inefficiency. Our specialized Span binary search isn't showing any unexpected function calls in the Processor Trace either, so we should understand how the code is running on the CPU to make further progress.\n\nWith the CPU Counters instrument, we can find out what bottlenecks the code is hitting when running on CPU.\n\nBefore we start using Instruments again, we need to build a mental model of how a CPU works.\n\nAt a basic level, a CPU is just following a list of instructions, modifying registers and memory, and interacting with peripheral devices.\n\nWhen a CPU is executing, it has to follow a series of steps, which are broadly categorized into two phases. The first is Instruction Delivery to ensure the CPU has instructions to execute. Then, Instruction Processing is responsible for executing them.\n\nIn Instruction Delivery, instructions are fetched and decoded into micro-operations that are easier for the CPU to execute. Most instructions decode into a single micro-operation, but some instructions do multiple things, like issuing a memory request and incrementing an index value.\n\nTo process a micro-operation, it's sent to the map and schedule units, which route and dispatch the operation. From there, the operation is assigned to an execution unit, or the load-store unit if the operation needs to access memory.\n\nIt would be pretty slow if the CPU had to run these phases serially before it can start to fetch again, so Apple silicon processors are pipelined.\n\nOnce one unit is finished with work, it can move onto the next operation, keeping each unit busy.\n\nPipelining and making extra copies of the execution units supports instruction-level parallelism.\n\nIt's different from process or thread-level parallelism that you would access with Swift Concurrency or Grand Central Dispatch, where multiple CPUs execute different operating system threads. Instruction-level parallelism allows a single CPU to take advantage of times when units might otherwise be idle and use hardware resources efficiently, keeping all parts of the pipeline busy. Your Swift source code is not directly in control of this parallelism, but instead must help the compiler generate an amenable sequence of instructions.\n\nUnfortunately parallelizable instruction sequences are not immediately intuitive due to the interaction between units in a CPU. Every arrow between the units shows where operations can stall in the pipeline, limiting the available parallelism. We call these bottlenecks.\n\nTo find out which bottleneck is relevant to our workload, Apple silicon CPUs can count interesting events in each unit and other characteristics of the instructions being executed. The CPU Counters instrument reads these counters to build higher-level metrics out of them. This year, we've added preset modes for these counters to make them much easier to use. Instruments uses them in a guided, iterative methodology for analyzing the performance of your code, called bottleneck analysis. Let's use it to find out why our binary search is still slow, despite not having any apparent function call overheads.\n\nThe CPU Counters instrument relies on sampling the workload, so we need to return to the test harness we used with CPU Profiler to measure throughput again.\n\nLet's profile a test of the specialized Span implementation with Instruments.\n\nWe'll select the CPU Counters template.\n\nThere's now a guided configuration with curated modes to measure with.\n\nIf you're curious about what each mode does, there's documentation available by clicking on the info icon next to the mode selection. Let's get counting.\n\nThis initial CPU Bottlenecks mode breaks up the work done by the CPU into four broad categories that account for all of the CPU's potential performance. Instruments shows these categories as a colored stacked bar chart and a summary table in the detail view. During recording, Instruments will collect CPU counter data for the threads used in our test and convert them into bottleneck percentages. We'll use the Points of Interest to orient ourselves, like before, to zoom and select our searches.\n\nWe'll then pin the thread that is running our binary search implementation to the timeline.\n\nHovering over its CPU Bottleneck lane shows a high percentage in the discarded bottleneck.\n\nThe details view below shows the metric aggregations in the inspection range. Selecting the Discarded Bottleneck row shows a description in the extended detail view on the right.\n\nInstruments also presents a remark above the chart in the timeline. Clicking that remark shows more details below.\n\nThis is useful, but I still don't know what part of search is responsible for the bottleneck. Secondary-clicking the Discarded Sampling cell under the Suggested Next Mode column offers the option to profile the workload again with a different mode. Let's try it out. This mode is a little different from CPU Bottlenecks. It's still gathering counter data, but it's also setting up the counters to trigger sampling. The sample data is just limited to the instruction that's generating discarded work. To show those, let's orient ourselves again with Points of Interest.\n\nThen, we'll select the test process track and navigate to Instruction Samples below the timeline.\n\nThis isn't a call stack, but the exact instruction that's causing the problem. We can open the Source Viewer by clicking on the arrow next to the function name to show the source code that was sampled because the CPU followed the wrong branch direction. Here, the comparisons being done between the needle and the middle value are incorrectly predicted. To understand why these source lines are responsible for so many bad predictions, we need to learn a bit more about CPUs.\n\nCPUs are sneaky and execute instructions out-of-order. It only appears like instructions execute sequentially thanks to an extra re-ordering step as instructions complete.\n\nThis means CPUs look ahead and make predictions about which instructions will execute next. The branch predictors responsible are usually accurate, but can take wrong paths when there's no consistent pattern from prior execution about whether a branch will be taken or not.\n\nThe loop in our binary search algorithm has two kinds of branches. The first loop condition is usually taken until the end of the loop, so is well-predicted and didn't show up in sampling. But the check for the needle is effectively a random branch, so it's no wonder the predictors have trouble with it.\n\nI've rewritten the loop body to avoid difficult to predict branches that affect control flow. The if statement's body is only assigning a value based on the condition. This allows the Swift compiler to generate a conditional move instruction, and avoid branching to a different instruction.\n\nReturning from a function or breaking a loop based on a condition must be implemented with a branch, so I also had to remove the early return.\n\nI've used unchecked arithmetic to avoid branches that would terminate the program. This is one of the areas where micro-optimization becomes fragile and easy to disrupt, not to mention less safe and understandable. When we make a change like this, you should go back to the initial CPU Bottlenecks mode to check how it impacts the rest of the bottlenecks.\n\nI've already collected a trace of our new, branchless binary search, which is now about twice as fast as the version with branches. It's now almost completely bottlenecked on Instruction Processing.\n\nInstruments indicates that we should re-run the workload with the Instruction Processing mode.\n\nThat mode had remarks that recommended running the L1D Cache Miss Sampling mode. The cache miss samples show that accessing memory from the array is the reason the CPU isn't able to execute instructions efficiently.\n\nLet's learn more about CPUs and memory to find out why.\n\nCPUs access memory through a hierarchy of caches that make repeated accesses to the same address, or even predictable access patterns, much faster.\n\nIt starts with the L1 caches that are located within each CPU. These can't store much data, but offer the fastest access to memory. A slower L2 cache sits outside of the CPUs and provides a lot more headroom. And finally, requests that miss both caches and need to access main memory become 50 times slower than the fast path.\n\nThese caches also group memory into 64- or 128 byte segments called cache lines: even if an instruction only requests 4 bytes, the caches will pull in more data with the expectation that subsequent instructions will need to access other bytes nearby.\n\nLet's consider how this impacts our binary search algorithm. In this example, the blue lines are elements in the array, while the grey capsules are the cache lines that the CPU caches operate on.\n\nThe array starts completely out of the cache. The first comparison brings in a cache line and several elements into the L1 data cache. But the next comparison experiences a cache miss. And subsequent iterations keep missing in the cache. This continues until the search narrows in on a cache-line sized region. Binary search turns out to be a pathological case for the CPU's memory hierarchy.\n\nBut if we can tolerate reordering the elements to be more friendly to the cache, then we can put the search points on the same cache line. This is called an Eytzinger layout, after a 16th century Austrian genealogist who organized family trees this way.\n\nThis isn't a general optimization we can make without significant consequences. This improves search speed at the cost of in-order traversal speed, making that operation now miss in the cache. Let's go back to the first example of binary search to show how to re-arrange a sorted array to an Eytzinger layout. Starting with the middle element as the root, we'll model the binary search operation as a tree, where midpoints are descendant nodes. An Eytzinger layout is arranged as the breadth-first traversal of that tree.\n\nThe elements closer to the root of the tree are arranged more densely, and are more likely to share cache lines. Searching for 5 again now spends the first three steps in the same cache line. The leaf nodes are sorted at the end of the array, which will incur an unavoidable cache miss.\n\nI've recorded a CPU Bottlenecks trace of the Eytzinger binary search, which shows it's two times faster again than the branchless search.\n\nBut this example highlights something interesting, it's still technically bottlenecked on Instruction Processing.\n\nWe've made our implementation more cache-friendly, but the workload is still inherently memory-bound.\n\nYou should monitor performance to know when to stop and optimize other code in your app, because our search is now no longer impacting critical path performance.\n\nDuring this process, we've significantly improved search throughput. First, with CPU Profiler, we were able to get a significant speedup when switching from Collection to Span.\n\nThen, Processor Trace showed us the overheads of unspecialized generics.\n\nAnd finally, we dialed up the performance significantly with some micro-optimizations, guided by Bottleneck Analysis.\n\nOverall, we made our search function about 25 times faster with Instruments. To achieve these speedups, we started in the correct mindset, using tools to confirm our guesses and develop an intuition about the cost of abstractions. We successively applied more detailed tools to find unexpected overheads. These were as easy to address as they are to overlook if you don't actually measure. And then, once software overheads were addressed, we looked at CPU bottleneck-focused optimizations. We became more aware of and maybe even sympathetic to features of the CPU that are taken for granted.\n\nThis order was important: we have to ensure the CPU-focused tools aren't confused by extra software runtime overheads.\n\nTo apply this to your own apps, collect data and follow leads with the performance mindset. Write performance tests so you can repeatedly measure with these Instruments. Provide feedback or ask questions about using the tools on the forums. Watch the sessions I mentioned earlier and the WWDC24 session on Swift performance, which will help you build a more accurate mental model of the costs of Swift's powerful abstractions. And to better understand how CPUs execute your code, read the Apple Silicon CPU Optimization Guide.\n\nThanks for watching and I hope you have fun using Instruments to find optimization needles in your code's haystacks.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "6:37",
      "title": "Binary search in Collection",
      "language": "swift",
      "code": "public func binarySearch<E, C>(\n    needle: E,\n    haystack: C\n) -> C.Index where E: Comparable, C: Collection<E> {\n    var start = haystack.startIndex\n    var length = haystack.count\n\n    while length > 0 {\n        let half = length / 2\n        let middle = haystack.index(start, offsetBy: half)\n        let middleValue = haystack[middle]\n        if needle < middleValue {\n            length = half\n        } else if needle == middleValue {\n            return middle\n        } else {\n            start = haystack.index(after: middle)\n            length -= half + 1\n        }\n    }\n\n    return start\n}"
    },
    {
      "timestamp": "7:49",
      "title": "Throughput benchmark",
      "language": "swift",
      "code": "import Testing\nimport OSLog\n\nlet signposter = OSSignposter(\n    subsystem: \"com.example.apple-samplecode.MyBinarySearch\",\n    category: .pointsOfInterest\n)\n\nfunc search(\n    name: StaticString,\n    duration: Duration,\n    _ search: () -> Void\n) {\n    var now = ContinuousClock.now\n    var outerIterations = 0\n    \n    let interval = signposter.beginInterval(name)\n    let start = ContinuousClock.now\n    repeat {\n        search()\n        outerIterations += 1\n        now = .now\n    } while (start.duration(to: now) < duration)\n    let elapsed = start.duration(to: now)\n    let seconds = Double(elapsed.components.seconds) +\n            Double(elapsed.components.attoseconds) / 1e18\n    let throughput = Double(outerIterations) / seconds\n    signposter.endInterval(name, interval, \"\\(throughput) ops/s\")\n    print(\"\\(name): \\(throughput) ops/s\")\n}\n\nlet arraySize = 8 << 20\nlet arrayCount = arraySize / MemoryLayout<Int>.size\nlet searchCount = 10_000\n\nstruct MyBinarySearchTests {\n    let sortedArray: [Int]\n    let randomElements: [Int]\n    \n    init() {\n        let sortedArray: [Int] = (0..<arrayCount).map { _ in\n                .random(in: 0..<arrayCount)\n        }.sorted()\n        self.randomElements = (0..<searchCount).map { _ in\n            sortedArray.randomElement()!\n        }\n        self.sortedArray = sortedArray\n    }\n\n    @Test func searchCollection() throws {\n        search(name: \"Collection\", duration: .seconds(1)) {\n            for element in randomElements {\n                _ = binarySearch(needle: element, haystack: sortedArray)\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "13:46",
      "title": "Binary search in Span",
      "language": "swift",
      "code": "public func binarySearch<E: Comparable>(\n    needle: E,\n    haystack: Span<E>\n) -> Span<E>.Index {\n    var start = haystack.indices.startIndex\n    var length = haystack.count\n\n    while length > 0 {\n        let half = length / 2\n        let middle = haystack.indices.index(start, offsetBy: half)\n        let middleValue = haystack[middle]\n        if needle < middleValue {\n            length = half\n        } else if needle == middleValue {\n            return middle\n        } else {\n            start = haystack.indices.index(after: middle)\n            length -= half + 1\n        }\n    }\n\n    return start\n}"
    },
    {
      "timestamp": "15:09",
      "title": "Throughput benchmark for binary search in Span",
      "language": "swift",
      "code": "extension MyBinarySearchTests {\n    @Test func searchSpan() throws {\n        let span = sortedArray.span\n        search(name: \"Span\", duration: .seconds(1)) {\n            for element in randomElements {\n                _ = binarySearch(needle: element, haystack: span)\n            }\n        }\n    }\n\n    @Test func searchSpanForProcessorTrace() throws {\n        let span = sortedArray.span\n        signposter.withIntervalSignpost(\"Span\") {\n            for element in randomElements[0..<10] {\n                _ = binarySearch(needle: element, haystack: span)\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "19:17",
      "title": "Binary search in Span",
      "language": "swift",
      "code": "public func binarySearchInt(\n    needle: Int,\n    haystack: Span<Int>\n) -> Span<Int>.Index {\n    var start = haystack.indices.startIndex\n    var length = haystack.count\n\n    while length > 0 {\n        let half = length / 2\n        let middle = haystack.indices.index(start, offsetBy: half)\n        let middleValue = haystack[middle]\n        if needle < middleValue {\n            length = half\n        } else if needle == middleValue {\n            return middle\n        } else {\n            start = haystack.indices.index(after: middle)\n            length -= half + 1\n        }\n    }\n    return start\n}"
    },
    {
      "timestamp": "23:04",
      "title": "Throughput benchmark for binary search in Span",
      "language": "swift",
      "code": "extension MyBinarySearchTests {\n    @Test func searchSpanInt() throws {\n        let span = sortedArray.span\n        search(name: \"Span<Int>\", duration: .seconds(1)) {\n            for element in randomElements {\n                _ = binarySearchInt(needle: element, haystack: span)\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "26:34",
      "title": "Branchless binary search",
      "language": "swift",
      "code": "public func binarySearchBranchless(\n    needle: Int,\n    haystack: Span<Int>\n) -> Span<Int>.Index {\n    var start = haystack.indices.startIndex\n    var length = haystack.count\n\n    while length > 0 {\n        let remainder = length % 2\n        length /= 2\n        let middle = start &+ length\n        let middleValue = haystack[middle]\n        if needle > middleValue {\n            start = middle &+ remainder\n        }\n    }\n\n    return start\n}"
    },
    {
      "timestamp": "27:20",
      "title": "Throughput benchmark for branchless binary search",
      "language": "swift",
      "code": "extension MyBinarySearchTests {\n    @Test func searchBranchless() throws {\n        let span = sortedArray.span\n        search(name: \"Branchless\", duration: .seconds(1)) {\n            for element in randomElements {\n                _ = binarySearchBranchless(needle: element, haystack: span)\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "29:27",
      "title": "Eytzinger binary search",
      "language": "swift",
      "code": "public func binarySearchEytzinger(\n    needle: Int,\n    haystack: Span<Int>\n) -> Span<Int>.Index {\n    var start = haystack.indices.startIndex.advanced(by: 1)\n    let length = haystack.count\n\n    while start < length {\n        let value = haystack[start]\n        start *= 2\n        if value < needle {\n            start += 1\n        }\n    }\n    \n    return start >> ((~start).trailingZeroBitCount + 1)\n}"
    },
    {
      "timestamp": "30:34",
      "title": "Throughput benchmark for Eytzinger binary search",
      "language": "swift",
      "code": "struct MyBinarySearchEytzingerTests {\n    let eytzingerArray: [Int]\n    let randomElements: [Int]\n\n    static func reorderEytzinger(_ input: [Int], array: inout [Int], sourceIndex: Int, resultIndex: Int) -> Int {\n        var sourceIndex = sourceIndex\n        if resultIndex < array.count {\n            sourceIndex = reorderEytzinger(input, array: &array, sourceIndex: sourceIndex, resultIndex: 2 * resultIndex)\n            array[resultIndex] = input[sourceIndex]\n            sourceIndex = reorderEytzinger(input, array: &array, sourceIndex: sourceIndex + 1, resultIndex: 2 * resultIndex + 1)\n        }\n        return sourceIndex\n    }\n\n    init() {\n        let sortedArray: [Int] = (0..<arrayCount).map { _ in\n            .random(in: 0..<arrayCount)\n        }.sorted()\n        var eytzingerArray: [Int] = Array(repeating: 0, count: arrayCount + 1)\n        _ = Self.reorderEytzinger(sortedArray, array: &eytzingerArray, sourceIndex: 0, resultIndex: 1)\n        self.randomElements = (0..<searchCount).map { _ in\n            sortedArray.randomElement()!\n        }\n        self.eytzingerArray = eytzingerArray\n    }\n\n    @Test func searchEytzinger() throws {\n        let span = eytzingerArray.span\n        search(name: \"Eytzinger\", duration: .seconds(1)) {\n            for element in randomElements {\n                _ = binarySearchEytzinger(needle: element, haystack: span)\n            }\n        }\n    }\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Analyzing CPU usage with the Processor Trace instrument",
        "url": "https://developer.apple.com/documentation/Xcode/analyzing-cpu-usage-with-processor-trace"
      },
      {
        "title": "Apple Silicon CPU Optimization Guide Version 4",
        "url": "https://developer.apple.com/documentation/Apple-Silicon/cpu-optimization-guide"
      },
      {
        "title": "Performance and metrics",
        "url": "https://developer.apple.com/documentation/Xcode/performance-and-metrics"
      },
      {
        "title": "Tuning your code’s performance for Apple silicon",
        "url": "https://developer.apple.com/documentation/Apple-Silicon/tuning-your-code-s-performance-for-apple-silicon"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/308/4/5c144645-dea8-4f16-97ac-a6dd76cf72d8/downloads/wwdc2025-308_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/308/4/5c144645-dea8-4f16-97ac-a6dd76cf72d8/downloads/wwdc2025-308_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "312",
      "year": "2025",
      "title": "Improve memory usage and performance with Swift",
      "url": "https://developer.apple.com/videos/play/wwdc2025/312"
    },
    {
      "id": "306",
      "year": "2025",
      "title": "Optimize SwiftUI performance with Instruments",
      "url": "https://developer.apple.com/videos/play/wwdc2025/306"
    },
    {
      "id": "226",
      "year": "2025",
      "title": "Profile and optimize power usage in your app",
      "url": "https://developer.apple.com/videos/play/wwdc2025/226"
    },
    {
      "id": "10217",
      "year": "2024",
      "title": "Explore Swift performance",
      "url": "https://developer.apple.com/videos/play/wwdc2024/10217"
    },
    {
      "id": "10248",
      "year": "2023",
      "title": "Analyze hangs with Instruments",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10248"
    },
    {
      "id": "110350",
      "year": "2022",
      "title": "Visualize and optimize Swift concurrency",
      "url": "https://developer.apple.com/videos/play/wwdc2022/110350"
    }
  ],
  "extractedAt": "2025-07-18T09:39:37.452Z"
}