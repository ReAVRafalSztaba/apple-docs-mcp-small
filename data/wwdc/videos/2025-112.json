{
  "id": "112",
  "year": "2025",
  "url": "https://developer.apple.com/videos/play/wwdc2025/112/",
  "title": "Platforms State of the Union (ASL)",
  "speakers": [],
  "duration": "",
  "topics": [
    "Essentials"
  ],
  "hasTranscript": true,
  "hasCode": false,
  "transcript": {
    "fullText": "Welcome to the 2025 Platform State of the Union. This is a huge year for our platforms. And today, we'll dive into some of the exciting new features, APIs, and technologies for your apps and games.\n\nAll of our work this year builds on top of Apple's powerful platforms. Platforms that bring your imaginative, creative, and ambitious ideas to life. And what makes that possible is the unique fusion of Apple's hardware, software, and services with your apps.\n\nAt the core is Apple silicon, which provides unprecedented power and efficiency across all our platforms.\n\nWith unified memory that maximizes performance, specialized accelerators for video and machine learning, and a secure enclave that guarantees the system's integrity.\n\nAnd all of this provides a strong foundation for our software technologies to build upon, like Metal, with low-level APIs that unlock the hardware's full potential.\n\nApple Intelligence, with powerful generative models and privacy built in from the ground up.\n\nSwift, delivering performance and safety across the system, SDK, and apps. And SwiftUI, enabling rich interactivity, animations, and adaptive design.\n\nAnd because our platforms are deeply infused with technologies like privacy, accessibility, and internationalization, your apps can benefit from them too. So the broadest range of users can access your apps in the ways that work best for them.\n\nThe potential for this builds even further with higher level APIs that help your apps expand across devices.\n\nUsing iCloud and CloudKit, your app's data automatically syncs from device to device, all while leveraging advanced privacy and security guarantees.\n\nWith widgets, your app's notification and content are intelligently surfaced at the right place and the right time. This also works for Live Activities, so your users have immediate access to your app's actions and data.\n\nAnd you can add support for new platforms like visionOS with the same frameworks and concepts.\n\nWhen your apps take advantage of Apple platforms, the experience becomes more than the sum of its parts.\n\nAll of this is an amazing opportunity, and it's getting even bigger and better this year as we dive into how you can bring vitality to your apps with the new design and Liquid Glass materials, deliver new possibilities for your users with Apple Intelligence and machine learning, transform the way you write apps with generative intelligence in Xcode and updates to Swift, and further elevate your app's performance and capabilities with enhancements to frameworks like SwiftUI and Metal. Let's start with our UI design built with Liquid Glass. Here's Billy to tell you more.\n\nThis year introduces our broadest design update ever and begins the next era of our software design. Transformations of this magnitude are rare. When they do occur, they mark inflection points, moments that redefine what's possible. And that's certainly been true for us. The new design beautifully scales across Apple's apps and platforms, all while maintaining the iconic experience users rely on every day.\n\nImportantly, it gives you the perfect opportunity to refresh and modernize elements of your app's design, making them more delightful than ever before. So today we're gonna walk you through our design thinking and high-level design concepts that you should consider, updates to our developer API for building your apps, and updates to how you express your app’s look through beautiful new icons.\n\nSoftware is the heart and soul of our products. We didn't approach such an ambitious design lightly. We had several goals behind the new design. First, we want to elevate the content users care about most. UI is in service of the experience. It should be easily accessible when needed and elegantly recede to the background when not, making users’ experiences of reading, creating, and watching content as immersive and impactful as possible. Secondly, we want to establish greater consistency, harmony, and usability by creating a universal design language, one that offers a seamless experience as your app's users move across platforms while maintaining the distinct qualities that make each unique. And finally, with the incredible technological advances of our hardware, we can craft interaction design in ways that simply weren't possible before today.\n\nThe new design is built around dynamism, expressiveness, and a focus on user experiences that are joyful and delightful. So let's start with the foundational element of the new design, Liquid Glass.\n\nLiquid Glass enables an entirely new level of depth and vitality by combining the optical qualities of glass with responsive fluidity. It's designed to refract content from below it, reflect light from around it, and have responsive lensing along its edges. As you interact with the glass, it feels fluid and alive, as if the physical glass of your device is the UI itself. This is much more than visual craft. Interactivity is at the core of the new design.\n\nSpecular highlights, refraction, and the translucent properties of Liquid Glass visually lift controls, all when maintaining a close relationship to the content beneath. Now that we’ve covered the high-level design thinking, let's focus on three guiding principles for implementing the new design in your apps.\n\nFirst, establishing hierarchy. Second, creating harmony. And third, maintaining consistency.\n\nLet's start with hierarchy. Liquid Glass controls and navigation act as a distinct functional layer that floats above your app. And this allows a new dimension of depth, bringing greater focus to your content. To enable this, elements once considered for rectangular displays have been redesigned to respect the rounded corners of the hardware, freeing up valuable space for your app's content And by thoughtfully grouping controls in this new floating layer, it removes visual density, allowing your content to extend to the edges of the display or window.\n\nThe color of Liquid Glass is informed by your app's content, intelligently adapting between light and dark app environments.\n\nOf course, you can apply your own custom colors to Liquid Glass as well.\n\nKeep this in mind as you indicate key actions, selection state, or status within your app.\n\nAnd Liquid Glass is dynamic by design. Controls can fluidly morph to recede in prominence when a user wants to focus on your app content. The moment a user needs them again, they'll expand. This works across different types of apps, whether they are designed as a single view or with multiple tabs. The new design ensures a content-rich experience.\n\nBy elevating controls and fluidly morphing between states, the new design elegantly reinforces hierarchy throughout the user experience.\n\nIt feels both familiar and entirely new all at once. And this brings me to our second design principle. These new dynamic controls create harmony between hardware, content, and controls. The shape of our modern devices inform the curvature, size, and shape of our UI elements, delivering concentricity and visual harmony. Going further, the new rounded forms are influenced by the geometry of your fingers, adding to more natural touch interactions across apps.\n\nConcentricity is key to the new design's ability to scale from smaller to larger screens, where sidebars and toolbars nest perfectly within windows. Along refreshed shapes, sizes, and color, our typography is updated as well. In an effort to make content more approachable, the new design includes larger, bolder, left-aligned typography. When applying the new design language to your app, you'll find nearly every component and control has been updated, with increased heights and softer shapes. This harmonizes the experience across Apple platforms, ensuring your app design feels considered. And it also helps with our third principle, maintaining consistency. The new design is universal across platforms, so now it's easier than ever for you to establish consistency for your apps. On macOS, your app becomes more visually aligned with iOS and iPadOS, while retaining the density Mac users love.\n\nAnd iPadOS now shares many of the same layout considerations as macOS, including consistent placement of common elements, as well as dynamically scalable windows and floating controls. And on watchOS, Liquid Glass controls allow content to shine through.\n\nAs a developer, the easiest way to design for consistency throughout your user experience is by adopting the new inset Liquid Glass controls.\n\nThis ensures your most essential UI elements are available across screen sizes, window sizes, and platforms. So that's a quick overview of the new design. We couldn't be more excited to see it come to life in your apps, making them feel more seamless and delightful, all while giving your users deeper connection to their content, harmony throughout their experience, and consistency across platforms.\n\nNow, over to Taylor to share the APIs that enable you to implement the new design in your app.\n\nEach of Apple's native UI frameworks, SwiftUI, UIKit and AppKit provide your app everything it needs to adopt the new design. You'll start by just recompiling your app on the new releases and seeing how it looks with no code changes. Next is refining that result using new APIs to tailor the design to your app. And finally, updating your custom views with the new design principles and Liquid Glass effects. Let's start by taking a look at what you can expect to see when you recompile your app for the new releases. The framework views your app is already using will automatically update with the new design. Here are some of the most common ones.\n\nMulti-tab apps using TabView or equivalent API will automatically get a new design with Liquid Glass tab bars. Or if your app uses NavigationSplitView, it will now have a Liquid Glass sidebar on both MacOS and iPadOS.\n\nYour iPad app will automatically now have the ability to resize columns too.\n\nAnd inspector columns have been updated with a unique edge-to-edge application of Liquid Glass to reflect the relationship with the content they edit.\n\nThe new design for toolbars and navigation bars build upon the existing patterns for how your app uses them.\n\nYour app’s toolbar items are now placed within glass and sit on top of a new scroll-edge effect, which provides separation between the glass and main app content. Navigation stacks have been updated to enable fluid morphing of the toolbar as users navigate and scroll through your app's content. Menus or popovers in your app's toolbar will automatically morph directly into those presentations. And a new API gives you additional control for other uses like sheet presentations.\n\nThose are just a few quick examples. Most of the views your app is already using will automatically get a fresh new look and new metrics to enhance utility and legibility.\n\nAnd these updates apply across all of the platforms your app runs on.\n\nLet's see the new design in action. I'm gonna try this out with a sample project we worked on last year, Destination Video. Now, I've just downloaded the existing project and I've opened it in Xcode 26, but I've made no other changes. The preview shows me how the app now looks with the new Liquid Glass tab bar. I'm gonna preview this on device to browse through more of the app.\n\nLook how the Liquid Glass plays beautifully against the content underneath as I scroll around. The tabs have a new control interaction effect as I switch between them. And when I drill into one of the videos, the back button is now within glass. Finally, the video player has a new design with Liquid Glass controls.\n\nDestination Video is a great example of making the most of framework-provided views. I didn't have to make any code changes to adopt the new design, and that'll be the case for many of your apps too.\n\nThis is also the perfect time to consider where you could simplify custom components and take more advantage of framework views.\n\nWith Liquid Glass enabled in your app, now we move on to refining that initial result and adopting new APIs to tailor the design to your app.\n\nNow it's unlikely your app will use all of these new APIs, so you can pick and choose the ones that best fit the design and intent of your app.\n\nToolbars have several new APIs that allow you to section and style items to reflect their role. Some views automatically create separation in the toolbar to reinforce the grouping of their content, and the new toolbar spacer API enables you to create additional groupings. Content in glass is monochrome by default, but you can apply color with purpose using tint modifiers and the prominent style for key actions.\n\nTabView has a new API for adding a custom bottom accessory that sits alongside the tab bar, which is perfect for showing playback controls or global status in your app.\n\nAnd this integrates with a new API that causes the tab bar to collapse when scrolling, offering even more space for your app's content.\n\nThe design of search has been updated to be more consistent within and across platforms.\n\nOn iPhone, the search field floats at the bottom of the screen, right where your thumb can reach it. We've also brought the toolbar search pattern from macOS to iPadOS, with the searchable API providing a consistent result between both platforms. Let's take a look at an example. We've been working on a sample app called Landmarks. I have the project open right here in Xcode.\n\nI've started with the detail page. Here's the code and a preview of Landmarks running on macOS. It shows the new design of the sidebar and toolbar. Now I want this prominent image to make the most of the Liquid Glass sidebar instead of ending at the edge. This is the perfect use case for background extension effect. This new modifier extends a copy of the image out of the safe area so that the real image isn't covered up. It's always so cool to see how much better that immediately looks in the preview.\n\nNext, I'll add some fixed spacers between items in the toolbar to create sections of the related elements.\n\nAnd now let's switch over to the iPad preview where you can see that the same two improvements have come through here as well. Now, in addition to the high-level framework views, there are APIs that allow you to create custom experiences with Liquid Glass material, complete with features like tinting and interactivity.\n\nLet's jump straight into adopting these in a custom control I've been building in the Landmarks app.\n\nHere I am back in the Landmarks project, this time editing the badges view, which displays badges for achievements I've received on the right side of the screen. It floats above the content already, which makes it a great candidate to adopt the Liquid Glass material. I’ll replace the custom background of each badge with the new glassEffect modifier, and I’ll replace the custom button style with the new glass button style. I’ll also add a glassEffect container, so all these effects are grouped as a set that can morph together.\n\nAnd there it is, the preview updates to show their new appearance, which is already looking great. But as you've seen, Liquid Glass almost feels like a physical material. The best way to experience it is with the device and input you're designing for.\n\nOn touch, the button springs up and glows to highlight my interaction with a subtle stretch effect. And on release, the Liquid Glass morphs to reveal the badges.\n\nOf course, every app will have its own path through its adoption of the new design, depending on how it was built and its design and complexity. But using Apple's native frameworks puts your app in a great position to make this adoption easy. Now, over to Bobby, who's going to tell you more about updating your app icons.\n\nLiquid Glass will make your app icons shine like never before. The new design creates opportunities for personalization and, for the first time, gives every app a cohesive identity across Apple platforms. Your app icon is a big deal. It represents your brand, tells your story, and builds engagement. Liquid Glass brings it to life with layering, depth, and vitality. Users can now choose from a variety of expressive appearances, each offering a light and dark variant.\n\nMost familiar is the classic full color appearance.\n\nTint mode has an even more colorful light variant and a more translucent dark variant. New this year, we're introducing a clear mode, that beautifully shows off the Liquid Glass material. On Mac, icons now support clear and tint modes and adopt the rounded rectangular shape that matches iPhone. And because you'll want to pay attention to how you're crafting the layers, highlights, and transparency of your icon, we're introducing Icon Composer, a new tool for building stunning icons for Apple platforms. As we've redesigned our own icons, we found that crafting them with two to four layers is the sweet spot. And Icon Composer is designed to help you do just that. Let's take a look. You can start by importing vector content. You can organize and annotate layers for multiple rendering modes, add blurring, adjust translucency, test specular highlighting, and preview icons in various tinting modes to ensure that your icon looks just right. When you're ready to publish, Icon Composer generates a single source artifact that you can import into Xcode. It can also export high resolution versions of the icon in fully rendered form for marketing and communication needs. Your icons will look radiant with Liquid Glass. And Icon Composer will help make your app's visual identity more consistent than ever. Icon Composer is included as part of Xcode, and you can get the latest download at the Apple Developer website. Back to you, Matthew.\n\nSo that's the iconic new design for apps and system experiences across Apple platforms. With Liquid Glass, your apps blend seamlessly into the display with new visual depth and a focus on your content.\n\nAll of our UI frameworks fully support Liquid Glass. So whether you're using SwiftUI, UIKit, or AppKit, your app can take advantage of its beauty and expressiveness.\n\nWhen run on iOS 26 and macOS Tahoe, apps built with Xcode 16 will have an unchanged user interface, keeping their current design. When you rebuild your app with Xcode 26, standard controls will automatically be rendered with the new design and materials. So you can then identify other opportunities where Liquid Glass can make your app shine, particularly for your custom controls and views. As you evaluate your app's UI and the time you need to adopt the new design, we're providing an option to continue to use your app's current design with Xcode 26.\n\nWe intend this option to be removed in the next major release.\n\nWe are so excited to see the transformative effects of Liquid Glass in your apps.\n\nNow let's talk about intelligence. At Apple, we use artificial intelligence and machine learning to add innovative features to all our platforms.\n\nAnd there are many opportunities for you to bring intelligence into your apps too. Josh will tell you how.\n\nApple Intelligence is the personal intelligence system built into the core of our operating systems that helps your users get things done through many features like Writing Tools, GenMoji, and Image Playground in apps across the system, including many of yours. For most of you using standard UI frameworks to render text fields, your apps support Writing Tools automatically. And those of you with custom text engines can adopt APIs to provide your users access to these capabilities in your apps. Similarly, GenMoji is automatically supported as stickers in your app when you use system text controls. And there are APIs for you to render them with custom text engines. And Image Playground APIs can support on-device image generation right in your apps. These are just the tip of the iceberg when it comes to the many ways your apps can take advantage of intelligence on Apple devices. This year, any app can tap into intelligence that's powerful, fast, built with privacy, and available even when the user is offline through direct access to the on-device foundation model that powers many of our features with the Foundation Models framework. This API allows you to tap into our highly optimized on-device foundation model which is specialized for everyday tasks. You can use the framework to power intelligent features in your app using model capabilities such as text extraction, summarization, and more. Prompting the model starts with just three lines of code: import the framework, create a session, and send your prompt to the model. The model is optimized with state-of-the-art quantization techniques and speculative decoding to provide performance, efficiency, and quality. And with Swift concurrency, the API enables you to choose to display the answer as one response or incrementally using streaming output. The framework also includes guardrails for our core features, and you can add your own safety rules for your specific use cases. It's great for things like content generation, in-app user guides, customized learning, and much more. We fine-tune the model for broad use cases, and when you invoke the model, you'll have the choice to add additional adapters like content tagging to improve performance even further for specific tasks. Often when you prompt a model, you'll need the LLM to generate structured responses to be used directly by your app. We've made this really easy with guided generation. With it, you can make your own data structures generable. Whenever you send a request to the model, your struct gets filled in with matching information, enabling you to prompt the model to produce full instances of your data type and allowing you to focus on your app's unique features instead of the details of directing and parsing model output. By supporting structured data output in addition to natural language, you can easily integrate intelligent features in whatever way best fits your app's user experience. This is a first-class experience written in Swift for Swift. The Foundation Models API also includes support for tool calling. It allows the model to identify a task that may require additional information or actions and call the appropriate function while it's processing the user's request. To utilize this capability, you define in Swift the tools the model can use, such as fetching up-to-date content from Wikipedia, referencing information from your app, or taking actions like creating a journal entry in your app. Then, the model can autonomously make decisions about what tool to use and when, so you don't have to decide it programmatically. Here's Richard to show you a demo of all of this in action.\n\nLet's use the Foundation Models framework to add generative content to a travel app I'm building. But first, let me show you how to prompt the model in Xcode.\n\nI'll start by importing Foundation Models.\n\nThen I'm going to use the new playground macro in Xcode to preview my non-UI code. To interact with the model, I first create a Language Model Session. As I type, it immediately sees the canvas on the right that we got a session. Then I just call session.respond to send my first prompt.\n\nI'll ask for a good name for a trip to Japan. And right away, we'll see the model's response appear in the canvas.\n\nFor large language models, prompt engineering is all about trying out different prompts and finding the best ones. I want to see how my prompt works for various travel destinations.\n\nSo I can add a for loop to iterate over all of the destinations featured in my Landmarks app. Now in the canvas, I can see the entire history of the model's responses to different prompts.\n\nQuickly iterating with Playgrounds is so great for prompt engineering.\n\nNow that we've seen it work, let's add a feature to my app to generate travel plans.\n\nI built my travel plan UI on top of these Swift data structures.\n\nWith guided generation, I can annotate them as @Generable so that the model can create them automatically based on my prompt. And I can also provide additional guides on the properties so that the model will only produce the values I expect. Then in my app logic, I go ahead and create a session and pass my custom instructions to the model. I ask it to generate an itinerary to help the user visit a landmark.\n\nI can also include an example itinerary that I created before to give the model an idea what kind of a response I'm looking for. In my session, I can leverage the model's tool calling capability.\n\nThis is a great way to let the model fetch external information autonomously.\n\nHere’s my custom tool that uses Mapkit to find points of interest whenever the model needs them.\n\nNow, I'm going to call session.streamResponse and ask the model to generate an itinerary struct. Pass in my prompt. Then streamResponse returns an async sequence that lets me update the itinerary in my UI as the model is generating it. And now, let's try it out. On my phone, I start by selecting the Grand Canyon, tap Generate Itinerary.\n\nThen I see the model generates a description and activities for each day.\n\nAnd the model also decided to fetch points of interest, like hotels, using our custom tool.\n\nWith prompting, guided generation, and tool calling, our on-device model created this Grand Canyon adventure for me right in the palm of my hand. Pretty amazing. Back to Josh.\n\nSince the model is on-device, your users’ data stays private and doesn't need to go to a server-side model or to anyone else. The on-device foundation model is readily available, so features you build will work offline, and you don't have to worry about account setup or API keys. And all of this at no cost to you or your users for any requests. The Foundation Models framework joins the suite of machine learning APIs and tools that you can use to tap into on-device intelligence for features within your app, including updates coming to the Speech API.\n\nBeyond the models we’ve provided on-device, you can also use Core ML to run models you bring onto device. Core ML optimizes performance by leveraging the CPU, GPU, and neural engine. With additional frameworks, you can further optimize your ML workloads for real-time signal processing on the CPU, or you can enable low-level access to GPU compute, all powered by Apple silicon. Based on your needs and level of expertise with models, you can pick the machine learning and AI frameworks and tools that best support you across our platforms. Then, for those of you experimenting with, training, and fine-tuning large language and other models, you can use MLX, an open-source library that takes full advantage of Apple silicon's unified memory. Another way you can enhance your app's features and capabilities is by giving them more visibility across our platforms through the App Intents framework. It can help your users easily find and use core functions of your app, even when they're not in your app. With this framework, you can define App Intents, the actions your app can perform, as well as app entities, the content your app can handle and produce. These come together to describe the important functions of your app. App Intents can be used with context-aware action button experiences, interactivity in widgets, automation via shortcuts, quick controls in Control Center, and customized results in Spotlight. And with the all-new Spotlight experience in macOS Tahoe, users can access all the App Intents that you've created right from Spotlight. If your App Intent has parameters, users can easily fill them in, creating more natural and seamless ways to find and get into your app from anywhere on the system. We're also introducing a new App Intents schema for Visual Intelligence. You can apply your app-specific Visual Search logic to content in Visual Intelligence. This brings your results right into the search experience so users can deep link into your app right from the results. Altogether, App Intents are the key to delivering rich experiences to your users all across the system. Now back to Matthew.\n\nIntelligence is also transforming how we write code. New developer tools and powerful coding models are already making us more productive and creative.\n\nAnd with the power and ease of Swift and SwiftUI, there is so much potential for everyone to explore new ideas. So let's take a look at some of the new features in our tools and languages designed to empower developers like never before.\n\nKen and Holly will start with what's new in Xcode.\n\nMillions of developers around the world use Xcode to build the most innovative and creative apps for Apple platforms. Xcode 26 is packed with incredible features and experiences that can help make your ideas a reality. Let's start by talking about intelligence. Last year, we introduced Predictive Code Completion, which uses a local model running on Apple silicon to provide intelligent suggestions based on your project and coding style.\n\nMillions of lines of code are created using Predictive Code Completion every week.\n\nWe continue to make the model better, improving accuracy and optimizing context gathering to help the model use more of your code, all running locally.\n\nBeyond code completion, generative intelligence really shines when you interact with code using natural language.\n\nWe tested Swift Assist with developers, and the main feedback we heard was about models. Many of you are already using models from multiple providers. And since Xcode plays such an important role in your development workflow, you want to have those models right at your fingertips as you craft code. This space is moving fast, with new capabilities like reasoning, multimodality, and more. So we expanded our vision, creating an even better experience that we think you'll love. First, we're excited to bring ChatGPT to Xcode. We're working with OpenAI to seamlessly integrate their optimized coding models right in Xcode. And you can try this in the first beta of Xcode 26 today. Let's see it in action. Take it away, Holly.\n\nI have an idea for a feature in the Landmarks app. Let's take a look at how easy it is to bring it to life.\n\nI'll open the new coding assistant from the toolbar right at the top of Xcode. I've already configured Xcode, so I'm ready to go. First, I'm going to add a new view to the Landmarks app.\n\nMy idea is to show statistics about my landmark locations.\n\nXcode created a statistics view and modified my existing navigation list. I can click on each code snippet and see the changes. I can also open the overview. It shows me all of the modified files in a single view with colorful annotations highlighting all of the code changes.\n\nThe best part? Xcode automatically updates the code for me. And with previews, I can visualize my app's UI almost instantly. Now that I've got the statistics view in place, I'll ask what landmark data is in the project that might make a more interesting dashboard.\n\nWhen interacting with the model, Xcode automatically sends the context, like the file I'm in, what code I have selected, errors, and related files. And the model can ask for more information from my project as it works on a response.\n\nHere it looked at various files in my project to understand my code and summarize data structures as feature ideas. And Xcode linked me to the reference files so I can understand the code too.\n\nThis looks like a good plan. I'll ask to implement these ideas.\n\nI haven't really given any direction on how to present this data. Let's see what it comes up with. Okay, now that I've got the data, let's design the dashboard.\n\nHere's a quick sketch of what I'm thinking. I'll drop it in with my request.\n\nSometimes the easiest way to express an idea is visually, with a drawing or mockup or annotated screenshot.\n\nWow, that's a pretty great start. It even used system colors and SF Symbols to bring my sketch to life.\n\nWhat was just an idea a few minutes ago is now a feature I can try in my app.\n\nAnd to help you stay in the flow and be more productive in the tasks you're doing every day, like writing tests and documentation, fixing issues, or simply understanding code, we're putting those capabilities right where you need them.\n\nLet me start by showing you a new feature called Coding Tools. It's just like Writing Tools, but for code.\n\nI can bring it up anywhere in my code and get suggested actions like generating a preview or playground or fixing an issue. And if I'm looking for something more specific, I can just ask for it.\n\nA great way to understand non-UI code is to try it out. Like you saw earlier, there's a new playground macro for exploring code. It's just like a preview, but it works for any code. I’ll choose Generate Playground.\n\nThe code appears right in my file. It gives me a real example calling into the code. I can preview anything.\n\nNow that I know how this code works, let's add documentation. I'll make a selection and choose Document.\n\nIt created DocC comments and I can also preview the rendered documentation.\n\nEveryone always writes perfect bug-free code, right? But if something does slip through the cracks, don't worry, I can just click on the issue and choose Generate Fix and get back to building.\n\nAnd my favorite feature, the conversation history. When I'm working with the model, I love having the freedom to safely explore ideas. I'm often many prompts in before I realize I want to go back in time and go a different way. With the history slider, I can scrub through time and see every change, making it easy to roll back and keep on vibing. With quick actions, intelligent error fixing, conversation history, and more, Xcode is now supercharged. In just a few clicks, you can start using ChatGPT and Xcode without even creating an account. You'll get a limited number of requests each and every day. ChatGPT subscribers can connect their accounts for even more requests. And you're in control of the data you share with OpenAI. If you're using models from different providers, it's easy to bring those to Xcode too. For example, you can add your anthropic API key. And use their latest models like Claude 4 Opus and Sonnet directly in Xcode. And you can run models locally on your Mac or private network, powered by the tool of your choice. This gives you flexibility and control to use the model that works best for you.\n\nAnd when a new model is released, you can just use it with Xcode.\n\nWith built-in intelligence powered by the best coding models, Xcode 26 will transform the way you create apps.\n\nThere are many more exciting features to discover, like the redesigned and simplified tab experience. So just like in Safari, you can quickly open a new tab with Command T and all navigation happens right in the same tab. And if you want to stay focused on a file in a tab, just pin it.\n\nXcode is also more accessible. If you have limited use of the keyboard, Xcode 26 improves support for Voice Control, letting you dictate Swift code and navigate through the interface entirely by voice. And to make localizing your app easier, you can automatically generate usage description comments in the string catalog, providing the right context for accurate translation. The first beta of Xcode 26 is available now, and you can start coding with ChatGPT and other models today. Next, to talk about all the exciting changes in Swift, here's Ben. We created Swift with a clear goal of building a language that meets the challenges of modern development without trading off performance for safety.\n\nOne that's powerful, but also easy to use. And today, we're taking that further with Swift 6.2.\n\nThis release adds some great features focused on performance. One of the most anticipated is inline arrays. These allow you to declare arrays with a fixed size that can be stored on the stack or directly inside other types without using heap memory.\n\nKnowing the size of an array at compile time unlocks optimizations by the Swift compiler, enabling significant performance wins.\n\nWe're also introducing a new span type.\n\nIt provides a safe alternative to pointers for fast direct access to contiguous memory.\n\nThis type is key to another new feature that improves communication with unsafe languages like C.\n\nWhen C pointers are annotated with length and lifetime information, they can be bridged into Swift as a span, providing a memory-safe interface that’s also easier to use.\n\nThis combination of safety and performance makes Swift a great option where these attributes are critical. And that's why WebKit is introducing Swift into its code base, making use of a new opt-in strict memory safety feature to make sure interaction with C APIs happens securely. We've also made improvements to C++ interoperability, with code written using more advanced C++ language features now callable from Swift. And beyond C, Swift's cross-language support has expanded with packages for interfacing with Java and JavaScript code. And you can now run Swift in the browser. Working with the open source community, we’ve brought official toolchain support for WebAssembly to Swift 6.2.\n\nNext, let's talk about concurrency.\n\nFor your app to be secure and stable, it's really important that your code is free from data races. The Swift 6 language mode is designed to make your concurrent code safer.\n\nIn many cases, though, like your app's user interface or command line tools, you're actually writing code that’s only ever meant to be single threaded.\n\nSwift 6.2 makes it easier to write that single-threaded code. You can configure modules or individual files as running on the main actor by default without any additional annotations.\n\nIt's also easier to make async calls from the MainActor, with better language defaults that generate fewer compiler warnings for code that is not intended to be run in parallel.\n\nAnd when performing CPU-intensive tasks, you can offload tasks to the background to keep apps responsive with the new concurrent attribute.\n\nAt Apple, we're using Swift everywhere.\n\nWe run it embedded on the silicon that secures memory management on the GPU. And in large-scale server workloads like the Apple Password service, which handles billions of requests a day from devices all over the world, and where a recent rewrite in Swift resulted in a huge reduction in server footprint.\n\nMany of you also develop cloud services for your apps.\n\nReusing your app's Swift code in your server implementation is an exciting opportunity for you as an app developer.\n\nLike the team at Cultured Code, who use Swift to power the server-based synchronization of their award-winning task manager, Things.\n\nTo streamline the development of these server-side components, we've created Containerization, a new framework and tool for containers. Its command line tool lets you create, download, and run Linux container images right on your Mac.\n\nAnd it's built on an open source framework that is optimized for Apple silicon and provides secure isolation between container images.\n\nContainerization is written in Swift, and it's available as open source today.\n\nYou can download the binaries or check out the repo on GitHub.\n\nOn the new Swift.org website, you'll find guides for getting up and running writing cloud services in Swift, along with a new Toolchain installer that makes using Swift on Linux easier than ever.\n\nWe're excited about the future of Swift wherever you use it. Now, back to Matthew.\n\nWhen you use Apple's native frameworks, you can write better apps with less code. Some other frameworks promise the ability to write code once for Android and iOS.\n\nAnd that may sound good, but by the time you've written custom code to adapt each platform's conventions, connected to hardware with platform-specific APIs, implemented accessibility, and then filled in functionality gaps by adding additional logic and relying on a host of plugins, you've likely written a lot more code than you'd planned on. And you are still left with an app that could be slower, look out of place, and can't directly take advantage of features like Live Activities and widgets. Apple's native frameworks are uncompromisingly focused on helping you build the best apps. So let's dig into some of the new experiences and capabilities in our SDKs, starting with improvements coming to SwiftUI. Here's Sommer.\n\nWhether you’re creating a new app or expanding an existing one, SwiftUI is the best way to build it. And today brings a set of exciting updates inspired directly by your questions, your ideas, and your feedback, including new web APIs, new rich text editing capabilities, and support for 3D charts, as well as big improvements to performance. Let's start with our powerful new web APIs. Today, your apps can embed web content with WebKit's WKWebView API.\n\nIn SwiftUI this year, a new version of OpenURL allows you to show a simple in-app browser. For more power, WebKit adds a new declarative WebView component that's designed for SwiftUI, as well as a new web page API for programmatically controlling web content.\n\nThese three new APIs are built with modern Swift technologies like observation and strict concurrency, meaning it's never been simpler to bring web content to your apps.\n\nNext, we have one of your most-requested features. The SwiftUI text editor has gotten rich. To enable rich text editing, simply change the binding of your text editor's text from a string to an attributed string. Now you have a rich text editor with styling you can fully customize. And by tracking attributed text selection, you can develop your own editing experiences, complete with controls and inspectors for formatting selected text.\n\nFinally, SwiftCharts now supports 3D thanks to RealityKit.\n\nThis update includes support for directly interacting with the camera to rotate and zoom in on your charts from every angle. And on visionOS, SwiftCharts works in spatial environments too.\n\nNow, let's talk about performance. Nothing makes an app feel fast like smooth scrolling. iOS, SwiftUI, and UIKit use a technique called idle prefetch, taking advantage of the idle time after rendering the current frame in order to get a head start on rendering the next frame. That extra time helps reduce the chance of dropping a frame while scrolling.\n\nThis year, SwiftUI brings idle prefetch to the Mac for the first time for a huge boost in performance with optimizations across all other platforms as well. Of course, the most common scrollable views are lists and tables, and SwiftUI is getting even faster at displaying lists and tables with very large amounts of data. On macOS, a list of 100,000 items will now load over six times faster. Incremental changes, like inserting new items, are up to 16 times faster, and larger lists will see even bigger gains.\n\nAltogether, these improvements represent a big step forward in SwiftUI scrolling performance. And this year, we're bringing you a powerful new performance instrument to help you optimize your own code, allowing you to drill down to the exact moments that impact your app's performance and analyze precisely where and why your own views are updating.\n\nPowerful web APIs, rich text editing, beautiful 3D charts, and faster performance join a whole host of other improvements to SwiftUI this year, including a more flexible SwiftData with model subclassing, entity inheritance, and support for additional common data types like attributed string. There's so much to get excited about: push notifications for widgets, better control over drag and drop, and scene interoperability. And if you're building a spatial app with SwiftUI, there are even more cool features in this year's release. Here's En to tell you more.\n\nvisionOS 26 will enable groundbreaking new spatial experiences in your apps and games. This year brings an extensive update with new volumetric APIs, advanced sharing capabilities, exciting immersive media tools, and powerful enterprise features. Now, enhancements across new and existing SwiftUI APIs make it even easier to build compelling volumetric experiences. You can create even richer 3D layouts in the same familiar way as 2D UI. Layouts are now aware of visual effects like rotation. Existing APIs allow you to easily align your views within volumes. You can align overlapping content in the same 3D space with spatial container, or anchor content to specific locations with 3D anchor preferences, all in SwiftUI.\n\nDynamic bounds restrictions allows you to draw outside your app's bounds in both volumes and windows with a simple view modifier. And environment occlusion makes it so virtual objects can be occluded by static real world objects.\n\nYou can bring this behavior to your apps by adding an environment blending component to any entity.\n\nNow, a new suite of APIs deep in the integration between SwiftUI, RealityKit, and ARKit. It is easier to position and translate content across these three fundamental frameworks, regardless of the coordinate space you're working in.\n\nRealityKit's entity and its animations are observable, allowing them to be used directly in SwiftUI views. You can directly apply gestures to entities or present and animate 3D content in your app with Model3D.\n\nBecause many of the best spatial experiences are shared, visionOS 26 introduces a new capability. With nearby window sharing, you can build shared spatial experiences for users located in the same room, like Rock Paper Reality's Defender-Ella, as well as bring in remote participants using FaceTime and spatial personas.\n\nNew SharePlay APIs make adoption even easier, and your existing SharePlay apps will automatically work.\n\nARKit also adds support for shared world anchors, so you can precisely anchor shared content to the room. In visionOS 26, apps and QuickLook content persist or reappear in the same place, even after restart. This behavior is also coming to widgets. The familiar widgetKit framework, along with new APIs to specify its texture or react to users' proximity, are now available, allowing you to build dynamic widgets that persist throughout user spaces.\n\nAround the world, users love experiencing content on Vision Pro, and now they have an incredible new way to experience photos. RealityKit's image presentation component can be used in your app to transform 2D images into 3D spatial scenes, leveraging on-device, generative AI algorithms to create a 3D representation of your image, optimized for real-time rendering from multiple points of view.\n\nIf you are building media applications, you can play and distribute even more types of immersive content with built-in support for 180 degree, 360 degree, and wide-field-of-view video through Apple Projected Media Profile, or APMP. You can play and stream these new immersive media formats inside your app using familiar AVKit, RealityKit, WebKit, and QuickLook APIs. And if you produce Apple immersive video content using the new Blackmagic Camera and DaVinci Resolve app, you can play it back in your apps or directly from web pages in Safari using the HTML video element.\n\nIn enterprise environments, visionOS 26 makes it even easier for developers to incorporate Vision Pro into their organizations. With enterprise entitlements, you can disable content captures of your app's view, enable apps to automatically follow the user's position as they navigate a space, access the left and right camera feed simultaneously, or grab a specified region of the camera feed for stabilization and enhancement. These are just a few of the incredible features coming to visionOS. Now I'll hand over to Eric to talk about gaming.\n\nApple silicon enables exceptional performance and visuals that unlock the most advanced games. This year, we're making it even easier to bring your game to Apple's unified gaming platform. We've focused on three key areas: advanced graphics technologies, improved game developer tools, and system features to deliver a great gaming experience for your players. The key graphics technology powering these experiences is Metal. Ten years ago, we brought Metal to the Mac, and since then we've been adding capabilities to power the most advanced graphics workloads. Now we're introducing Metal 4 with tons of new features to support the most advanced graphics and ML technologies like neural rendering which combines traditional graphics with machine learning inference. With Metal 4 you can now run inference networks directly in your shaders to compute lighting, materials, and geometry, enabling highly realistic visual effects for your games. And you can use MetalFX upscaling, frame interpolation, and denoising APIs to take your game’s graphics performance to the next level. MetalFX frame interpolation generates an intermediate frame for every two input frames to achieve higher and more stable frame rates. Here we're showing the upcoming Mac version of Cyberpunk 2077 on the M4 MacBook Air. On the right, CD Projekt Red is using MetalFX frame interpolation to increase performance to a solid 60 frames per second. They also use MetalFX denoising to enable ray tracing on the game's ultra settings on the M4 Max MacBook Pro. Metal 4 is designed exclusively for Apple silicon and it sets the stage for the next generation of games on Mac. This year, we further streamlined the game development experience with improved developer tools. The updated Game Porting Toolkit provides everything you need to get started bringing your Windows game to Apple's platforms. Like tools to evaluate and profile your game, convert your shaders and assets, and human interface guidelines and code samples to build native games that feel at home on Apple devices. For example, CD Projekt Red used Game Porting Toolkit to dramatically speed up the process of bringing Cyberpunk 2077 to Mac. And the latest version of Game Porting Toolkit provides new tools to make it even easier to optimize your game, with support for Windows upscaling technologies. For example, a developer like Remedy could use it early in the porting process to evaluate how much MetalFX could improve the performance of their game, Control, when running on Apple silicon. Game Porting Toolkit provides on-screen insights and guidance for optimizing your graphics code for the best possible performance. And you can now customize the Metal Performance HUD as you profile and debug your code.\n\nAnd finally, there are new tools to build, run, and debug games remotely from a Windows environment. This is great for targeting Mac from an existing game development toolchain. And once you've got your game up and running, you can adopt powerful system frameworks to deliver immersive graphics and audio, incredibly responsive input, and a seamless gaming experience for your players. This year, the frameworks that power input across iPhone, iPad, Mac, and Vision Pro are getting major upgrades, with easier pairing for PlayStation DualSense controllers across all of your devices. A new touch controller API provides an easy way to add on-screen controls for iPhone and iPad, and you can enable powerful new ways to play games on your Vision Pro, with support for PlayStation VR2 Sense controllers and up to three times faster hand tracking. To enable seamless play across devices, you can bring cloud save to your games with the new GameSave framework. For playing on the go, macOS Tahoe has optimized Low Power Mode for gaming. As a developer, you can further extend battery life by enabling more efficient game settings when the system is in Low Power Mode. With the new Game Center Challenges API, you can turn single player game activities into social experiences with friends. These challenges will appear in the new Games app and players can also access them in the Game Overlay, interact with friends and change settings all without leaving the game.\n\nWith all of these new features, it's never been a better time to bring your next generation games to Apple's unified gaming platform. Now back to Matthew.\n\nMetal 4 is a great example of the tight integration of our software with Apple silicon, creating a whole new class of experiences. In fact, since we began the transition to Apple silicon over five years ago, we've been able to add incredible features like Apple Intelligence, Game Mode, Presenter Overlay, and more.\n\nWe completed the transition to Apple silicon across our entire product lineup two years ago. So your apps can now depend on and build upon these features too.\n\nApple silicon enables us all to achieve things that were previously unimaginable. And it's time to put all of our focus and innovation there.\n\nAnd so, macOS Tahoe will be the final release for Intel Macs.\n\nSo if you've not done so already, now is a great time to help your users migrate to the Apple silicon versions of your apps.\n\nThere's a lot to love in this year's release.\n\nThe new design with Liquid Glass brings new depth, fluidity, and dynamism to your apps. Apple Intelligence lets you take advantage of on-device models with guided generation.\n\nXcode transforms the way you create apps using any coding model. And updates to Swift and SwiftUI further expand your app's performance and capabilities, making it easy to bring your apps to the full range of Apple platforms.\n\nSo, is that all we have? Not even close. There are so many other new features and APIs to explore. And we have over 100 sessions that go deeper into what you've heard about today and what you haven't yet. Here's a few examples. Let's do this as a lightning round. You can now create menus and commands that are included in the new iPad menu bar. With an updated Background Tasks API on iOS and iPadOS, you can start long-running tasks that will complete in the background, like a video export. CarPlay now supports Live Activities so your app can show timely, relevant updates, even when users are on the road. In macOS, Terminal has a fresh look with 24-bit color, new themes inspired by Liquid Glass, and support for Powerline fonts. The HTML model element embeds 3D models into your webpages. And on visionOS, it can be viewed stereoscopically in line and dragged out into the real world. When you adopt Look to Scroll on visionOS, users can browse hands-free just by looking at the edges of content. The declared age range API helps you adjust your app's experience to be age appropriate while preserving user privacy. The new PermissionKit framework gives your apps new tools to help children communicate safely with parental supervision. You can now highlight your app's accessibility features in a dedicated section of your App Store product page. And for assistive access, you can now customize the experience in your app with more focused features and a simplified user interface.\n\nThere's just so much to explore. The sessions are available starting today, so you can just go dive in. And we look forward to connecting with you in the Labs and the Apple Developer Forums.\n\nWhat you do as developers is amazing. You turn ideas into incredible experiences for your users and bring our platforms to life.\n\nApps have become essential to how we all connect and communicate, deliver our best work, get creative, or explore new things. So whether you are just starting out or have been building apps for years, thank you for being part of such a vibrant developer community.\n\nWe can't wait to see what you create next.",
    "segments": []
  },
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/112/2/185eed6d-badf-45be-ab91-f18c30c4cc71/downloads/wwdc2025-112_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/112/2/185eed6d-badf-45be-ab91-f18c30c4cc71/downloads/wwdc2025-112_sd.mp4?dl=1"
  },
  "extractedAt": "2025-07-18T10:19:57.588Z"
}