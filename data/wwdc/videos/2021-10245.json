{
  "id": "10245",
  "year": "2021",
  "url": "https://developer.apple.com/videos/play/wwdc2021/10245/",
  "title": "Design for spatial interaction",
  "speakers": [],
  "duration": "",
  "topics": [
    "Design"
  ],
  "hasTranscript": true,
  "hasCode": false,
  "transcript": {
    "fullText": "♪ Bass music playing ♪  ♪ Peter Tsoi: Hello and welcome to \"Design for spatial interaction.\" My name is Peter, and I'm a designer on the Apple Design team.\n\nTogether with my colleagues Arian, Taylor, Linus, and Pedro, we're excited to share some of the considerations that guide how we approach designing spatial interactions.\n\nWe'll share lessons learned while creating interactions for AirTag, HomePod mini, and iPhone, and also give you tips and tricks that will help you apply those principles while designing interactions of your own.\n\nLet's get started.\n\nAs our devices have evolved, so too has the way that we interact with them.\n\nEarly computers were operated only using a keyboard and interfaces were entirely text based.\n\nUsers memorized abstract commands and relied on arrow keys to move around the screen and express their intent.\n\nNowadays, most people use their keyboards together with graphical user interfaces and a mouse.\n\nThese innovations removed a layer of abstraction and allowed people to use the mouse to point, click, and drag to more directly express their intent.\n\nLater, Multi-Touch brought the digital world to people's fingertips, removing yet another layer of abstraction and allowing people to touch their music, their photos, and the mobile web.\n\nToday we want to talk about a class of interactions that go beyond the bounds of a single device and ways to use the capabilities of the latest Apple devices to help users interact more directly with their physical surroundings.\n\nSome of these interactions may already feel familiar, like how you can pay for a coffee by holding your phone or watch close to a payment terminal.\n\nOr how you can quickly check the battery level on your AirPods just by flipping open the case when it's near your phone.\n\nThese human-scale interactions -- the ones that happen between devices nearby -- have the ability to remove layers of abstraction and allow users to interact more directly with their surroundings.\n\nAnd with the enhanced spatial awareness enabled by the U1 chip in the latest Apple products, we've been able to deliver more capable and responsive experiences for AirTag, HomePod mini, and iPhone.\n\nAnd now, thanks to new APIs and the ability to interact with third-party hardware that you build in iOS 15, you can now bring these kinds of experiences to your own products.\n\nIn designing these interactions, we learned that it's important to consider distance and ability, provide continuous feedback, and embrace the physical action.\n\nLet's dive right in.\n\nIn iOS 13, we made it even easier to share content by bringing spatial awareness to the share sheet.\n\nUsing information about who you are facing or physically close to, combined with on-device knowledge about the frequency and recency of your communications, allows your device to intelligently predict who you are trying to share with.\n\nThis is a great example of how existing features in your apps can be made more intelligent with spatial awareness.\n\nYou don't have to build an entirely new experience to take advantage of these new capabilities.\n\nWhen incorporating spatial awareness into existing features, be mindful that these capabilities are not available on all devices and your design should accommodate varying levels of capabilities.\n\nWith the share sheet, all devices are able to look for others using Bluetooth and Wi-Fi.\n\nHowever, iPhones with the U1 spatial awareness chip take this a step further by prioritizing others that you are facing or are very close to.\n\nOf course, spatial awareness can be used to not just enhance existing experiences; it can help build entirely new ones.\n\nNext, Arian will share how these same concepts apply to AirTag.\n\nArian Behzadi: With AirTag, we will show you how your design can adapt to accommodate different distances and abilities.\n\nIn this example, imagine I've misplaced an item I care about.\n\nI retrace my steps and realize I'm not sure where it is.\n\nI use my iPhone and feel relieved that I can see where it is on a map.\n\nPreviously, I'd only be able to get to the general vicinity of this item, but now my iPhone can guide me at a scale that is much more helpful.\n\nIn fact, the same button that gave me directions when I was far away has now become a button that will help me find it as I'm closer.\n\nAugmenting elements of the design when at the appropriate scale is a great way for people to discover and use these new abilities.\n\nAs iPhone is locating my AirTag, I'm invited to move around and search my surroundings.\n\nOnce connected, an arrow forms and points directly to where I need to go. As I align with the direction of my AirTag, the design lights up to reinforce this.\n\nWe emphasize this facing direction and provide guidance when not facing this direction.\n\nIn a subtle way, this distinction also scales based on distance.\n\nWhen further away, we found that it is really hard to remain facing a specific direction.\n\nTo remedy this, we are actually more generous with the angle that iPhone considers facing.\n\nAs you get closer, this forgiveness becomes more narrow to become more and more specific.\n\nThis happens without you noticing as you're gently guided to your AirTag.\n\nWhen you're finally within arm's reach of your AirTag, the arrow defers to a form of feedback that we find much more effective at this small scale. The design transforms to highlight haptic feedback as you pass your iPhone over where your AirTag might be hiding.\n\nWith this example, we've shown you how your design can transform based on distance, whether you're very far away or within millimeters.\n\nThink about how your design can best accommodate varying abilities we all share as humans.\n\nBe forgiving with angles at a distance, and consider changing the dominant form of feedback to one that works best at smaller scale.\n\nNext, I'd like to hand it over to Taylor to show you just how critical this continuous feedback is when designing these kinds of interactions.\n\nTaylor Carrigan: Thanks, Arian.\n\nWe've gone to great lengths to make onscreen interactions fluidly and dynamically respond to touch input, but this continuous feedback is even more critical when interacting in the physical world where your device becomes a more literal extension of your body.\n\nThe right type of feedback applied and choreographed at the right time throughout these interactions can help make the feature you're designing discoverable, provides instruction, and can communicate success or failure.\n\nIt also acknowledges that our movements can start, stop, and be interrupted at any time.\n\nWe're going to talk about how you can use feedback you can see,  hear, and feel to help connect your interaction to the physical world.\n\nThough dependent on the capabilities of the devices used, there are multiple types of feedback worth considering and using together.\n\nThese can include visual feedback -- such as user interface changes, lights and hardware interactions, and visual feedback coordinated across both devices -- audio feedback , and haptic feedback.\n\nWe'll also talk about natural interruption and cancellation and the importance these play in creating a successful interaction between two devices.\n\nLet's look first at how different types of feedback help make the transfer of music to HomePod mini intuitive and satisfying.\n\nWith HomePod mini, we use multiple types of feedback to establish a physical relationship with your iPhone, provide direction, and ultimately confirm that your music has been successfully transferred.\n\nWe achieved this by creating two discrete boundaries around the HomePod, and respond to your movement across and between them.\n\nFeedback is provided when your iPhone reaches the first zone, between the first and second to instruct you to move closer or allow you to cancel the interaction, and finally when your iPhone reaches the second zone, to confirm and transfer music.\n\nFeedback is continuously provided from the moment iPhone reaches the first distance threshold.\n\nOn screen, the banner position, scale, and modality created by the background blur increase fluidly and in direct response to your distance from the HomePod.\n\nWhen designing visual feedback, look for opportunities for the onscreen elements to animate in a way that feels related to your physical movements -- like how the movement of this banner is related to the linear movement of your hand towards or away from the HomePod.\n\nHaptic feedback complements this with a physical acknowledgement that the two devices are aware of each other, and increases in strength to encourage you to continue moving closer.\n\nIn addition to the feedback on iPhone, HomePod mini acknowledges the proximity of iPhone through the animations on the top of the speaker.\n\nGreat spatial interactions use natural body movements to not only confirm but also to cancel actions.\n\nWhen moving an iPhone near a HomePod mini, clear and continuous feedback that tracks your distance from the HomePod makes it clear that you can simply pull away to cancel or interrupt the gesture.\n\nThis allowed us to create an interaction that can be done entirely without looking at the display or requiring additional onscreen buttons to cancel or confirm user intent.\n\nAll of these aspects are driven by and respond to the movement of your body as it brings iPhone closer to or farther away from the HomePod.\n\nThe directness of the relationship between someone's movements in the physical world and what they're seeing, hearing, and feeling on one or both devices are critical when designing a spatial interaction.\n\nNext, Linus will talk about the unique constraints of providing feedback while finding an AirTag.\n\nLinus Persson: HomePod mini is a good example of how to choreograph feedback across two devices, both capable of expressing rich feedback.\n\nFor designs where this is not a possibility, you might have to rely more heavily on a single device.\n\nSince AirTag helps you find an item when it's lost, the experience is designed for scenarios when it's hidden from view.\n\nLet's revisit the finding experience we saw before, this time with an eye on how we provide continuous feedback across the senses using iPhone's rich haptic, visual, and acoustic capabilities.\n\nA spatial interaction feels great when it's responsive and follows your motion.\n\nThat means responding to movements that are big as well as ones that are subtle.\n\nEven as iPhone is connecting to the AirTag, the interface gently rotates and responds as I turn, implying the nature of the interaction that is about to follow.\n\nThese dots also provide the building blocks for a continuously adapting experience.\n\nOnce connected, the distance between me and my AirTag also appears.\n\nThis distance responds incredibly precisely as I walk in any direction.\n\nIt instantly updates and tells me if I'm walking toward, or away, from where I want to go.\n\nAs I walk around, the same dots smoothly form an arrow pointing me in the right direction.\n\nThis moment is reinforced further using a delicate haptic and sound to mark its importance.\n\nIt feels as though the arrow floats in space as my iPhone moves around it.\n\nAs you saw earlier, the interface reinforces moments when I am on the right path.\n\nWhen facing the direction of my lost AirTag, the screen boldly lights up and I also feel and hear the arrow snap into place.\n\nBy responding to every step and subtle turn, the design feels tightly coupled with me and the space I'm in.\n\nWhen designing your spatial interaction, consider how people will connect their own movement with what they experience on the device.\n\nTry to make a clear link between action and feedback in a way that is mindful of the particular motion.\n\nJust ahead of the arrow, there's a dot that starts to pulse in my hand.\n\nThe pace of this pulse picks up as I get closer, and the haptics become tighter and crisper along with it.\n\nIf I break away from this direction, the pulse stops and the interface guides me back.\n\nIf I walk far enough away from my AirTag, the arrow disassembles to mark the loss of signal.\n\nNuance in moments like these also provide important and very helpful guidance.\n\nBe mindful of how feedback can complement human motions that are varied and unpredictable, and build a design that is resilient and can adaptively change to accommodate the way we move.\n\nWhen within arm's reach, we visually zoom in on this dot, and the pulse changes to continuous haptic feedback that shifts with my movement.\n\nThis haptic response changes in character as I move closer and further away.\n\nI can move my iPhone over where AirTag might be and sense through my hand more precisely where it's hiding.\n\nNote that we do not use sound for this part of the experience, as haptic feedback does a much more effective job.\n\nAs with any continuous feedback, your design should be mindful of how haptics, sound, and visuals work together in concert across the senses.\n\nSimilarly to the way we perceive what we see, hear and feel in the world as one holistic experience.\n\nWhen designing your interaction, it is important to consider the different strengths of each form of feedback and how they can be used together.\n\nSeek to provide a visual layer of continuous feedback tied to physical motion and use additional feedback to emphasize important moments in your interaction.\n\nUse haptics and sound judiciously and at the right levels, with a repeatable cause and effect so it is clear what is being communicated.\n\nIn addition to working with feedback in this way, keep in mind that your design will complement a physical action.\n\nAnd to speak about this is my colleague Pedro.\n\nPedro Mari: In the case of HomePods, we use spatial awareness to make the task of selecting a speaker more natural.\n\nOftentimes, I'll be listening to some music, and I'll want to move it from my iPhone to a better speaker near me.\n\nPreviously, if I wanted to play this song on a different speaker, I'd have to go into the AirPlay list.\n\nIn this list, all of the other devices in my space are presented equally, whether they are near me or not.\n\nWhat I am really interested in, though, is this particular HomePod.\n\nWith this new human-scale interaction, I can finally transfer a song to this HomePod without navigating an onscreen UI, or having to interact with the screen at all, for that matter.\n\nWe provide visual feedback on the HomePod itself to show that the music is about to be transferred.\n\nThe light modulates precisely with my movement, growing in intensity as I approach, and shrinking as I move away.\n\nAnd of course, the ultimate confirmation of the transfer is the music playing from the speaker itself.\n\nBy embracing the physical action, a previously intangible experience now feels completely tangible, visceral, and instinctive.\n\nThink of ways that your own interfaces can support a natural physical motion, rather than an abstracted list of options.\n\nTry to make your experiences and designs defer to the physical task at hand.\n\nAlso, consider how the concepts of \"this\" and \"that\" can be finally used to create a more natural experience that aligns with how we think.\n\nMake sure to provide instant and continuous feedback on both devices.\n\nSpecifically, be very clear on the target device that an action is happening, as this is where your attention will be naturally drawn to.\n\nSimilarly, when finding a lost item, my attention is meant to be directed at the world around me and where it might be hiding.\n\nPlaying sound from the AirTag gives me an immediate sense of its place in space, drawing my attention to my surroundings.\n\nWhen it comes to the visual feedback on iPhone itself, make sure it can be read when performing a task that extends beyond the bounds of the display.\n\nWe often design our interfaces to work in this manner when we anticipate it will be viewed in the periphery.\n\nFor instance, when using turn-by-turn navigation, the typography of when and where I need to take my next turn is bigger and bolder than other notifications that might occupy the same space.\n\nThe buttons and active area of TV Remote are extra large and are designed for your eyes to be directed to the TV itself, rather than the control for it.\n\nThe digits and operation buttons in Calculator are large and bold, as we anticipate you might be referencing numbers on a restaurant bill to calculate a tip, for instance.\n\nAs in the previous examples, the type size we use for the distance when finding an AirTag is set much larger than in other parts of the system, and the entire UI revolves around the central element of a giant arrow.\n\nBold color changes can be read in my periphery.\n\nSound reinforces key moments and states -- I can actually feel the distance getting smaller as I walk without ever having to look at the screen.\n\nThis ultimately supports a more natural pose as I use it.\n\nWhen in arm's reach, haptic feedback can guide me closer even if the screen is not in my periphery.\n\nThese large and bold UI choices enable those with varying abilities to access the experience as well.\n\nMake sure your design uses more than one mode of feedback, and that each one is salient enough to be clearly understood.\n\nYour design shouldn't demand too much attention to communicate its information or compete with the primary task.\n\nLastly, try to reinforce good states and celebrate positive progress along the way to completing a spatial interaction.\n\nAlways consider the physical action your experience is complementing.\n\nAnd now, back to Peter! Peter: We've covered a lot today.\n\nWe've seen how new and existing experiences become more intelligent and relevant when they consider the device's context and surroundings.\n\nYou learned how responsive and continuous feedback can help users discover and use these new experiences.\n\nAnd finally, we took a look at techniques that allow your design to be effectively experienced peripherally while keeping attention on the physical surroundings.\n\nWe hope that you are just as excited as we are about how spatial awareness can simplify and bring more directness to interactions between our devices and our users' physical surroundings.\n\nFor more information, be sure to visit developer.apple.com where you'll find the Human Interface Guidelines for spatial interactions, as well as technical information about how to implement these types of interactions.\n\nWe can't wait to see what you'll build next.\n\n♪",
    "segments": []
  },
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Human Interface Guidelines: Nearby interactions",
        "url": "https://developer.apple.com/design/human-interface-guidelines/nearby-interactions"
      },
      {
        "title": "Nearby Interaction",
        "url": "https://developer.apple.com/documentation/NearbyInteraction"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2021/10245/8/3C4EE68C-F0FF-4ECE-ADF0-6419DCDE9C00/downloads/wwdc2021-10245_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2021/10245/8/3C4EE68C-F0FF-4ECE-ADF0-6419DCDE9C00/downloads/wwdc2021-10245_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10008",
      "year": "2022",
      "title": "What's new in Nearby Interaction",
      "url": "https://developer.apple.com/videos/play/wwdc2022/10008"
    },
    {
      "id": "10165",
      "year": "2021",
      "title": "Explore Nearby Interaction with third-party accessories",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10165"
    },
    {
      "id": "10324",
      "year": "2021",
      "title": "Thursday@WWDC21",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10324"
    },
    {
      "id": "10668",
      "year": "2020",
      "title": "Meet Nearby Interaction",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10668"
    }
  ],
  "extractedAt": "2025-07-18T09:37:16.145Z"
}