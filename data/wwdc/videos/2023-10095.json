{
  "id": "10095",
  "year": "2023",
  "url": "https://developer.apple.com/videos/play/wwdc2023/10095/",
  "title": "Explore rendering for spatial computing",
  "speakers": [],
  "duration": "",
  "topics": [
    "Spatial Computing"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "♪ Mellow instrumental hip-hop ♪ ♪ Hello! I'm Ivan, and I'm an engineer on the RealityKit team.\n\nWelcome to my session, \"Explore rendering for spatial computing.\" RealityKit is a framework for rendering, animating, and simulating 3D models.\n\nOne of the strongest suits of RealityKit is applying realistic rendering for your content.\n\nIn order to help you make the most of the rendering abilities of RealityKit and enhance the look of your content, I wanted to share some rendering considerations to keep in mind while developing your app for spatial computing.\n\nWe'll start with lighting and shadows for your 3D content.\n\nThen we'll learn what's new with RealityKit materials.\n\nNext, I will introduce rasterization rate maps which greatly improves system performance.\n\nI will share recommendations on how to adjust your content to make it work well with this optimization.\n\nFinally, I will introduce a technique called dynamic content scaling, which ensures that the UI is always sharp.\n\nLet's start with lighting and shadows.\n\nIf you are familiar with RealityKit on iOS and macOS, you will find that most of that knowledge also applies to building spatial experiences.\n\nWe introduced image-based lighting in RealityKit to make your content look realistic.\n\nImage-based lighting, or IBL, uses textures, like the one on the right to produce realistic reflections.\n\nShadows help us understand how objects are positioned with respect to each other.\n\nBefore we look at the new features, let's quickly go over the components of image-based lighting.\n\nThere are two main components to an IBL: an Environment probe texture that is provided by ARKit and is specific to the physical space in the room and the system IBL texture which is packaged with the OS.\n\nThe system IBL texture adds extra highlights to ensure that your content looks great in any environment.\n\nThe two components are added together to produce the combined IBL texture.\n\nIf you have an active environment, it would also have an effect on the combined IBL texture.\n\nThis year RealityKit adds ability to override the system IBL texture in order to customize lighting.\n\nLet's take a look at an example.\n\nThis is the \"Hello World\" experience that offers a view of the solar system.\n\nBy default RealityKit would light it using the system IBL.\n\nHowever, if you assign a new IBL to the new image-based light component, it would replace the system IBL and light those objects using the surrounding immersive environment.\n\nLet me show you how that's done.\n\nHere we first load our 3D content.\n\nIn this case, it's the satellite model.\n\nThen we load an environment resource called Sunlight.\n\nIt contains an image of the Sun and stars surrounding the Earth.\n\nWe need both the model and the environment resource to set up IBL, so let's make sure both loading operations have finished.\n\nNext, we add the ImageBasedLightComponent.\n\nIt references the Environment resource that we've just loaded.\n\nFinally we add ImageBasedLightReceiverComponent to the satellite entity.\n\nYou can add these receiver components even to other entities in order to light them using the same IBL.\n\nAnd that's how easy it is to customize lighting in RealityKit.\n\nNext, let's take a look at how to add shadows to your application.\n\nLet's consider a simple example where you place a 3D object like this vase on top of a floating plane.\n\nWithout any shadows turned on, it might be hard to understand the relative position of the vase and the plane.\n\nBut by simply adding RealityKit's grounding shadow, it becomes a lot clearer that the vase is above the center of the plane.\n\nLet's see how to do this in code.\n\nWe start by loading the vase model.\n\nHere, flower_tulip is the name of our 3D model in our project.\n\nNext, we add the grounding shadow component.\n\nMake sure to set castsShadow flag set to true.\n\nAnd that's it! The vase entity will now cast grounding shadows.\n\nSimple, isn't it? Grounding shadows appear on top of 3D models as well as objects in the physical environment.\n\nUsing a custom IBL for lighting your scene and including grounding shadows can make your content look a lot better, but you could also directly work on the look of your objects by tweaking materials.\n\nMost of the RealityKit materials that are available on macOS and iOS can also be used on xrOS.\n\nLet's quickly review them.\n\nThe most commonly used material is PhysicallyBasedMaterial.\n\nPhysicallyBasedMaterial in RealityKit reacts to lighting and can be used to represent a variety of real-world materials, such as plastics or metals.\n\nSimpleMaterial also reacts to lighting, but uses a smaller subset of parameters.\n\nIt is especially good for quick experiments.\n\nUnlitMaterial doesn't react to lighting.\n\nIn other words, it maintains a constant look under changing lighting conditions.\n\nVideoMaterial is a variation of unlit material that can map a movie file onto the surface of an entity.\n\nIn addition to these materials, RealityKit introduces a new type of material called ShaderGraphMaterial.\n\nYou can author the new ShaderGraphMaterial in Reality Composer Pro or load it from a MaterialX file.\n\nYou can learn more about ShaderGraphMaterial in the session \"Explore Materials in Reality Composer Pro.\" The color output of all of these materials goes through a special step called tone mapping.\n\nTone mapping is a transformation that RealityKit applies by default to the color output of a material.\n\nIt enables more natural perceived colors using a variety of techniques.\n\nOne such technique is remapping values above one into the visible range.\n\nLet me demonstrate this with an example.\n\nHere's a 3D render of a TV with tone mapping disabled.\n\nI assigned a texture with very bright values to the display.\n\nNow, if I enable tone mapping, you can see more details in the bright regions, like these flower petals.\n\nTone mapping works great in general and renders beautiful visuals; but for some use cases, you may want to display the exact colors of the object, for which you will have to opt out of tone mapping.\n\nLet's look at an example.\n\nHere's a simple application that shows a traffic light and three buttons with labels \"Stop,\" \"Wait,\" and \"Go.\" The traffic light itself is a 3D model, and the three buttons were added using SwiftUI.\n\nIn order to precisely match the color of the lamp to the color of the button, we could use an unlit material for the lamps, since unlit materials maintain the same constant look of the object, independent of the lighting conditions.\n\nHowever, the output of unlit material is still affected by tone mapping which is on by default for all of RealityKit materials.\n\nSo, even if the same color is assigned to the SwiftUI button and the material of the lamp, they may appear slightly different from each other.\n\nThe screenshot you see was taken with tone mapping enabled; let me show you what it looks like when tone mapping is disabled for the lamp material.\n\nYou will notice that the colors of lamps and buttons accurately match.\n\nLet's toggle tone mapping for lamp material one more time.\n\nThis is with tone mapping enabled and this is with tone mapping disabled.\n\nLet's take a look at the code sample that shows how tone mapping can be toggled in code.\n\nWe start by loading the traffic light model.\n\nHere, traffic_light is the name of our 3D model in our project.\n\nNext, we find the entity named red_light.\n\nThis entity corresponds to the top lamp of the traffic light.\n\nOnce we have the entity, we access its model component.\n\nNext, we create a new unlit material.\n\nWe pass both our desired color and a new Boolean parameter called applyPostProcessToneMap.\n\nThis Boolean parameter is set to false in order to disable tone mapping transformation for this material.\n\nFinally, we replace material on the model component and assign the model component back to the entity.\n\nThis is done for each of the three lamps.\n\nNow the color of button and color of lamps should match closely.\n\napplyPostProcessToneMap flag is useful in cases when you want to show an exact representation of the colors in your scene.\n\nThis can come in handy when using RealityKit to build something like a menu or a heads-up display.\n\nThis new property is also exposed in the material editor of Reality Composer Pro.\n\nNow, let's take a look at some quality considerations.\n\nWe'll start with the rasterization rate maps for spatial computing.\n\nThe displays used in the headset have a high resolution, and the OS needs to update these displays many times a second.\n\nLet me explain this with a visual.\n\nAs you may know already, the headset has the ability to detect exactly where a person's eyes are looking.\n\nHere's a simulated scenario where a person moves their eyes to the right and then back to the center.\n\nThe yellow circle represents the center point of the person's focus.\n\nThe area that is surrounding that point is highlighted with a glow, and the periphery is darkened.\n\nRasterization rate map makes it so that fewer calculations are performed in the areas that are darkened.\n\nYou can see that at any given moment the highlighted region is small in comparison to the periphery.\n\nThis allows the system to achieve significant memory and performance savings.\n\nIn RealityKit, this optimization is automatically enabled for you.\n\nWhile it greatly improves the system performance, in some situations you may have to adjust your content to make it work well with this optimization at play.\n\nFor example, here's a palm leaf asset, When placed in the center of the screen, it looks sharp and detailed.\n\nBut when I move the object to the left and apply the eye movement simulation again, you can observe flickering on the palm leaf.\n\nThe flickering is especially strong when the yellow circle representing the eye direction is close to the right edge of the screen.\n\nThe flickering happens because the rasterization rate map enables higher detail around the point where the person is looking, and the pixels around the palm leaf are rendered at a lower detail as the eyes move away from it.\n\nNow, you can reduce the flickering by simply adjusting a few parameters of your content.\n\nLet's take a look at this.\n\nHere's a representation of the same palm leaf asset with a red wireframe overlay on top.\n\nYou can see that there are a lot of small triangles here.\n\nThese small triangles were the reason of flickering in the periphery.\n\nWe can reduce the flickering by simply making the triangles larger and storing the fine details in an opacity texture.\n\nHere's how the simulation looks after adjusting the asset.\n\nThis 3D model looks better after adjustment, because RealityKit automatically generates lower-resolution versions of the opacity map when the asset is loaded.\n\nThose lower-resolution versions of the texture are called mipmaps and automatically used by the GPU to improve the look in the lower-detail region.\n\nFor more details on rasterization rate maps, please refer to the article \"Rendering at Different Rasterization Rates.\" Similar to rasterization rate maps, there is another technique called \"dynamic content scaling\" that automatically improves the look of content that was authored using SwiftUI.\n\nLet's take a look.\n\nHere's an application that displays a list of months arranged in a grid.\n\nEach month is represented with a text label.\n\nWhen the eyes look at the month of June, the system rasterizes the text in that area at the highest level of detail.\n\nThe area marked in blue surrounding \"June\" will be rasterized at a slightly reduced level of detail, but still maintains a high quality overall.\n\nThe area marked in purple, however, is rasterized at a much lower level of detail since human vision system perceives fewer details in the periphery and it wouldn't be as noticeable.\n\nThis kind of rasterization at variable levels of detail based on what the eyes are looking at is called \"dynamic content scaling.\" The system relies on dynamic content scaling to draw UI content at the right scale and ensures that it's always sharp.\n\nDynamic content scaling affects the relative size in memory for the rasterized content.\n\nIn other words, our text labels are scaled to different sizes depending on how close they are to the point where the eyes are looking at.\n\nFor example, you can see that the label that says \"June\" is the largest -- it has the most resolution and detail.\n\nThen there is a group of eight months -- January, February, March, and so on that have slightly less detail.\n\nFinally, there is a group of three months -- April, August, December -- that are farthest away from eye look-at direction.\n\nThat last group would be represented with smaller images in memory.\n\nNow, let's understand how to enable dynamic content scaling.\n\nIf you are using UIKit and SwiftUI, your application will automatically benefit from this technique.\n\nIf you are relying on the Core Animation framework to build your UI, there is a new API to enable dynamic content scaling.\n\nLet's take a look at this API.\n\nDynamic content scaling can be enabled by setting the property of CALayer wantsDynamicContentScaling to true.\n\nNote that this technique relies on rasterizing at higher resolutions, so it is not recommended to use with primarily bitmap-based content.\n\nYou can find the full list of recommendations regarding dynamic content scaling on developer.apple.com.\n\nLet me summarize everything we've learned.\n\nWe started by looking at how to add image-based lights and grounding shadows to RealityKit applications.\n\nThen we reviewed materials that are available for spatial experiences, including the new ShaderGraphMaterial.\n\nAnd we've also learned how to control tone mapping for unlit material.\n\nNext we learned how rasterization rate maps are used for spatial computing, including an example how to adjust 3D model to reduce flickering in the periphery.\n\nFinally, we learned how dynamic content scaling works on the system and how you can make use of it.\n\nWe're very excited about this year's release and can't wait to see the beautiful spatial experiences you build on xrOS.\n\nThank you.\n\n♪",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "3:05",
      "title": "Image based lighting",
      "language": "swift",
      "code": "RealityView { content in\n    async let satellite = Entity(named: \"Satellite\", in: worldAssetsBundle)\n    async let environment = EnvironmentResource(named: \"Sunlight\")\n\n    if let satellite = try? await satellite, let environment = try? await environment {\n        content.add(satellite)\n\n        satellite.components.set(ImageBasedLightComponent(\n           source: .single(environment)))\n\n        satellite.components.set(ImageBasedLightReceiverComponent(\n           imageBasedLight: satellite))\n   }\n}"
    },
    {
      "timestamp": "4:28",
      "title": "Grounding shadows",
      "language": "swift",
      "code": "RealityView { content in\n    if let vase = try? await Entity(named: \"flower_tulip\") {\n        content.add(vase)\n\n        vase.components.set(GroundingShadowComponent(castsShadow: true))\n    }\n}"
    },
    {
      "timestamp": "8:48",
      "title": "Disable tone mapping",
      "language": "swift",
      "code": "RealityView { content in\n    if let trafficLight = try? await Entity(named: \"traffic_light\") {\n        content.add(trafficLight)\n\n        if let lamp = trafficLight.findEntity(named: \"red_light\") {\n            if var model = lamp.components[ModelComponent.self] {\n                let material = UnlitMaterial(color: .init(color), \n                                             applyPostProcessToneMap: false)\n\n                model.materials = [material]\n\n                lamp.components[ModelComponent.self] = model\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "15:34",
      "title": "Dynamic content scaling",
      "language": "swift",
      "code": "// Enable dynamic content scaling on CALayer with:\n\nvar wantsDynamicContentScaling: Bool { get set }"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Rendering at Different Rasterization Rates",
        "url": "https://developer.apple.com/documentation/Metal/rendering-at-different-rasterization-rates"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10095/4/CCE7B88E-E0C4-4BA3-87E7-9C1D644FA6CB/downloads/wwdc2023-10095_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10095/4/CCE7B88E-E0C4-4BA3-87E7-9C1D644FA6CB/downloads/wwdc2023-10095_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10081",
      "year": "2023",
      "title": "Enhance your spatial computing app with RealityKit",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10081"
    },
    {
      "id": "10202",
      "year": "2023",
      "title": "Explore materials in Reality Composer Pro",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10202"
    },
    {
      "id": "10100",
      "year": "2023",
      "title": "Optimize app power and performance for spatial computing",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10100"
    }
  ],
  "extractedAt": "2025-07-18T10:45:33.134Z"
}