{
  "id": "10043",
  "year": "2020",
  "url": "https://developer.apple.com/videos/play/wwdc2020/10043/",
  "title": "Build an Action Classifier with Create ML",
  "speakers": [],
  "duration": "",
  "topics": [
    "Developer Tools"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "Hello and welcome to WWDC.\n\nI am Yuxin, an engineer on the Create ML team. Today, my colleague, Alex, and I are very excited to introduce a new template: Action Classification in Create ML. Last year, we introduced Activity Classification, which allows you to create a classifier using motion data. But what if you wanted it to classify actions from videos? Cameras are ubiquitous today, and it's become so easy to record our own videos with our phones. Whether we are in a gym or at home, a self-guided workout could easily brighten our days.\n\nOther activities, such as gymnastics, reveal the complexity of human body movements. Automatic understanding of these actions could be helpful for athletes' training and competition.\n\nThis year, we built Action Classification to learn from human body poses. Look at the sequence of this amazing dance. It would be interesting to recognize these moves and even build an entertainment app around it. So, what is Action Classification? First, it's a standard classification task that has the goal to assign a label of action from a list of your predefined classes.\n\nThis year, the model is powered by Vision's human body pose estimation. And therefore, it works best with human body actions but not with actions from animals or objects.\n\nTo recognize an action over time, a single image won't be enough. Instead, the prediction happens to a time window that consists of a sequence of frames.\n\nFor a camera or video stream, predictions could be made, window by window, continuously. For more details about poses from Vision, please check our session \"Body and Hand Pose Estimation.\" Now, with Create ML, let's see how it works.\n\nAssume we'd like to create a fitness classifier to recognize a person's workout routines, such as jumping jacks, lunges, squats, and a few others. First, I need to prepare some video clips for each class, then launch Create ML for training. And finally, save the Fitness Classifier model and build our fitness training app. Now, let's dive in to each step. My colleague, Alex, will first talk about data and training for Action Classification.\n\nThanks, Yuxin. Since I can't get to my gym class right now, an exercise app which can respond to how I do workouts seems like a good way to keep fit. My name is Alex, and I'm an engineer on the Create ML team. Today I'm going to show you how to capture action videos and train a new Action Classifier model using the Create ML app. Let's start by capturing the actions on video. We want our model to be able to tell when we're exercising and which workout we're doing. We need to collect videos of each of the five workouts: jumping jacks, squats, and three others. Each video should contain just one of the action types and should have just one human subject. After taking your video, if there's extra movement at the start or end of your video, you might want to trim these off in your photo app. We identify actions using the whole body, so make sure the camera can see the arms and legs throughout the range of motion. You might need to step back a bit.\n\nNow, make a folder to hold each action type. The folder name is the label the model will predict for actions like this.\n\nIf you collected some examples of resting, stretching, and sitting down, you could put all these together in a single folder called \"other.\" We're now ready for training. But I want to take a moment to consider what to do if we have a different sort of data. Perhaps someone else prepared the video for us, or we downloaded it from the Internet. In that case, we have a montage. With a montage, a single video contains multiple different actions we need, as well as titles, establishing shots, and maybe people just resting.\n\nLooking at this video in sequence, we see periods of no action mixed with specific actions we want to train for. We have two options here. We can use video editing software to trim or split videos into the actions we need, and then put them into folders like before, or... we can find the times in the video where the actions start and stop and record those in an annotation file in CSV or JSON format. Here's an example. You can find out more in the Create ML Documentation on developer.apple.com. Let's set that aside, though, and keep going with the exercise videos we already prepared.\n\nOn my Mac, I'm going to start up the Create ML application.\n\nIf you are using Mac OS Big Sur, it has a new template: Action Classification.\n\nLet's create an Action Classification project for our exercise model.\n\nWe give it a suitable name.\n\nAnd I'm going to add a description so I can recognize it later on.\n\nOur new Action Classification project opens on the Model Source Settings page. This is where we prepare for training, by adding the data we captured and making some decisions about the training process. We're going to need to drag the folder of videos we already prepared into the training data source. Let's take a look at that now.\n\nI've collected all the videos in a \"train\" folder, which has sub-folders named for each of the actions we want to train. Let's have a look in the \"jacks\" folder. It contains all of the videos of jumping jacks.\n\nLet's drag that \"train\" folder into the training data well. The Create ML app checks the data's in the right format and tells us a little about it. It says we've recorded seven classes. That's the five exercises plus two negative classes: \"other\" and \"none.\" It also says we recorded 362 items. Videos. We can dive into this using the data source view.\n\nHere we can see that each action has around 50 videos.\n\nThat's what you should aim for when building your project.\n\nBelow the data inputs are some parameters we can set. There's two I want to talk about here: Action Duration and Augmentations. Action Duration is a really important parameter. It represents the length of the action we're trying to recognize. If we're working with a complex action, like a dance move, this needs to be long enough to capture the whole motion, which might be ten seconds.\n\nBut for short, repeating actions, like our exercise reps, we should set the window to around two seconds.\n\nThe length of the action, also known as prediction window, is actually measured in frames, so Create ML automatically calculates the number of frames, 60, based upon the frame rate and action duration.\n\nAugmentations are a way to boost your training data, without going out and recording more videos, by transforming the ones you already have in ways which represent real-world scenarios. If all of our videos were taken with a person facing to the left, then \"horizontal flip\" generates a mirror-image video that make the model work better in both orientations. Let's turn it on.\n\nYou'll have noticed two other boxes you can add data to. These are Validation and Testing. If you have extra data set aside to test your model, you can add that now to the testing data well. Create ML will automatically perform tests on it when the model is done training.\n\nMachine learning veterans might like to choose their own validation data, but, by default, Create ML will automatically use some of your training data for this. Let's hit the \"train\" button to begin making our model.\n\nWhen your Mac is training an Action Classifier from videos, it does this in two parts. First, it does feature extraction, which takes the videos and works out the human poses. Once that's complete, it can train the model.\n\nFeature extraction is a big task and can take a while, so let's dig a little deeper.\n\nUsing the power of the Vision API, we look at every frame of our video and encode the position of 18 landmarks on the body, including hands, legs, hips, eyes, etcetera. For each landmark, it records x and y coordinates, plus a confidence, and these form the feature that we use for training. Let's go back and find out how it's getting on. The feature extraction is going to take about half an hour, and we don't want to wait, so I'm going to stop this training process and open a project with a model that we made earlier.  This model's already finished training on the same data that we used before. I also added some validation data. You can see the final report in the training and evaluation tabs.\n\nWe can see how the model performance improved over time. It looks like 80 iterations was a reasonable choice. The line going up and to the right and then flattening out means that training has reached a stable state.\n\nI can review the performance per class using the evaluation tab. This lets me check that the model will perform equally well across each kind of action we want to recognize. If we added validation or test data, you could see the results here.\n\nBut what I really want is to see the model in action. Let's go to the preview tab, where we can try the model out on new videos we haven't used for training. Here's one I recorded in my garden yesterday.\n\nThe video is processed to find the poses, and then the model classifies these into actions. Right now, it's classifying the whole video up front, but when you make your own app, you can choose to stream it if you're working with live videos or want a responsive experience. Let's press \"play\" to see what it's detected. Look out for the label on the video. You can see the classification change as the action in the video unfolds.\n\nLet's see it one more time. You can see that pose skeleton superimposed over the video. We can turn that off.\n\nOr we can just watch the pose skeleton exercising in the dark.\n\nBelow the video, we can see the timeline, which has divided the video into two-second windows. Remember when we chose that? And it shows the best prediction for each window underneath, as well as other predictions with lower probability.\n\nI think we're ready to make a great app using this model. For this, I need to export the model from the project. And we can do this on the output tab. On the output tab, we can also see some facts about the model, including the size, which is an important consideration for assets in mobile apps, and you can find out what operating system versions this model is supported on. Let's save the model by dragging the icon into the finder.\n\nFitnessClassifier.MLmodel.\n\nAnd now we can share it with Yuxin, who's going to show us how to build an awesome iOS fitness app. Now we've got a classifier from Alex to drive our fitness training app. Let's first check out how to use the model to make a prediction. For example, we'd like to recognize jumping jacks either from a camera or a video file.\n\nThe model, however, takes poses, rather than a video, as the input. To extract poses, we use Vision's API: VNDetectHumanBodyPoseRequest.\n\nIf we are working with a video URL, VNVideoProcessor could handle the pose request for an entire video, and pose results for every frame can be obtained in a completion handler.\n\nAlternatively, when working with a camera stream, we could use VNImageRequestHandler to perform the same pose request for each captured image.\n\nRegardless of how we get poses from each frame, we need to aggregate them into a prediction window in a three-dimensional array as the model input.\n\nThis looks complicated, but if we use the convenient keypointsMultiArray API from Vision, we don't have to deal with the details. Since our fitness model's window size is 60, we could simply concatenate poses from 60 frames to make one prediction window.\n\nWith such a window prepared, it can be passed to our model as the input. And finally, the model output result includes a top predicted action name and a confidence score. Now let's take a tour of these steps in Xcode.\n\nThis is our fitness training app. Let's open it.\n\nAnd this is the Fitness Classifier we've just trained. This Metadata page shows us relevant user and model information, such as the author, description, class label names, and even layer distributions.\n\nThe Predictions page shows us detailed model input and output information, such as the required input multi-array dimensions, as well as the names and types for the output. Now let's skip the other app logic and jump right to the model prediction.\n\nIn my predictor, I first initialize the Fitness Classifier model, as well as the Vision body pose request. It's better to do this only once.\n\nSince the model takes a prediction window as the input, I also maintain a window buffer to save the last 60 poses from the past two seconds. Sixty is the model's prediction window size we have used for training.\n\nWhen a frame is received from the camera, we need to extract the poses.\n\nThis involves calling Vision APIs and performing pose requests that we have already seen on the slides. Before we add the extracted poses to our window... remember that Action Classifier takes only a single person, so we need to implement some person-selection logic if Vision detects multiple people. Here in this app, I simply choose the most prominent person based on their size, and you may choose to do it in other ways. Now, let's add the pose to the window.\n\nAnd once the window is full, we can start making predictions. This app makes a prediction every half a second, but you can decide a different interval or trigger based on your own use cases. Now let's move on to make a model prediction.\n\nTo prepare the model input, I need to convert each pose from the window into a multi-array using Vision's convenient API, keypointsMultiArray. And if no person is detected, simply pad zeros.\n\nThen we concatenate 60 of these multi-arrays into a single array, and that's our model input.\n\nFinally, it's just the one line of code to make a prediction.\n\nThe output includes a top-predicted label name and the confidences in a dictionary for all the classes. In the end, don't forget to reset the pose window so that once it is filled again, we're ready to make another prediction.\n\nAnd that's everything about making predictions with an Action Classifier. Now let's try out this app and see how it works in action.\n\nI feel some extra energy today and would like to challenge myself to a little workout. Let's open the app... and hit the \"start\" button to begin a workout. Now my poses are extracted from every incoming frame, and predictions are made continuously, as displayed at the bottom of the app. That's our Debugging View: confidence and the labels. Now, let me get started.\n\nWow. I finished a five-second challenge. As you may have seen, as soon as the model recognized my action, the timer starts, and as soon as I stopped, the model recognized my action as \"other action\" class now and starts to wait for next challenge, lunges. I'm ready.\n\nI finished lunges, but I am not in a hurry yet and would like to take a rest and drink some water.\n\nEverything happens interactively... and I don't have to get back and operate a device, which is very convenient for home exercises.\n\nLastly, let's try squats.\n\nNow all three challenges are finished, and here is the summary of my time spent for each. Wow. That's a lot of exercise for today.\n\nNow we have covered how to train an Action Classifier in making predictions. Let's invite Alex back and wrap up this session with some best practices. Thanks, Yuxin. I love the way this application waits for me to start my workout. I often need time to remember how an exercise is done and maybe which arm or leg is moved first. Building great features like that needs great data, so let's make sure we get the best performance out of the athletes, gamers and kids who bring our videos to life. Your model will train best if it's exposed to repetition and variety.\n\nEarlier, I made sure I had around 50 videos for each exercise I wanted to classify, and you should, too, in your apps. Make sure you train with a variety of different people. They'll bring different styles, abilities and speeds, which your model needs to recognize. If you think your users will move around a lot, consider capturing the action from the sides and back as well as the front.\n\nThe model does need to understand exercises, but it also needs to understand when we're not exercising, or not moving at all. You could create two extra folders: one for walking around and stretching, and one for sitting or doing nothing quietly.\n\nLet's consider how to capture great videos for the Action Classifier. Any motion from the camera person might be interpreted as the subject moving around, so let's keep the camera stable using a tripod or resting it somewhere solid.\n\nThe pose detector needs to clearly see the parts of the body, so if your outfit blends into the background, it won't work as well, and flowing clothing might conceal the movement you're trying to detect.\n\nNow you know how to take the best videos for your Action Classifier. Yuxin, can you tell us how to get the most out of training in Create ML? Sure, Alex. Once you have the data, take a moment to configure your training parameters. One key parameter is action duration, in seconds, or prediction window size, which is number of frames in your time window. The length should match the action length in your videos, and try to make all actions roughly at the same duration.\n\nFrame rate in your video affects the effective length of the prediction window... so it's important to keep the average frame rate consistent between your training and testing videos to get accurate results.\n\nWhen it comes to using the model in your applications, make sure to only select a single person. Your app may remind users to keep only one person in view when multiple people are detected, or you can implement your own selection logic to choose a person based on their size or location within the frame, and this can be achieved by using the coordinates from pose landmarks.\n\nIf you'd like to count the repetitions of your actions, you may need to prepare each training video to be just one repetition of a single action. And when you make predictions in your app, you should find a reliable trigger to start a prediction at the right time or implement some smoothing logic to properly update the counter.\n\nFinally, you can use an Action Classifier to score or judge the quality of an action. For example, your app could use a prediction's confidence value to score the quality of an action compared to the example actions in your training videos.\n\nSo, that's Action Classification in Create ML. We can't wait to see the amazing apps you are going to create using it. Thanks for joining us today,\nand enjoy the rest of WWDC.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "5:28",
      "title": "Working with montage videos",
      "language": "swift",
      "code": "[ \n    {\n        \"file_name\": \"Montage1.mov\",\n        \"label\": \"Squats\",\n        \"start_time\": 4.5,\n        \"end_time\": 8\n    }\n]"
    },
    {
      "timestamp": "14:05",
      "title": "Getting poses",
      "language": "swift",
      "code": "import Vision\nlet request = VNDetectHumanBodyPoseRequest()"
    },
    {
      "timestamp": "14:10",
      "title": "Getting poses from a video",
      "language": "swift",
      "code": "import Vision\nlet videoURL = URL(fileURLWithPath: \"your-video-file.MOV\")\nlet startTime = CMTime.zero\nlet endTime = CMTime.indefinite\n\nlet request = VNDetectHumanBodyPoseRequest(completionHandler: { request, error in\n    let poses = request.results as! [VNRecognizedPointsObservation]\n})\n\nlet processor = VNVideoProcessor(url: videoURL)\ntry processor.add(request)\ntry processor.analyze(with: CMTimeRange(start: startTime, end: endTime))"
    },
    {
      "timestamp": "14:26",
      "title": "Getting poses from an image",
      "language": "swift",
      "code": "import Vision\nlet request = VNDetectHumanBodyPoseRequest()\n// Use either one from image URL, CVPixelBuffer, CMSampleBuffer, CGImage, CIImage, etc. in image request handler, based on the context.\nlet handler = VNImageRequestHandler(url: URL(fileURLWithPath: \"your-image.jpg\"))\n\ntry handler.perform([request])\nlet poses = request.results as! [VNRecognizedPointsObservation]"
    },
    {
      "timestamp": "14:57",
      "title": "Making a prediction",
      "language": "swift",
      "code": "import Vision\nimport CoreML\n\n// Assume pose1, pose2, ..., have been obtained from a video file or camera stream.\nlet pose1: VNRecognizedPointsObservation\nlet pose2: VNRecognizedPointsObservation\n// ...\n\n// Get a [1, 3, 18] dimension multi-array for each frame\nlet poseArray1 = try pose1.keypointsMultiArray()\nlet poseArray2 = try pose2.keypointsMultiArray()\n// ...\n\n// Get a [60, 3, 18] dimension prediction window from 60 frames\nlet modelInput = MLMultiArray(concatenating: [poseArray1, poseArray2], axis: 0, dataType: .float)"
    },
    {
      "timestamp": "16:27",
      "title": "Demo: Building the app in Xcode",
      "language": "swift",
      "code": "import Foundation\nimport CoreML\nimport Vision\n\n@available(iOS 14.0, *)\nclass Predictor {\n    /// Fitness classifier model.\n    let fitnessClassifier = FitnessClassifier()\n\n    /// Vision body pose request.\n    let humanBodyPoseRequest = VNDetectHumanBodyPoseRequest()\n\n    /// A rotation window to save the last 60 poses from past 2 seconds.\n    var posesWindow: [VNRecognizedPointsObservation?] = []\n    init() {\n        posesWindow.reserveCapacity(predictionWindowSize)\n    }\n\n    /// Extracts poses from a frame.\n    func processFrame(_ samplebuffer: CMSampleBuffer) throws -> [VNRecognizedPointsObservation] {\n        // Perform Vision body pose request\n        let framePoses = extractPoses(from: samplebuffer)\n\n        // Select the most promiment person.\n        let pose = try selectMostProminentPerson(from: framePoses)\n\n        // Add the pose to window\n        posesWindow.append(pose)\n\n        return framePoses\n    }\n\n    // Make a prediction when window is full, periodically\n    var isReadyToMakePrediction: Bool {\n        posesWindow.count == predictionWindowSize\n    }\n\n    /// Make a model prediction on a window.\n    func makePrediction() throws -> PredictionOutput {\n        // Prepare model input: convert each pose to a multi-array, and concatenate multi-arrays.\n        let poseMultiArrays: [MLMultiArray] = try posesWindow.map { person in\n            guard let person = person else {\n                // Pad 0s when no person detected.\n                return zeroPaddedMultiArray()\n            }\n            return try person.keypointsMultiArray()\n        }\n\n        let modelInput = MLMultiArray(concatenating: poseMultiArrays, axis: 0, dataType: .float)\n\n        // Perform prediction\n        let predictions = try fitnessClassifier.prediction(poses: modelInput)\n\n        // Reset poses window\n        posesWindow.removeFirst(predictionInterval)\n\n        return (\n            label: predictions.label,\n            confidence: predictions.labelProbabilities[predictions.label]!\n        )\n    }\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Building a feature-rich app for sports analysis",
        "url": "https://developer.apple.com/documentation/Vision/building-a-feature-rich-app-for-sports-analysis"
      },
      {
        "title": "Create ML",
        "url": "https://developer.apple.com/documentation/CreateML"
      },
      {
        "title": "Creating an Action Classifier Model",
        "url": "https://developer.apple.com/documentation/CreateML/creating-an-action-classifier-model"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10043/3/D1F0FA14-5265-4AB7-9D8D-118CA6F3E162/wwdc2020_10043_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10043/3/D1F0FA14-5265-4AB7-9D8D-118CA6F3E162/wwdc2020_10043_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10020",
      "year": "2022",
      "title": "Compose advanced models with Create ML Components",
      "url": "https://developer.apple.com/videos/play/wwdc2022/10020"
    },
    {
      "id": "10039",
      "year": "2021",
      "title": "Classify hand poses and actions with Create ML",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10039"
    },
    {
      "id": "10156",
      "year": "2020",
      "title": "Control training in Create ML with Swift",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10156"
    },
    {
      "id": "10653",
      "year": "2020",
      "title": "Detect Body and Hand Pose with Vision",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10653"
    },
    {
      "id": "10099",
      "year": "2020",
      "title": "Explore the Action & Vision app",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10099"
    },
    {
      "id": "430",
      "year": "2019",
      "title": "Introducing the Create ML App",
      "url": "https://developer.apple.com/videos/play/wwdc2019/430"
    }
  ],
  "extractedAt": "2025-07-18T10:15:18.899Z"
}