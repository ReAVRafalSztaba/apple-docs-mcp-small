{
  "id": "10153",
  "year": "2021",
  "url": "https://developer.apple.com/videos/play/wwdc2021/10153/",
  "title": "Create image processing apps powered by Apple silicon",
  "speakers": [],
  "duration": "",
  "topics": [
    "Graphics & Games"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "♪ Bass music playing ♪  ♪ Eugene Zhidkov: Hi and welcome to WWDC.\n\nMy name is Eugene Zhidkov. I am from GPU software.\n\nAnd together, with Harsh Patil from Mac system architecture, we'll show you how to create image-processing applications powered by Metal on Apple silicon.\n\nFirst, I will focus on the best practices and lessons learned, optimizing image-processing applications for M1 based on developer engagements we have had over the last year.\n\nAnd then Harsh will give you a step-by-step guide to how you can redesign your image-processing pipeline for optimal performance on Apple silicon.\n\nSo let’s jump right in! To start, let’s briefly revisit Apple system on-the-chip architecture and its benefits.\n\nMany image-processing and video-editing apps are designed with discrete GPUs in mind.\n\nSo it’s important to highlight what’s so different about Apple GPUs.\n\nFirst, all Apple chips use Unified Memory Architecture.\n\nAll blocks -- such as CPU, GPU, Neural and Media engines -- have access to the same system memory using unified memory interface.\n\nAnd second, our GPUs are Tile Based Deferred Renderers, or TBDRs.\n\nTBDRs have two main phases: tiling, where whole render surfaces split into tiles and processed geometry is then handled independently; and rendering, where all of the pixels will be processed for each tile.\n\nSo in order to be most efficient on Apple silicon, your image-processing app should start leveraging unified memory -- to avoid any copies your pipeline used to have -- and TBDR architecture by exploiting tile memory and local image block.\n\nTo learn more about how Apple TBDR works at low level and how to target our shader core, please watch these sessions from the last year.\n\nAnd now, let’s talk about the exact things we are going to do to optimize image-processing compute workloads for Apple silicon.\n\nLast year, we’ve been working closely with many great developers on their image pipeline transitions.\n\nWe picked the six most rewarding tips to share.\n\nFirst, we’ll discuss how to avoid unnecessary memory copies or blits.\n\nThis is really important given we are now working with images up to 8K.\n\nThen, we wanted to highlight the benefits of using render pipeline and textures instead of using compute on buffers and how you could do that in your own image-processing pipeline.\n\nOnce we have the render and textures paths up and running, we wanted to show you the importance of proper load/store actions and memoryless attachments.\n\nThis will help you getting the most out of the tile memory.\n\nThen, we’ll talk how to best approach Uber-shaders with its dynamic control flow and also how to leverage smaller data types -- such as short and half -- to improve performance and efficiency.\n\nAnd we’ll finish with important advice about texture formats to get the best throughput.\n\nAll right.\n\nSo let’s get started with one of the most rewarding tips: avoiding unneeded blits on Apple silicon.\n\nMost image-processing apps are designed around discrete GPUs.\n\nWith discrete GPUs, you have separate system memory and video memory.\n\nTo make the frame image visible or resident to the GPU, explicit copy is required.\n\nMoreover, it is usually required twice; to upload the data for the GPU to process it, and to pull it back.\n\nLet’s consider we are decoding an 8K video, processing it, and saving it to disk.\n\nSo this is a CPU thread, decoding, in this case.\n\nThat’s where we need to copy the decoded frame to GPU VRAM.\n\nAnd here is GPU timeline, where all the effects and filters are applied.\n\nLet’s take it one step further and let’s recall we need to save the results to disk, right? So we must also consider bringing the processed frame back to system memory and the actual encoding of the frame.\n\nSo, these are known as \"copy\" or \"blit gaps\", and advanced image-processing applications had to do deep pipelining and other smart things to fill them in.\n\nWell, the good news is that on Apple GPUs, blitting for the sake of residence is no longer needed.\n\nSince memory is shared, both the CPU and GPU can access it directly.\n\nSo please add a simple check to detect if you are running on unified memory system and avoid unnecessary copies.\n\nIt will save you memory, time, and is an absolute first step to do.\n\nSo this is where we land on Unified Memory Architecture with the blits removed.\n\nBy removing the blits, we completely avoid copy gaps and can start processing immediately.\n\nThis also gives better CPU and GPU pipelining with less hassle.\n\nLet’s make sure you implement unified memory path with no copies involved.\n\nIf you just leave blit copies exactly as it was on the discrete GPU, you’ll pay with system memory bandwidth, less GPU time for actual processing, and potential scheduling overhead.\n\nNot to mention we no longer need separate VRAM image allocated.\n\nGPU frame capture can help you with spotting large blits.\n\nPlease inspect your application blits and make sure you only do the copies required.\n\nNow, let’s talk about how exactly we should start leveraging Apple GPU TBDR architecture for image processing.\n\nMost image-processing applications operate on image buffers by dispatching series of compute kernels.\n\nWhen you dispatch a compute kernel in default serial mode, Metal guarantees that all subsequent dispatches see all the memory writes.\n\nThis guarantee implies memory coherency for all shader cores, so every memory write is made visible to all other cores by the time the next dispatch starts.\n\nThis also means memory traffic could be really high; the whole image has to be read and written to.\n\nWith M1, Apple GPUs enable tile dispatches on MacOS.\n\nIn contrast to regular compute, they operate in tile memory with tile-only sync points.\n\nSome filters -- like convolutions -- cannot be mapped to the tile paradigm, but many other filters can! Deferring system memory flush until the encoder end point provides solid efficiency gains.\n\nYou can execute more useful GPU work when not limited by system memory bandwidth.\n\nTo take it even further, let’s notice that many per-pixel operations don’t require access to neighboring pixels, so tile sync point is not necessary.\n\nThis maps really well to fragment functions.\n\nFragment functions can be executed without implicit tile sync, requiring sync only at the encoder boundary or when tile kernels are dispatched serially after the fragment kernels.\n\nWe now learned that Apple GPUs enable fragment functions and tile kernels for more efficient image processing.\n\nLet’s see how we could use that.\n\nWe do that by converting regular compute dispatches on buffers to render command encoder on textures.\n\nAs we just discussed, rule of thumb is the following.\n\nPer-pixel operations with no interpixel dependency should be implemented using fragment functions.\n\nAny filter with threadgroup scoped operations should be implemented with tile shading, since neighbor pixels access within a tile is required.\n\nScatter-gather and convolution filters cannot be mapped to tile paradigm since they require random access, so these should still remain compute dispatches.\n\nRender command encoder also enables a unique Apple GPU feature: lossless bandwidth compression for textures and render targets.\n\nThis is a really great bandwidth saver, especially for an image-processing pipeline, so let’s see how we should use it.\n\nWell, speaking of enabling lossless compression, it’s actually easier to say what you should not do.\n\nFirst, already-compressed texture formats cannot benefit from lossless.\n\nSecond, there are three particular texture flags which cannot work with this compression, so make sure you don’t set them just by an accident.\n\nAnd third, linear textures -- or backed by an MTLBuffer -- are not allowed as well.\n\nSome special treatment is also required for nonprivate textures; make sure to call optimizeContentsForGPUAccess to stay on the fastest path.\n\nGPU frame capture Summary pane now shows you lossless compression warnings and highlights the reasons why the texture has opted out.\n\nIn this example, PixelFormatView flag was set.\n\nIn many cases, developers are setting these flags unintentionally.\n\nDon’t set PixelFormatView if all you need is components swizzle or sRGB conversion.\n\nAll right, we have the render and textures path up and running.\n\nNow, let’s make sure we properly use tile memory.\n\nTile memory TBDR concepts -- such as load/store actions and memoryless attachments -- are totally new to the desktop world.\n\nSo let’s make sure we use them properly.\n\nLet’s start with load/store actions! As we already know, the whole render target is split into the tiles.\n\nLoad/store are per-tile bulk actions guaranteed to take the most optimal path through memory hierarchy.\n\nThey are executed at the beginning of the render pass -- where we tell the GPU how to initialize the tile memory -- and at the end of the pass to inform the GPU what attachments need to be written back.\n\nThe key thing here is to avoid loading what we don’t need.\n\nIf we are overwriting the whole image, or the resource is temporary, set load action to LoadActionDontCare.\n\nWith render encoder, you no longer need to clear your output or temporary data, as you probably did before with dedicated compute pass or fillBuffer call.\n\nBy setting LoadActionClear, you can efficiently specify the clear value.\n\nAnd the same goes for the store action.\n\nMake sure to only store the data you will later need -- like the main attachment -- and don’t store anything temporary.\n\nBesides explicit load and store actions, Apple GPUs saves your memory footprint with memoryless attachments.\n\nWe can explicitly define an attachment as having memoryless storage mode.\n\nThis enables tile-only memory allocation, meaning that your resource will persist for each and every tile only within encoder lifetime.\n\nThis can greatly reduce your memory footprint, especially for 6K/8K images, where every frame takes hundreds of megabytes.\n\nLet’s see how this all can be done in code.\n\nWe start by creating the textureDescriptor and then create the outputTexture.\n\nWe then create a temporary texture.\n\nNotice that I’ve marked it memoryless, as we don’t want any storage here.\n\nThen we create the render pass by first describing what the attachments are and then what are the load/store actions.\n\nWe don’t care about loading the output since it is fully overwritten, but we need to store it.\n\nAs for the temporary texture, we don’t load but clear it, and we don’t need to store it either.\n\nFinally, we create our renderPass from the descriptor.\n\nThat’s it.\n\nSo we are using unified memory, moved our image-processing pipeline to render command encoder, and are properly leveraging tile memory.\n\nNow, let’s talk about uber-shaders.\n\nUber-shaders, or uber-kernels, is a pretty popular way to make developers' life easier.\n\nHost code sets up the control structure, and shader just loops through a series of if/else statements, for example, if tone mapping is enabled or if the input is in HDR or SDR formats.\n\nThis approach is also known as \"ubers-shader\" and is really good at bringing total number of pipeline state objects down.\n\nHowever, it has drawbacks.\n\nThe main one is increased register pressure to keep up with more complex control flow.\n\nUsing more registers can easily limit maximum occupancy your shader is running at.\n\nConsider a simple kernel where we pass in the control struct.\n\nWe use flags inside the struct to control what we do.\n\nWe have two features here: if the input is in HDR and if tonemapping is enabled.\n\nAll look good, right? Well, here is what happens on the GPU.\n\nSince we cannot deduce anything at compile time, we have to assume we could take both paths -- HDR and non-HDR -- and then combine based on the flag.\n\nSame goes for tone mapping.\n\nWe evaluate it and then mask it in or out, based on the input flag.\n\nThe problem here is registers.\n\nEvery control flow path needs live registers.\n\nThis is where uber-shaders are not so good.\n\nAs you recall, registers used by the kernel define maximum occupancy the shader could run.\n\nThat happens because registers file is shared by all simdlanes on the shader core.\n\nIf we could only run what’s only needed, that would enable higher simdgroup concurrency and GPU utilization.\n\nLet’s talk how to fix this.\n\nMetal API has the right tool for the job, and it's called \"function_constants\".\n\nWe define both control parameters as function_constants, and we modify the code accordingly.\n\nHere, we are showing the modified kernel code.\n\nHost side must be also updated to provide function_constant value at pipeline creation time.\n\nAnother great way to reduce register pressure is using 16-bit types in your shaders.\n\nApple GPUs have native 16-bit type support.\n\nSo, when using smaller data types, your shaders will require less registers, increasing occupancy.\n\nHalf and short types also require less energy and might achieve higher peak rates.\n\nSo, please use half and short types instead of float and int when possible, since type conversions are usually free.\n\nIn this example, consider a kernel using the thread_position in threadgroup for some computations.\n\nWe are using unsigned int, but the maximum threadgroup size supported by Metal can easily fit in unsigned short.\n\nthreadgroup_position_in_grid, however, could potentially require a larger data type.\n\nBut for the grid sizes we’re using in image processing -- up to 8K or 16K -- unsigned short is also enough.\n\nIf we use 16-bit types instead, the resulting code will use a smaller number of registers, potentially increasing the occupancy.\n\nNow, let me show you where you can have all the details on registers.\n\nGPU frame debugger in Xcode13 now has advanced pipeline state object view for render, tile, and compute PSOs.\n\nYou can inspect detailed pipeline statistics -- now with registers usage -- and fine-tune all your shaders.\n\nWith register concerns covered, let’s talk about texture formats.\n\nFirst, we want to note that different pixel formats might have different sampling rates.\n\nDepending on hardware generation and number of channels, wider floating-point types might have reduced point sampling rate.\n\nEspecially floating-point formats such as RGBA32F will be slower than FP16 variants when sampling filtered values.\n\nSmaller types reduce memory storage, bandwidth, and cache footprint as well.\n\nSo we encourage, again, to use the smallest type possible, but in this case, for the textures storage.\n\nThis was actually a common case for 3D LUTs in image processing; most applications we worked with were using float RGBA for a 3D LUT application phase with bilinear filtering enabled.\n\nPlease consider if your app can instead use halfs and the precision will be enough.\n\nIf that’s the case, switch to FP16 right away to get peak sampling rates.\n\nIf half precision is not enough, we found out that fixed-point unsigned short provides great uniform range of values, so encoding your LUTs in unit scale and providing LUT range to the shader was a great way to get both peak sampling rate and sufficient numerical accuracy.\n\nAll right, so we just went over how we should leverage Apple GPU architecture to make your image-processing pipeline run as efficient as possible.\n\nTo apply it all right away, please meet Harsh! Harsh Patil: Thanks, Eugene.\n\nNow let’s walk through redesigning an image-processing pipeline for Apple silicon based on all the best practices we have learned so far.\n\nTo be specific, we are going to tailor the image-processing phase of the video-processing pipeline for Apple GPUs.\n\nReal-time image processing is very GPU compute and memory bandwidth intensive.\n\nWe will first understand how it is usually designed and then how we can optimize it for Apple silicon.\n\nWe are not going to go into the details of video-editing workflow in this section, so please refer to our talk from two years ago.\n\nWe will solely focus on transitioning the compute part of image processing to render path.\n\nBefore we start, let's begin quickly take a look at where image-processing phase stands in a typical video-processing pipeline.\n\nWe'll take ProRes-encoded input file as an example.\n\nWe first read the ProRes-encoded frame from the disk or external storage.\n\nWe then decode the frame on CPU, and now the image-processing phase executes on this decoded frame on the GPU and renders the final output frame.\n\nFinally, we display this output frame.\n\nWe could additionally also encode the final rendered frame for delivery.\n\nNext, let’s take a look at what comprises an image-processing pipeline.\n\nImage processing starts with unpacking different channels of the source image RGB in alpha into separate buffers in the beginning.\n\nWe will process each of these channels in our image-processing pipeline, either together or separately.\n\nNext, there might be color space conversions to operate in the desired color-managed environment.\n\nWe then apply a 3D LUT; perform color corrections; and then apply spatial-temporal noise reduction, convolutions, blurs, and other effects.\n\nAnd finally, we pack the individually processed channels together for final output.\n\nWhat do these selected steps have in common? They are all point filters, operating only on a single pixel with no interpixel dependency.\n\nThese map well to fragment shader implementation.\n\nSpatial and convolution-style operations require access to large radius of pixels, and we have scattered read-write access patterns as well.\n\nThese are well-suited for compute kernels.\n\nWe’ll use this knowledge later.\n\nFor now, let’s see how these operations are executed.\n\nApplications represent chain of effects applied to an image as a filter graph.\n\nEvery filter is its own kernel, processing the inputs from the previous stage and producing outputs for the next stage.\n\nEvery arrow here means a buffer being written to/from output of one stage and read as the input in the next stage.\n\nSince memory is limited, applications usually linearize the graph by doing a topological sort.\n\nThis is done to keep the total number of intermediate resources as low as possible while also avoiding race conditions.\n\nThis simple filter graph in that example would need two intermediate buffers to be able to operate without race conditions and produce the final output.\n\nThe linearized graph here roughly represents the GPU command buffer encoding as well.\n\nLet’s look deeper on why this filter graph is very device memory bandwidth intensive.\n\nEvery filter operation has to load whole image from device memory into the registers and write the result back to the device memory.\n\nAnd that’s quite a bit of memory traffic.\n\nLet’s estimate the memory footprint for a 4K-frame image processing based on our example image-processing graph.\n\nA 4K decoded frame itself takes 67 megabytes of memory for floating-point 16 precision or 135 megabytes of memory for floating-point 32 precision, and professional workflows absolutely need floating-point 32 precision.\n\nFor processing one 4K frame in floating-point 32 precision through this image-processing graph, we are talking more than two gigabytes of read-write traffic to device memory.\n\nAlso, writes to buffers holding the intermediate output thrashes the cache hierarchy and impacts other blocks on the chip as well.\n\nRegular compute kernels don’t benefit from the on-chip tile memory implicitly.\n\nKernels can explicitly allocate threadgroup-scoped memory, which will be backed by the on-chip tile memory.\n\nHowever, that tile memory is not persistent across dispatches within a compute encoder.\n\nIn contrast, the tile memory is actually persistent across draw passes within one render command encoder.\n\nLet’s see how we can redesign this representative image-processing pipeline to leverage the tile memory.\n\nWe are going to address this by following three steps.\n\nWe first change the compute pass to render pass and all the intermediate output buffers to textures.\n\nWe then encode per-pixel operations with no interpixel dependency as fragment shader invocations within one render command encoder, making sure to account for all the intermediate results and setting appropriate load/store actions.\n\nAnd finally, we discuss what do we do in more complex situation than just point filters.\n\nOur first step is to use separate MTLRenderCommandEncoder to encode eligible shaders.\n\nIn this filter graph, unpack, color space conversion, LUT, and color-correction filters are all point per-pixel filters that we can convert to fragment shader and encode them using one render command encoder.\n\nSimilarly, mixer and pack shaders -- which are towards the end of this image-processing pipeline -- can also be converted to fragment shaders and encoded using another MTLRenderCommandEncoder.\n\nThen we can invoke these shaders within their respective render passes.\n\nWhen you create the render pass, all the resources attached to the color attachments in that render pass are implicitly tiled for you.\n\nA fragment shader can only update the image block data associated with fragment’s position in the tile.\n\nNext shader in the same render pass can pick up the output of the previous shader directly from the tile memory.\n\nIn the next section, we will take a look at how we can structure the fragment shaders which map to these filters.\n\nWe will also take a look at what constructs we need to define and use to enable access to the underlying tile memory from within these fragment shaders.\n\nAnd finally, we will take a look at how the output generated in the tile memory by one fragment shader can be consumed directly from the tile memory by the next fragment shader within the same render command encoder.\n\nThis is what you have to do in your code.\n\nHere I have attached output image as a texture attached to color attachment 0 of the render pass descriptor.\n\nI have attached texture holding intermediate result to color attachment 1 of the render pass descriptor.\n\nBoth of these will be implicitly tiled for you.\n\nPlease set the appropriate load/store properties as discussed earlier in the talk.\n\nNow, set up a structure to access these textures in your fragment shader.\n\nIn the coming examples, we will show how to use this structure within your fragment shaders.\n\nYou simply access the output and the intermediate textures within your fragment shader as highlighted by using the structure we defined earlier.\n\nWrites to these textures are done to appropriate tile memory location corresponding to the fragment.\n\nOutput produced by the unpack shader is consumed as input by color space conversion shader using the same structure that we defined earlier.\n\nThis fragment shader can do its own processing and update the output and intermediate textures which will, once again, update the corresponding tile memory location.\n\nYou are to continue the same steps for all the other fragment shaders within the same render encoder pass.\n\nNext, let's visualize how this sequence of operations looks now with these changes.\n\nAs you can see, now you have unpack, color space conversion, application of 3D LUT, and color-correction steps, all executed on the tile memory using one render pass with no device memory passes in between.\n\nAt the end of the render pass, render targets that are not memoryless are flushed to the device memory.\n\nYou can then execute the next class of filters.\n\nLet’s talk a bit about filters that have scatter-gather access patterns.\n\nKernels representing such filters can directly operate on the data in the device memory.\n\nConvolution filters are very well-suited for tile-based operations in compute kernels.\n\nHere, you can express intent to use tile memory by declaring a threadgroup-scoped memory.\n\nNow, you bring in the block of pixels into the tile memory along with all the necessary halo pixels, depending upon the filter radius, and perform the convolution operation directly on the tile memory.\n\nRemember, tile memory is not persistent across compute dispatches within a compute encoder.\n\nSo after executing Filter1, you have to explicitly flush the tile memory contents to device memory.\n\nThat way, Filter2 can consume the output of Filter1.\n\nSo where do we land once we make all of these changes? For processing one 4K frame in floating-point 32 precision through our example restructured image-processing graph, here’s what we have now.\n\nBandwidth goes down from 2.16 gigabytes to just load and store worth 810 megabytes, and that’s 62 percent reduction in memory traffic to the device memory.\n\nWe don't need two intermediate device buffers saving 270 megabytes of memory per frame.\n\nAnd finally, we have reduced cache thrashing, and that's because all the fragment shaders within that render pass are operating directly on the tile memory.\n\nOne of the key features of Apple silicon is its Unified Memory Architecture.\n\nLet’s see an example of how to leverage this Unified Memory Architecture for interaction between different blocks on the Apple silicon.\n\nWe will take HEVC encoding of the final video frame rendered by GPU as a case study.\n\nThis encoding is done using dedicated hardware media engines on Apple silicon.\n\nThe final output frame rendered by the GPU can be consumed directly by our media engines with no extra memory copies.\n\nIn the coming section, we will walk through an example on how to set up a pipeline for HEVC encoding of the final output frame produced by the GPU in the most efficient way.\n\nFor that, first we will leverage CoreVideo API to create a pool of pixel buffers backed by IOSurfaces.\n\nThen, using the Metal API, we render the final frames into Metal textures backed by IOSurfaces from the pool we just created.\n\nAnd finally, we dispatch these pixel buffers directly to the media engine for encode without any additional copies of the output frames produced by the GPU, thus leveraging the Unified Memory Architecture.\n\nLet’s walk through on how to do this step by step and covering all the constructs we need to enable this flow.\n\nFirst, we create a CVPixelBufferPool backed by IOSurface in the desired pixel format.\n\nHere, we will use the biplanar chroma-subsampled pixel format for HEVC encode.\n\nNow, you get a CVPixelBuffer from this CVPixelBufferPool.\n\nPass this CVPixelBuffer to the MetalTextureCache with the right plane index to get the CVMetalTextureReference.\n\nSince we are using biplanar pixel format, you need to perform this step for both planes of the biplanar pixel buffer.\n\nNext, get the underlying Metal texture from the CVMetalTextureReference object.\n\nPerform this step for both luma and chroma planes.\n\nRemember that these Metal textures are backed by the same IOSurfaces which are also backing the CVPixelBuffer planes.\n\nUsing Metal API, render into the textures corresponding to luma and chroma planes.\n\nThis will update the IOSurface which backs these Metal textures as well.\n\nWe highly recommend doing the chroma subsampling step on the chroma planes on the GPU itself as a shader pass within your image-processing pipeline.\n\nAn important thing to note is that both CVPixelBuffer and the Metal textures -- which we just rendered into -- are backed by the same underlying IOSurface copy in the system memory.\n\nYou can now send this CVPixelBuffer directly to the media engine for encode.\n\nAs you can see, due to the Unified Memory Architecture, we can seamlessly move data between GPU and media engine block with no memory copies.\n\nAnd finally, remember to release the CVPixelBuffer and CVMetalTexture reference after every frame.\n\nReleasing the CVPixelBuffer enables recycling of this buffer for future frames.\n\nTo wrap up, we encourage you again to do the following: leverage Unified Memory Architecture, use MTLRenderCommandEncoder instead of compute when applicable, merge all your eligible render passes within single render command encoder, set appropriate load/store actions, use memoryless for transient resources, leverage tile shading when applicable, and use buffer pools with other APIs for zero-copy.\n\nWe want to thank you for joining this session today.\n\nEnjoy the rest of WWDC 2021!  ♪",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "10:53",
      "title": "Memoryless attachments",
      "language": "swift",
      "code": "let textureDescriptor = MTLTextureDescriptor.texture2DDescriptor(…)\nlet outputTexture = device.makeTexture(descriptor: textureDescriptor)\n\ntextureDescriptor.storageMode = .memoryless\nlet tempTexture = device.makeTexture(descriptor: textureDescriptor) \n\nlet renderPassDesc = MTLRenderPassDescriptor()\nrenderPassDesc.colorAttachments[0].texture      = outputTexture\nrenderPassDesc.colorAttachments[0].loadAction   = .dontCare\nrenderPassDesc.colorAttachments[0].storeAction  = .store\nrenderPassDesc.colorAttachments[1].texture      = tempTexture\nrenderPassDesc.colorAttachments[1].loadAction   = .clear\nrenderPassDesc.colorAttachments[1].storeAction  = .dontCare\n\nlet renderPass = commandBuffer.makeRenderCommandEncoder(descriptor: renderPassDesc)"
    },
    {
      "timestamp": "12:25",
      "title": "Uber-shaders impact on registers",
      "language": "swift",
      "code": "fragment float4 processPixel(const constant ParamsStr* cs [[ buffer(0) ]])\n{\n  if (cs->inputIsHDR) {\n    // do HDR stuff\n  } else {\n    // do non-HDR stuff\n  }\n  if (cs->tonemapEnabled) {\n    // tone map\n  }\n}"
    },
    {
      "timestamp": "13:32",
      "title": "Function constants for Uber-shaders",
      "language": "swift",
      "code": "constant bool featureAEnabled[[function_constant(0)]];\nconstant bool featureBEnabled[[function_constant(1)]];\n\nfragment float4 processPixel(...)\n{\n  if (featureAEnabled) {\n    // do A stuff\n  } else {\n    // do not-A stuff\n  }\n  if (featureBEnabled) {\n    // do B stuff\n  }\n}"
    },
    {
      "timestamp": "23:02",
      "title": "Image processing filter graph",
      "language": "swift",
      "code": "typedef struct\n{\n    float4 OPTexture        [[ color(0) ]];\n    float4 IntermediateTex  [[ color(1) ]];\n} FragmentIO;\n\nfragment FragmentIO Unpack(RasterizerData in [[ stage_in ]],\n                           texture2d<float, access::sample> srcImageTexture [[texture(0)]])\n{\n    FragmentIO out;\n    \n    //...\n                         \n    // Run necessary per-pixel operations\n    out.OPTexture       = // assign computed value;\n    out.IntermediateTex = // assign computed value;\n    return out;\n}\n\nfragment FragmentIO CSC(RasterizerData in [[ stage_in ]], FragmentIO Input)\n{\n    FragmentIO out;\n\n    //...    \n    \n    out.IntermediateTex = // assign computed value;\n    return out;\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Debugging the shaders within a draw command or compute dispatch",
        "url": "https://developer.apple.com/documentation/Xcode/Debugging-the-shaders-within-a-draw-command-or-compute-dispatch"
      },
      {
        "title": "Metal",
        "url": "https://developer.apple.com/documentation/Metal"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2021/10153/4/F8C484C1-A0A2-4377-992B-313BEB320A28/downloads/wwdc2021-10153_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2021/10153/4/F8C484C1-A0A2-4377-992B-313BEB320A28/downloads/wwdc2021-10153_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10602",
      "year": "2020",
      "title": "Harness Apple GPUs with Metal",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10602"
    },
    {
      "id": "10632",
      "year": "2020",
      "title": "Optimize Metal Performance for Apple silicon Macs",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10632"
    }
  ],
  "extractedAt": "2025-07-18T10:32:50.190Z"
}