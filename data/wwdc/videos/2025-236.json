{
  "id": "236",
  "year": "2025",
  "url": "https://developer.apple.com/videos/play/wwdc2025/236/",
  "title": "Unlock GPU computing with WebGPU",
  "speakers": [],
  "duration": "",
  "topics": [
    "App Services"
  ],
  "hasTranscript": true,
  "hasCode": false,
  "transcript": {
    "fullText": "Hi, I’m Mike, an engineer on the Safari team. Today I’m going to show you how WebGPU allows you to unlock parallel computing on GPUs from the web. WebGPU can do everything WebGL can when it comes to 3D graphics, but with far greater performance and flexibility. It’s the best choice for graphics on the web. And on top of that, it’s the only way to run general-purpose computations on the GPU right in the browser. And if you’re familiar with Metal, you’ll feel right at home. Most calls are one-to-one mapping with Metal framework calls. In fact, it’s supported on all platforms where Metal is supported, specifically: Mac, iPhone, iPad, and Vision Pro. Being a web API, websites and web apps using WebGPU will run everywhere it is supported. On non-Apple systems, WebGPU is implemented with APIs similar to Metal. And if you’re not familiar with low-level graphics programming, there are many web graphics libraries you can use that support WebGPU, giving you access to all of the performance and capabilities it offers. In fact, you can use threeJS running WebGPU under-the-hood to animate these gorgeous 3D jellyfish in real time. I think it’s a stunning example, and it runs so smoothly in Safari, thanks to WebGPU being built from the ground up to take full advantage of today’s modern hardware. I’ll start by exploring the API and how WebGPU maps to Metal. This will walk you through much of the code necessary for any WebGPU application. Then, I’ll go over how to create WebGPU shader programs— code that runs directly on the GPU. I’ll cover the shading language and why a new one is necessary for the web.\n\nAfter I've covered the fundamentals, I’ll talk about how to get the best performance with the API. If you’re already familiar with WebGPU, this will be especially interesting as I cover specific optimizations for Apple’s platforms. So let’s get started by taking a quick look at the graphics pipeline.\n\nThe pipeline can be thought of as flowing from left to right. It starts with your website or web app, which loads content— things like images, videos, or binary data.\n\nThat content then gets passed to WebKit, which is responsible for making it ready for the GPU.\n\nWebKit calls into Metal framework to create the resources and programs which will later be run directly on the graphics hardware.\n\nNow let's break that down a bit. In WebGPU, Metal generates three types of resources: buffers, textures, and samplers. These get organized by WebKit into something called a GPU bind group. Basically, a structured way of grouping resources together so they can be efficiently used by the GPU. Under the hood, these are all packed into an argument buffer which is just a Metal buffer that holds references to the actual GPU resources.\n\nThe programs themselves come from strings of code and get compiled into three main types: compute, vertex, and fragment programs. These are the actual instructions that run on the GPU, doing everything from calculations to rendering pixels on the screen. So, with a solid understanding of how resources and programs fit into the pipeline, I’ll give a brief overview of how WebGPU defines the different interfaces in its API.\n\nWebGPU is a flat API, but it has a lot of interfaces. At the very top of the hierarchy is the GPU object and GPU adapter interfaces.\n\nA canvas is often used with WebGPU. Canvas can now return a GPUCanvasContext by querying the WebGPU context.\n\nA device is the main entry point for most API calls. It’s what you use to create most other interfaces.\n\nWhile there are a lot of different interfaces in the API, they simplify into a few categories. Namely, resources like textures, buffers, and samplers.\n\nEncoders, which issue commands on resources. Pipelines, which tell how various resources should be interpreted by the encoders. Bind groups, which group related resources together. And shader modules, which contain instructions for running calculations on the GPU. So now, with an understanding of the overall structure of WebGPU, let me introduce you to working with the API by showing you how to create the device and resources.\n\nA device is the entry point into most API calls. If you’re familiar with Metal, it’s very similar to MTLDevice.\n\nAssuming you have a page with a canvas, start by getting the canvas element. Then, use navigator.gpu.requestAdapter() to create an adapter and call requestDevice to create your GPU device.\n\nWebGPU supports several extensions, one being the shader-f16 extension, which lets you use half-precision floats.\n\nThey help with performance by cutting down memory bandwidth.\n\nWhile supported on all Apple devices, it’s an optional feature, so please make sure to check for support before using it on other platforms.\n\nNext, set up the canvas context with the device by calling configure. That links the canvas to memory the GPU can actually write to.\n\nNow that the device is ready, I can start creating some resources. In WebGPU, you’ll be working with things like buffers and textures a lot. In Metal, they’re represented by MTLBuffer and MTLTexture.\n\nBuffers are super flexible. You can use them to store all sorts of data, from simple stuff like a vector of floats to more complex, custom data types you define yourself. For example, you might have a buffer holding multiple instances of a particle type. Imagine, three particles stored right there in the buffer.\n\nA buffer is created by calling createBuffer on the device. Passing the buffer size and a usage mode.\n\nThe usage mode allows WebGPU to avoid data races without additional API complexities.\n\nDevice has a property called queue, which is used for performing operations on buffers and textures.\n\nOnce the buffer is created, populate its contents by calling the writeBuffer, passing the buffer, an offset, and a JavaScript arrayBuffer.\n\nLike buffers, textures are basically blobs of memory, but they get associated with special texture registers and instructions on the GPU. They’re often a representation of some image data and can be one-dimensional, two-dimensional, an array of two-dimensional textures, a cube map, which is just an array of six two-dimensional textures, or alternatively, a three-dimensional texture.\n\nYou create a texture by calling device.createTexture, passing in the width and height of the texture, the 2D texture format, and the usage modes.\n\nAfter creating the GPUTexture, we can load image data with device.queue.copyExternalImageToTexture passing the image bitmap, the 2D texture we just created, and the image size.\n\nA texture is often created from image data and represents an image on the GPU. After creating a device and resources, let’s look at how to create a pipeline.\n\nA pipeline specifies how textures and buffers will be used on the GPU. There are two types of pipelines. Render pipelines, which are used with vertex and fragment programs, and compute pipelines, which are used with compute programs. These map to MTLRenderPipelineState and MTLComputePipelineState objects in Metal.\n\nTo create a compute pipeline, simply call device.createComputePipeline passing either a bind group layout, or the constant auto identifier, which generates a layout from the shader.\n\nA layout is a structured way buffers, textures, and samplers are passed from the API to the GPU program.\n\nA shader module is required to create a pipeline. It’s created from a string.\n\nRender pipelines are created in a similar fashion, with an auto layout, a vertex shader module, and a fragment shader module.\n\nWith the device, resources, and pipelines created, the basic setup for any WebGPU application is complete.\n\nNow that we’ve explored the architecture of the WebGPU API, let's look at how to develop your shaders.\n\nThe WebGPU shading language, known as WGSL, allows websites to easily write programs which run directly on the GPU. Apple is heavily involved in the design and implementation of the WGSL shading language. WGSL is built from the ground up to be safe for the web. WGSL supports three types of programs: vertex programs, fragment programs, and compute programs.\n\nI’m going to walk through how to create this simple WebGPU example, which is composed of: a vertex program, which takes buffer data from JavaScript and creates triangles on the screen. A fragment program, which computes individual color and depth values of textures. And a compute program, which can do any general computations, but in this case, we will perform a physics simulation.\n\nThe vertex program computes where triangles appear on the screen.\n\nHere, we can see the outlines of the 100,000 triangles which are used in this example.\n\nTo write the output position of the triangles, use the @builtin position attribute.\n\nHere is the definition of the main function along with the vertex shader inputs. It just writes the position and a color. Now, let’s take a look at a fragment shader.\n\nSo take the color we generated in the vertex stage and store that color in the texture. Like this is a simple example, but you can insert any logic here to compute color and depth values. You can also write to storage textures, buffers, perform atomic operations, and a lot more. WGSL is really flexible. So now let’s look at something even more flexible: compute shaders.\n\nLike other program types, compute shaders can contain many bindings, which are inputs from JavaScript into a shader.\n\nCompute shaders are really cool because you can perform any computations you want, store the results in buffers, and read the buffers back out into your JavaScript code. There doesn’t have to be any visualization on the screen. Compute shaders weren’t possible with WebGL, which is another reason to use WebGPU for any new applications.\n\nThe compute program requires a workgroup size, which defines the size of the grid the compute shader will execute over.\n\nWe’ll also use the global_invocation_id, which is the position in the entire grid. This is a builtin variable, which can be used without needing to pass anything from JavaScript.\n\nThe body of the compute shader updates the particle simulation, applying gravity, velocity, and elapsed time.\n\nYou can perform any computations you’d like in a compute shader and it executes in parallel with incredible performance on the GPU.\n\nOnce the particle fades out completely, a new point is chosen to respawn to the particle by calling textureLoad on a probability map and selecting a new position for the particle.\n\nFinally, the rest of the particle attributes are reset to their starting values, and the particle is stored in the buffer.\n\nPut it all together and we get this nice animation with the WebGPU logo. By leveraging the parallel processing capabilities of the GPU, you can perform arbitrary computations of sizes not previously possible from the web while still achieving real-time performance.\n\nIt's great, isn't it? That was a quick overview of how to develop shaders for WebGPU applications. So now, let me show you how to get the best performance out of WebGPU.\n\nThere are just a few guidelines to keep in mind that will help you deliver the best experience on Apple’s platforms. A key to great performance is to be mindful of memory use, and that means: use memory efficient data types, record your render commands once, then reuse them, and keep the number of resources low. OK, so let's dive into more detail.\n\nThere are a couple of ways to minimize memory use.\n\nFirst, you can use half precision floats. They're an IEEE standard. In WGSL, the data type is called f16.\n\nThey really help cut down on memory use and boost performance. That said, they're not always practical. You need to make sure your algorithms are stable with reduced precision, and keep in mind, their values max out at just over 65,000, unlike 32-bit floats that can handle much larger values. On iOS and visionOS specifically, storing data in f16 or compressed formats can really help you avoid getting your program terminated due to memory pressure. To use half-precision floats, you’ll need to enable them during device creation and in your WGSL code. Let me show you how to do that with a quick code example.\n\nFirst, enable the shader-f16 extension in the call to requestDevice, and in the shader, add the ‘enable f16’ statement.\n\nThen you can use f16 scalar and vector types, along with all the 32-bit types like before. And, even if you just store the data in half precision and immediately unpack to f32, you’ll still get a lot of memory benefit to avoid getting your app terminated due to memory pressure.\n\nAnother way to minimize memory usage is by avoiding unnecessary buffer and texture update calls. These require data copies from JavaScript into the memory which backs the Metal resource. Updating buffers with index and indirect usage modes can be especially expensive since validation needs to be performed prior to using the buffer again. Those buffers index either directly or indirectly into vertex buffers, and WebGPU must ensure all the offsets into these buffers are within bounds before any draw commands execute.\n\nOnly update these type of buffers when needed. This also applies to using a buffer in a bind group with write or read/write access. As illustrated in the code example, prefer read-only access unless you are writing through the resource in the shader, especially if the resource is an index or indirect buffer.\n\nFollowing those memory tips can have a big impact on performance, not just on Apple’s platforms, but across all mobile and desktop devices. Next, I want to tell you more about reusing your render commands.\n\nRender bundles are a great way to do that, allowing you to encode commands once and replay them as many times as you need. WebGPU has to make sure all reads and writes are well-defined and within bounds, which normally means a lot of validation every frame. But with render bundles, that validation happens once, when the bundle is created, instead of every time it runs. That saves time and brings your app closer to native performance, leaving more room for your actual logic. And creating a render bundle is pretty simple. Start by creating a render bundle encoder and then encode your draw calls, just like you would with a render pass encoder. Calling finish() creates the bundle for reuse.\n\nNow that you have a bundle, you can run all those draw commands with a call to executeBundles(), and you can do that over and over as needed.\n\nUnder the hood, render bundles map to Metal’s indirect command buffers and give similar performance benefits. So now that we’ve tackled memory usage and cut down validation overhead, let’s look at reducing the number of resources.\n\nSpecifically, command buffers, render and compute passes, bind group layouts, and bind groups.\n\nCommand buffer boundaries require synchronization between high-speed on-chip memory and unified on-device memory. If possible, use a single command buffer per update loop, or if that is not possible, a general good rule of thumb is that as few command buffers as possible should be used. Remember, it is only necessary to split command buffers when you require the data written back to unified memory. Usually, that's a rare occurrence.\n\nUnlike command buffers, passes don’t require synchronization with unified memory. They still consume substantial memory bandwidth, depending on the render target and compute dispatch sizes. So it’s best to use as few as possible to save memory bandwidth.\n\nLike many mobile phones, the GPU in Apple’s devices is based on a tile-based deferred renderer. Following best practices for combining passes and saving memory bandwidth will help your site or web app excel on Apple’s hardware. For more information on tile-based renderers, check out “Optimize Metal Performance for Apple silicon Macs” and “Harness Apple GPUs with Metal” from WWDC 2020.\n\nWith that, let me focus on bind groups. They are implemented with Metal argument buffers, so creating a bind group also creates a new MTLBuffer By using dynamic offsets, a single bind group, which shares the same layout, but uses different resources at runtime, can be created.\n\nTo use dynamic offsets, a custom bind group layout must be created instead of using an auto layout from the shader module.\n\nThe layout is created by calling createBindGroupLayout with hasDynamicOffset, and then pass the newly created layout to create the bind group. The dynamic offsets are involved in calls to setBindGroup. One offset per dynamic buffer in the bind group is required.\n\nIn this case, the bind group has one buffer using dynamic offsets, so one offset is passed to setBindGroup.\n\nFor instance, instead of creating 10 bind groups, each with a 64-byte buffer inside, a much better approach is to make one 640-byte buffer to represent 10 64-byte objects. I’ve just avoided creating 9 Metal buffers by doing that.\n\nBy storing similar data in less memory, avoiding repeated validation, and minimizing the total number of Metal objects created, you can create stunning, highly efficient websites and web apps with WebGPU. I hope you take these performance considerations into account when using WebGPU. WebGPU allows for running custom algorithms directly on the GPU, something not previously possible from the web. I encourage you to start using WebGPU today on Mac, iPhone, iPad, and Vision Pro, and please consider the optimal usage guidelines.\n\nI’m so excited for the future of GPU programming on the web.",
    "segments": []
  },
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Metal Performance Shaders",
        "url": "https://developer.apple.com/documentation/MetalPerformanceShaders"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/236/5/3eb6d0b1-48d5-46d6-bb81-a6ce4b03b8aa/downloads/wwdc2025-236_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/236/5/3eb6d0b1-48d5-46d6-bb81-a6ce4b03b8aa/downloads/wwdc2025-236_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "231",
      "year": "2025",
      "title": "Meet WebKit for SwiftUI",
      "url": "https://developer.apple.com/videos/play/wwdc2025/231"
    },
    {
      "id": "233",
      "year": "2025",
      "title": "What’s new in Safari and WebKit",
      "url": "https://developer.apple.com/videos/play/wwdc2025/233"
    },
    {
      "id": "10602",
      "year": "2020",
      "title": "Harness Apple GPUs with Metal",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10602"
    },
    {
      "id": "10632",
      "year": "2020",
      "title": "Optimize Metal Performance for Apple silicon Macs",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10632"
    }
  ],
  "extractedAt": "2025-07-18T09:10:48.928Z"
}