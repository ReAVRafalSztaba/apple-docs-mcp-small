{
  "id": "10091",
  "year": "2023",
  "url": "https://developer.apple.com/videos/play/wwdc2023/10091/",
  "title": "Evolve your ARKit app for spatial experiences",
  "speakers": [],
  "duration": "",
  "topics": [
    "Spatial Computing",
    "SwiftUI & UI Frameworks"
  ],
  "hasTranscript": true,
  "hasCode": false,
  "transcript": {
    "fullText": "♪ Mellow instrumental hip-hop ♪ ♪ Omid Khalili: Hello! My name is Omid.\n\nOliver and I are engineers on the ARKit team and we are thrilled to review the concepts -- some familiar and some new -- that you'll need know about when bringing your iOS AR app to our new platform.\n\nARKit was introduced on iOS in 2017 and with it, we introduced three key concepts to building augmented reality applications.\n\nWith world tracking, ARKit is able to track your device's position in the world with six degrees of freedom.\n\nThis allows anchoring virtual content with a position and orientation to the real world.\n\nScene understanding provides insight about the real world around you.\n\nUsing the provided geometry and semantic knowledge, your content can be intelligently placed and realistically interact with surroundings.\n\nFinally, rendering engines can correctly register and composite your virtual content over captured images utilizing camera transforms and intrinsics provided by ARKit.\n\nInitially, we started with a SceneKit view to use ARKit's camera transforms and render 3D content on iOS.\n\nWe then introduced RealityKit, laying out the foundation for an engine capable of highly realistic physically-based rendering and accurate object simulation with your surroundings.\n\nTo enable spatial computing, ARKit and RealityKit have matured and are deeply integrated into the operating system.\n\nFor example, ARKit's tracking and Scene Understanding are now running as system services, backing everything from window placement to spatial audio.\n\nThe system takes on responsibilities that used to belong to applications.\n\nCamera pass-through and matting of the user's hands are now built-in, so your application gets these capabilities for free.\n\nAnother built-in capability is that ARKit world maps are continuously persisted by a system service, so your application doesn't need to do it anymore.\n\nWe believe that this will free you up to focus on building the best application and content possible for this platform.\n\nHere is an example demonstrating these capabilities, along with new ones introduced with this platform.\n\nFor example, ARKit now provides hand tracking to your app, which allows people to reach out and directly interact with virtual content that can then interact with their surroundings.\n\nIn order to take advantage of all the new capabilities and immersive experiences this new platform offers, you'll need to update your iOS ARKit-based experience.\n\nThis is a great opportunity to reimagine your app and AR experience for spatial computing.\n\nAs a part of this transition, you'll be using familiar concepts that we've introduced with ARKit and RealityKit.\n\nWe're going to cover how these concepts have carried over, how they've evolved, and how you can take advantage of them.\n\nLet's get started! First, we'll explore some new ways you can present your app for spatial computing, and introduce new content tools available to you.\n\nNext, we will talk about Reality Kit, which is the engine to use to render and interact with your content.\n\nWe'll see how RealityView lets your app leverage spatial computing similar to ARView on iOS.\n\nThen, we'll talk about the different ways your app can bring content into people's surroundings.\n\nRaycasting is something many iOS applications use to place content.\n\nWe'll show an example of how to combine ARKit data and RealityKit to enable raycasting for spatial computing.\n\nAnd finally, we'll review the updates to ARKit and see the new ways to utilize familiar concepts from iOS.\n\nLets get into preparing to migrate your experience for spatial computing.\n\nSpatial computing allows you to take your iOS AR experience and expand it beyond the window.\n\nThis platform offers new ways to present your application that you'll want to consider as you bring your iOS experience over.\n\nHere is an example from our Hello World sample app.\n\nYou can now display UI, including windows and three-dimensional content, anywhere around you.\n\nBy default, applications on this platform launch into the Shared Space.\n\nThe Shared Space is where apps exist side by side, much like multiple apps on a Mac desktop.\n\nInside a Shared Space, your app can open one or more windows to display content.\n\nAdditionally, your app can create a three-dimensional volume.\n\nFor example, now you can show a list of available board games in one window, the rules in another, and open the selected game in its own volume.\n\nThe game can be played while keeping a Safari window open to read up on winning strategies.\n\nThe content you add to the window and volume stays contained within its bounds to allow sharing the space with other applications.\n\nIn some cases, you may want your app to have more control over the level of immersion in your experience -- maybe to play a game that interacts with your room.\n\nFor this, your app can open a dedicated Full Space in which only your app's windows, volumes, and 3D objects appear.\n\nOnce in a Full Space, your application has access to more features.\n\nUsing RealityKit's anchor entities, you can target and attach objects to the surroundings like tables, floors, and even parts of your hands like the palm or wrist.\n\nAnchor entities work without requiring user permission.\n\nARKit data is something else your app can only access in a Full Space.\n\nWith permission, ARKit will provide data about the real-world surfaces, scene geometry, and skeletal hand tracking, expanding your app's ability for realistic physics and natural interactions.\n\nWindows, volumes, and spaces are all SwiftUI scene types.\n\nThere's so much more for you to learn about these.\n\nFor starters, you can go to the session mentioned here.\n\nNext, let's review the main steps needed to prepare your content for bringing it to spatial computing.\n\nMemorable AR experiences on iOS begin with great 3D content; the same is true for spatial experiences on this platform.\n\nAnd when it comes to 3D content, it's great to rely on an open standard like Universal Scene Description, or USD for short.\n\nUSD is production proven, and scales from creators making single assets to large studios working on AAA games and films.\n\nApple was an early adopter of USD, adding it to our platforms in 2017 and growing support since.\n\nToday, USD is at the heart of 3D content for spatial computing.\n\nWith USD assets ready, you can bring them into our new developer tool, Reality Composer Pro, to compose, edit, and preview your 3D content.\n\nIf you're using CustomMaterials for your 3D content on iOS, then you will need to rebuild them using its shader graph.\n\nYou also have the ability to edit your RealityKit components directly through the UI.\n\nAnd finally, you can import your Reality Composer Pro project directly into Xcode, allowing you to easily bundle all of your USD assets, materials, and custom components into your Xcode project.\n\nWe have some great sessions to help you learn more about Reality Composer Pro and how to build your own custom materials for spatial computing.\n\nNow that we've seen the different ways to present your application, let's learn more about the features RealityView offers as you bring your experience over.\n\nWe just saw how spatial computing allows apps to display content in your space.\n\nOne of the key differences coming from iOS is how different elements can be presented side by side.\n\nNotice how your 3D content and 2D elements can appear and work along side each other.\n\nComing from iOS, you're going to use familiar frameworks to create each of these.\n\nYou'll use SwiftUI to build the best 2D UI and get system gestures events like the ones on iOS.\n\nAnd you'll use RealityKit to render your 3D content for spatial experiences.\n\nThe way to interface with both of these at the same time is through RealityView - a new SwiftUI view that we're introducing to cater to the unique needs of spatial computing.\n\nRealityView truly bridges SwiftUI and RealityKit, allowing you to combine 2D and 3D elements and create a memorable spatial experience.\n\nYou'll be using the RealityView to hold all the entities you wish to display and interact with.\n\nYou can get gesture events and connect them to the entities in your view to control them.\n\nAnd with access to ARKit's scene understanding, you can enable realistic simulations with people's surroundings and even their hands using RealityKit's collision components.\n\nBefore we look at how using RealityKit carries over from iOS, let's do a quick refresher on how to work with RealityKit's Entity Component System.\n\nIn the Reality Kit Entity Component System, each entity is a container for 3D content.\n\nDifferent components are added to an entity to define its look and behavior.\n\nThis can include a model component for how it should render; a collision component, for how it can collide with other entities; and many more.\n\nYou can use RealityComposer Pro to prepare RealityKit components like collision components and get them added to your entities.\n\nSystems contain code to act on entities that have the required components.\n\nFor example, the system required for gesture support only operates on entities that have a CollisionComponent and InputTargetComponent.\n\nA lot of the concepts used by RealityView for spatial computing carry over from those of ARView on iOS.\n\nLets see how these two stack up.\n\nBoth views are event-aware containers to hold the entities you wish to display in your app.\n\nYou can add Gesture Support to your views to enable selection and interaction with entities.\n\nWith SwiftUI for spatial computing, you can reach out to select or drag your entities.\n\nBoth ARView and RealityView provide a collection of your entities.\n\nARView uses a Scene for this.\n\nRealityView has a Content to add your entities to.\n\nYou can add AnchorEntities to them, allowing you to anchor your content to the real world.\n\nOn both platforms, you create an entity to load your content model and an AnchorEntity to place it.\n\nOne main difference between the platforms is in the behavior of anchor entities.\n\nARView on iOS uses an ARSession and your app must receive permission to run scene understanding algorithms necessary for anchor entities to work.\n\nRealityView is using System Services to enable anchorEntities.\n\nThis means that spatial experiences can anchor content to your surroundings without requiring permissions.\n\nApps using this approach do not receive the underlying scene understanding data or transforms.\n\nNot having transform data for your app to place content has some implications that Oliver will talk about later in his section.\n\nAs we've seen, there are many familiar concepts that carry over coming from iOS, but there are also new capabilities that RealityKit provides for spatial computing.\n\nWe've only scratched the surface of what's possible with RealityKit on this new platform, and you may want to check out the session below to follow up on more.\n\nNow over to Oliver, who will talk more about RealityView and how to bring in your content from iOS.\n\nOliver Dunkley: Thanks, Omid! Let's continue by exploring the different ways you can bring in your existing content to spatial computing.\n\nLets start in the Shared Space.\n\nWe can add 3D content to a window or volume, and use system gestures to interact with it.\n\nTo display your assets, you just add them directly to RealityView's Content.\n\nYou do this by creating an entity to hold your model component and position it by setting the transform component.\n\nYou can also set up gesture support to modify the transform component.\n\nNote that all entities added to the view's content exist in the same space relative to the space's origin and can therefore interact with one another.\n\nIn the Shared Space, content cannot be anchored to your surroundings.\n\nLet's consider our options if we transition our app to a Full Space.\n\nOne of the key differences coming from the Shared Space is that apps can now additionally anchor content to people's surroundings.\n\nAnchoring your content here can happen in two ways.\n\nLets first look at using RealityKit's AnchorEntity to place content without requiring permissions to use ARKit data in your app.\n\nRealityKit's AnchorEntities allow you to specify a target for the system to find and automatically anchor your content to.\n\nSo for example, in order to place a 3D model on a table surface in front of you, you can use a RealityKit AnchorEntity with a target set to table.\n\nDifferent from iOS, AnchorEntities can be used without having to prompt for user permissions.\n\nPeople's privacy is preserved by not sharing the underlying transforms of the AnchorEntity with your application.\n\nNote: this implies children of different anchor entities are not aware of one another.\n\nNew to anchorEntities, you can target hands, which opens up a whole new realm of interesting interaction opportunities.\n\nFor example, you could anchor content to a person's palm and have it follow their hands as they move them.\n\nThis is all done by the system, without telling your app where the persons hands actually are.\n\nAnchorEntitys provide a quick, privacy-friendly way for your app to anchor content to people's surroundings.\n\nComing back to a Full Space, we can also leverage ARKit to incorporate system-level knowledge of the people's surroundings.\n\nThis enables you to build your own custom placement logic.\n\nLet's take a look at how this works.\n\nSimilar to iOS, your application receives anchoring updates for scene understanding data.\n\nYou can integrate this anchor data into your app logic to achieve all sorts of amazing experiences.\n\nFor example, you could use the bounds of a plane to center and distribute your content onto.\n\nOr, you could use planes and their classifications to find the corner of a room by looking for the intersection of two walls and a floor.\n\nOnce you've decided where to place your content, you add a world anchor for ARKit to track and use it to update your entity's transform component.\n\nThis not only allows your content to remain anchored to the real world, as the underlying world map is updated, but it also opens the door to anchor persistence, which we will explore shortly.\n\nAll the entities added to your space can interact with one another as well as with the surroundings.\n\nThis all works because scene understanding anchors are delivered with transforms relative to the space's origin.\n\nUser Permission is required to use ARKit capabilities.\n\nYou just saw how integrating ARKit data into your app logic can enable more advanced features.\n\nSo far we have talked about letting your app place content.\n\nLet's explore how we can let people guide placement.\n\nOn iOS, you can use raycasting to translate 2D input to a 3D position.\n\nBut with this new platform, we don't need this 2D-3D bridge anymore, as we can use hands to naturally interact with experiences directly.\n\nRaycasting remains powerful; it lets people reach out beyond arms length.\n\nThere are various ways to set up raycasting.\n\nFundamentally, you need to setup RealityKit's collision components to raycast against.\n\nCollision components can also be created from ARKit's mesh anchors to raycast against people's surroundings.\n\nLet's explore two examples of how to raycast for spatial computing: first, using system gestures, and the second using hands data.\n\nAfter obtaining a position, we can place an ARKit worldAnchor to keep our content anchored.\n\nLet's consider the following example.\n\nImagine our app revolves around placing inspirational 3D assets for modelers.\n\nMaybe in this particular scenario, a person wants to use our app to place a virtual ship on their workbench for some modeling project.\n\nHere is our workbench we want to place our ship on.\n\nWe'll start with an empty RealityView.\n\nARKit's scene understanding provides mesh anchors that we'll use to represent the surroundings.\n\nThey provide geometry and semantic information we can use.\n\nRemember that meshes for scene reconstruction data are delivered as a series of chunks.\n\nWe'll create an entity to represent this mesh chunk, and we'll correctly place this entity in a full space using the mesh anchor's transform.\n\nOur entity then needs a collision component to hit test against.\n\nWe'll use RealityKit's ShapeResources method to generate a collision shape from the meshAnchor for our entity.\n\nWe'll then add our correctly placed entity which supports hit testing.\n\nWe'll build an entity and collision component for each mesh chunk we receive to represent all the surroundings.\n\nAs scene reconstruction is refined, we may get updates to meshes or have chunks removed.\n\nWe should be ready to update our entities on these changes as well.\n\nWe now have a collection of entities representing the surroundings.\n\nAll these entities have collision components and can support a raycast test.\n\nLet's first explore raycasting using system gestures, and then continue the example using hands data.\n\nWe can raycast and get a position to place our ship using system gestures.\n\nGestures can only interact with entities that have both Collision and InputTarget components, so we add one to each of our mesh entities.\n\nBy adding a SpatialTapGesture to the RealityView, people can raycast by looking at entities and tapping.\n\nThis resulting event holds a position in world space representing the place people looked at when tapping.\n\nInstead of using system gestures, we could also have used ARKit's hand anchors to build a ray.\n\nLets take a step back and explore this option.\n\nTo know where people point, we first need a representation of the person's hand.\n\nARkit's new hand anchors gives us everything we need.\n\nWe can use finger joint information to build the origin and direction of the ray for our query.\n\nNow that we have the origin and direction of our ray, we can do a raycast against the entities in our scene.\n\nThe resulting CollisionCastHit provides the entity that was hit, along with its position and a surface normal.\n\nOnce we identify a position in the world to place our content, we'll add a world anchor for ARKit to continuously track this position for us.\n\nARKit will update this world anchor's transform as the world map is refined.\n\nWe can create a new entity to load our ship's model, and set its transform using the world anchor update, positioning it where the user wanted.\n\nFinally, we can add the entity to our content to render it over the workbench.\n\nWhenever ARKit updates the world anchor we added, we update the transform component of our ship entity, making sure it stays anchored to the real world.\n\nAnd that's it! We used our hands to point to a location in our surroundings and placed content there.\n\nRaycasting is not only useful for placing content, but also for interacting with it.\n\nLet's see what it takes to raycast against our virtual ship.\n\nRealityKit collision components are very powerful.\n\nWe can let the ship entity participate in collisions by simply adding an appropriate collision component to it, which Reality Composer Pro can help us with.\n\nAfter enabling the ship's collision component and building a new ray from the latest hand joint positions, we can do another raycast and tell if the user is pointing at the ship or the table.\n\nThe previous examples demonstrated the power and versatility of combining RealityKit's features with ARKit's scene understanding to build truly compelling experiences.\n\nLets see how using ARkit has changed for spatial computing.\n\nFundamentally, just as on iOS, ARKit still works by running a session to receive anchor updates.\n\nHow you configure and run your session, receive anchors updates, and persist world anchors has changed on this new platform.\n\nLet's take a look! On iOS, ARKit provides different configurations to chose from.\n\nEach configuration bundles capabilities necessary for your experience.\n\nHere, for example, we selected ARWorldTrackingConfiguration, and will enable sceneReconstruction for meshes and planeDetection for planes.\n\nWe can then create our ARSession and run it with the selected configuration.\n\nOn this new platform, ARKit now exposes a data provider for each scene understanding capability.\n\nHand tracking is a new capability offered by ARKit and gets its own provider as well.\n\nEach data provider's initializer takes the parameters needed to configure that provider instance.\n\nNow instead of choosing from a catalogue of preset configurations, you get an à la carte selection of the providers you need for your application.\n\nHere for example, we choose a SceneReconstructionProvider to receive mesh anchors and a PlaneDetectionProvider to receive plane anchors.\n\nWe create the providers and initialize the mesh classification and plane types we wish to receive.\n\nThen we create an ARKitSession and run it with the instantiated providers.\n\nNow that we have seen how configuring your session has been simplified, let's go and understand which way these new data providers change the way your app actually receives ARKit data.\n\nOn iOS, a single delegate receives anchor and frame updates.\n\nAnchors are aggregated and delivered with ARFrames to keep camera frames and anchors in sync.\n\nApplications are responsible for displaying the camera pixel buffer, and using camera transforms to register and render tracked virtual content.\n\nMesh and plane anchors are delivered as base anchors, and it is up to you to disambiguate them to figure out which is which.\n\nOn our new platform, it is the data providers that deliver anchor updates.\n\nHere are the providers we previously configured.\n\nOnce you run an ARKitSession, each provider will immediately begin asynchronously publishing anchor updates.\n\nThe SceneReconstructionProvider gives meshAnchors, and the planeDetectionProvider gives us PlaneAnchors.\n\nNo disambiguation is necessary! Anchor updates come as soon as they're available, and are decoupled from updates of other data providers.\n\nIt is important to note that ARFrames are no longer provided.\n\nSpatial computing applications do not need frame or camera data to display content, since this is now done automatically by the system.\n\nWithout having to package anchor updates with an ARFrame, ARKit can now deliver them immediately, reducing latency, allowing your application to quickly react to updates in the person's surroundings.\n\nNext, let's talk about worldAnchor persistence.\n\nYou will love these changes! In our raycasting examples, we used world anchors to place and anchor virtual content to real-world positions.\n\nYour app can persist these anchors, enabling it to automatically receive them again, when the device returns to the same surroundings.\n\nLet's first quickly recap how persistence worked on iOS.\n\nOn iOS, it is the application's responsibility to handle world map and anchor persistence.\n\nThis included requesting and saving ARKit's world map with your added anchor, adding logic to reload the correct world map at the right time, then waiting for relocalization to finish before receiving previously persisted anchors and continuing the application experience.\n\nOn this new platform, the system continuously persists the world map in the background, seamlessly loading, unloading, creating, and relocalizing to existing maps as people move around.\n\nYour application does not have to handle maps anymore, the system now does it for you! You simply focus on using world anchors to persist locations of virtual content.\n\nWhen placing content, you'll be using the new WorldTrackingProvider to add WorldAnchors to the world map.\n\nThe system will automatically save these for you.\n\nThe WorldTrackingProvider will update the tracking status and transforms of these world anchors.\n\nYou can use the WorldAnchor identifier to load or unload the corresponding virtual content.\n\nWe just highlighted a few updates to the ARKit principles that you knew from iOS, but there is so much more to explore! For a deeper dive, with code examples, we recommend you watch \"Meet ARKit for spatial computing.\" Let's conclude this session! In this session, we provided a high-level understanding of how ARKit and RealityKit concepts have evolved from iOS, the changes you need to consider, and which sessions to watch for more details.\n\nThis platform takes on many tasks your iOS app had to handle, allowing you to really focus on building beautiful content and experiences using frameworks and concepts you're already familiar with.\n\nWe are thrilled to see how you leverage spatial computing and all of its amazing capabilities to evolve your app! Thank you for watching! ♪",
    "segments": []
  },
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10091/4/8582CCE8-B637-4A9F-94F5-69EE67ED58D4/downloads/wwdc2023-10091_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10091/4/8582CCE8-B637-4A9F-94F5-69EE67ED58D4/downloads/wwdc2023-10091_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10080",
      "year": "2023",
      "title": "Build spatial experiences with RealityKit",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10080"
    },
    {
      "id": "10082",
      "year": "2023",
      "title": "Meet ARKit for spatial computing",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10082"
    },
    {
      "id": "10083",
      "year": "2023",
      "title": "Meet Reality Composer Pro",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10083"
    },
    {
      "id": "10090",
      "year": "2023",
      "title": "Run your iPad and iPhone apps in the Shared Space",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10090"
    }
  ],
  "extractedAt": "2025-07-18T10:45:45.710Z"
}