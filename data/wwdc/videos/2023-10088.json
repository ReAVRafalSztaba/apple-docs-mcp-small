{
  "id": "10088",
  "year": "2023",
  "url": "https://developer.apple.com/videos/play/wwdc2023/10088/",
  "title": "Create immersive Unity apps",
  "speakers": [],
  "duration": "",
  "topics": [
    "Graphics & Games"
  ],
  "hasTranscript": true,
  "hasCode": false,
  "transcript": {
    "fullText": "♪ Mellow instrumental hip-hop ♪ ♪ John Calsbeek: Welcome! I'm John, and I work on RealityKit.\n\nVladimir Vukićević: And I'm Vlad, from Unity.\n\nJohn: I'm thrilled to introduce Unity support for immersive apps.\n\nUnity has worked with Apple to bring the full Unity experience to this new platform.\n\nUnity is used by tens of thousands of apps, and now you can use Unity to build immersive apps.\n\nTriband brought their Apple Arcade title What The Golf?, which is built in Unity, to this platform.\n\nIt's really fun to play on an iPhone, and it feels great to play it this way.\n\nThere are two main approaches for creating immersive experiences on this platform with Unity.\n\nYou can create experiences which mix your content with real-world objects using passthrough, either as an immersive experience or in the Shared Space alongside other apps.\n\nYou can also bring a fully immersive Unity experience to the platform.\n\nIf you're interested in this approach, I recommend you check out \"Bring your Unity VR app to a fully immersive space.\" Creating experiences for the shared space with Unity opens up exciting opportunities for your apps.\n\nHere's Vlad to tell you more.\n\nVladimir: Thanks, John.\n\nUnity and Apple have been working together for the past two years to make sure your Unity content looks great on the platform.\n\nWhether you're starting with an existing project, or building something completely new, Unity is a great tool for creating immersive experiences using familiar tools and some new capabilities.\n\nOn this platform, you can achieve the visual look you want using Unity's shaders and materials.\n\nWe're introducing the ability to enter play mode directly to device, improving your iteration time.\n\nThere's also a new concept called the volume camera, which controls how content from your Unity scene is brought into the real world.\n\nInput on this new device can be as simple as a look-and-tap gesture or involve more complex interactions.\n\nAnd there are a few things you can do today to prepare your Unity content for spatial computing.\n\nHere's an example of some of these elements working together.\n\nThis scene uses materials built with Unity's Shader Graph, and it's being displayed in the shared space in the Simulator with passthrough.\n\nThere are fully rigged and animated characters, like the ogre in the back.\n\nPhysics interactions work just like you're used to.\n\nAll of the residents of this town are using character navigation to move around, and custom dynamic scripted behaviors are used to make this scene feel alive.\n\nWe put this together in two weeks with the help of the Asset Store, and it looks great when viewed in your space, where you can get up close and look at a scene from any angle.\n\nAll content in the shared space is rendered using RealityKit.\n\nYour Unity materials and shaders need to be translated to this new environment.\n\nUnity has created PolySpatial, which takes care of this translation for you and brings many Unity features over to this environment.\n\nPolySpatial translates materials, regular and skinned mesh rendering, as well as particle effects and sprites.\n\nUnity simulation features are supported, and you continue to use MonoBehaviours, scriptable objects, and other standard tools.\n\nThree categories of materials are translated.\n\nThey are the physically based materials, custom materials, and some special effect materials.\n\nMaterials based on Unity's physically based shaders translate directly to RealityKit.\n\nIf you're using the Universal Render Pipeline, you can use any of the Lit, Simple Lit, or Complex Lit Shaders in your materials.\n\nWith the built-in pipeline, you can use the Standard Shader.\n\nAll of these are translated to a RealityKit PhysicallyBasedMaterial.\n\nCustom shader and material types are supported through Unity Shader Graph.\n\nUnity Shader Graphs are converted to MaterialX, a standard interchange format for complex materials.\n\nMaterialX shaders become a ShaderGraphMaterial in RealityKit.\n\nMany Unity Shader Graph nodes are supported, so you can create complex and interesting effects.\n\nHandwritten shaders are not supported for rendering through RealityKit, but you can use them with RenderTextures in Unity.\n\nYou can then use that RenderTexture as a texture input to a Shader Graph for displaying through RealityKit.\n\nTwo additional material shader types are supported.\n\nFirst is the Unlit Shader, which lets you create objects that take on a solid color or texture, unaffected by lighting.\n\nThe second is the Occlusion Shader, which lets passthrough show through the object.\n\nYou can use the Occlusion Shader with world mesh data to help your content feel more integrated with the real world.\n\nUnity MeshRenderers and SkinnedMeshRenderers are supported and are the primary way of bringing visual content into real space.\n\nRigged characters and animation are available.\n\nYou can use either the Universal or the Built-in Render Pipeline, and your content will be translated to RealityKit via Unity PolySpatial.\n\nRendering features, such as postprocessing effects and custom pipeline stages, are not available, as RealityKit performs the final rendering.\n\nParticle effects using Unity's Shuriken system are either translated to RealityKit's particle system, if they are compatible, or are translated to baked meshes.\n\nSprites become 3D meshes, though you should consider how you're using them in a spatial context.\n\nPolySpatial works to optimize and translate rendering between Unity and RealityKit.\n\nSimulation features in Unity work just like you're used to, such as Physics, Animation and Timeline, Pathfinding and NavMesh, your custom MonoBehaviours, and other non-rendering features.\n\nTo help you fine-tune your look and to speed up iteration, Unity PolySpatial enables \"Play to device.\" It can take time to go through the build process to see what your content looks like on your device.\n\nWith PolySpatial, you have access to Play to device for the first time.\n\nPlay to device lets you see an instant preview of your scene and make live changes to it.\n\nIt works in the simulator and works great on device as well.\n\nYou can use Play to device to rapidly explore the placement and size of your content, including adding and removing elements.\n\nYou can change materials, textures, and even Shader Graphs to fine-tune your look while seeing your content in place with passthrough.\n\nYou can test out interaction because events are sent back to the editor.\n\nYour simulation continues to run, so it's easy to debug just by attaching to the editor.\n\nHere's the same castle scene you saw earlier.\n\nI have it open in Unity on the left, and with Play to device, I can see it running in the simulator on the right.\n\nI can add more ogres just by dragging them into my scene.\n\nThey're instantly visible in the simulator or device.\n\nIf I want to see how pink or neon green ogres look, I can.\n\nPlay to device is a really efficient workflow for iterating on your content, and it's currently available in Unity only for creating content in the shared space.\n\nBecause you're using Unity to create volumetric content that participates in the shared space, a new concept called a volume camera lets you control how your scene is brought into the real world.\n\nA volume camera can create two types of volumes, bounded and unbounded, each with different characteristics.\n\nYour application can switch between the two at any time.\n\nBounded volumes exist in the shared space as volumes, alongside other apps and games.\n\nThey have dimensions and a transform in Unity, as well as a specific real-world size.\n\nThey can be repositioned, but people cannot resize them.\n\nThe dimensions and the transform of the volume camera define the region of your scene that your app will display in a volume.\n\nThey're specified in scene units.\n\nYou can see a preview of the volume in green in Unity's scene view.\n\nBy manipulating the dimensions and the transform of the volume camera, different parts of the scene can be brought into the volume.\n\nIf I move or rotate the camera, new objects become visible in my space.\n\nIf I scale up its size, more of the scene comes into view.\n\nIn both cases, the volume remains the same size.\n\nOnly the content visible inside of it changes.\n\nNotice that in the initial placement of the volume camera, the spring intersects the side of the volume; content is clipped by RealityKit.\n\nIf you will have content intersecting the edges of your volume, consider placing the same mesh in your scene a second time with a back-facing material to fill in the clipped sections.\n\nYour unbounded volume displays in a full space on this platform and allows your content to fully blend with passthrough for a more immersive experience.\n\nIt has no dimensions because it selects your entire scene, and its transform specifies how your scene units are mapped to real-world units.\n\nThere can only be one unbounded volume camera active at a time.\n\nYou'll see an example of an unbounded volume when we talk about interaction.\n\nUnity supports multiple input types for apps on this platform.\n\nOn this platform, people use their eyes and hands to look at content and tap their fingers together to select it.\n\nFull hand tracking as well as head pose data lets you create realistic interactions.\n\nAugmented reality data from ARKit is available, as are Bluetooth devices such as keyboards and game controllers.\n\nThe tap gesture is the most common way of interacting with content on this platform.\n\nIn order for your objects to receive these events, they must have input colliders configured.\n\nYou can look and tap to select an object from a distance, or you can reach out and directly touch an object with a finger.\n\nUp to two simultaneous tap actions can be in progress.\n\nIn Unity, taps are available as WorldTouch events.\n\nThey are similar to 2D tap events, but have a full 3D position.\n\nHand and head pose tracking gives your application precise information about each hand joint and the viewer's head position relative to the global tracking origin.\n\nLow-level hand data is provided via Unity's Hands package, and head pose is provided through the Input System.\n\nBoth of these are available in unbounded volumes only, and accessing hand tracking requires your application to request permission to receive the data.\n\nAugmented reality data such as detected planes, the world mesh, and image markers are available through ARKit and Unity's AR Foundation.\n\nLike hands and head pose, AR data is only available in unbounded volumes and requires extra permissions.\n\nFinally, Bluetooth devices such as keyboards, controllers, and other supported devices are available for you to access through Unity's Input System.\n\nBecause some types of input are only available in unbounded volumes, you'll need to decide what type of interaction you would like to build.\n\nUsing look and tap will allow your content to work in a bounded volume that can live alongside other applications, but if you need access to hand tracking or augmented reality data, you'll need to use an unbounded volume and request permissions.\n\nEach of these is delivered to your Unity application via an appropriate mechanism.\n\nThis sample uses tapping, hand tracking, and plane detection in an unbounded volume scene.\n\nYou can look at a surface found via ARKit plane detection and create flowers by dragging your finger along it.\n\nThe flowers are painted using hand tracking, and you can tap to grow flowers.\n\nFlowers that you grow react to hand movement using Unity's physics system.\n\nBy incorporating the real world into your content in this way, you can create a much deeper sense of immersion.\n\nThe best way to adapt your existing interactions depends on the type.\n\nIf you are already working with touch, such as on an iPhone, you can add appropriate input colliders and continue to use tap as your primary input mechanism.\n\nIf you are using VR controllers, you will have to redefine your interactions in terms of either tap or hand-based input, depending on how complex they are.\n\nExisting hand-based input should work without changes.\n\nAnd if you have existing UI panels using one of Unity's UI systems, you can bring them to this platform.\n\nUser interface elements built using uGUI, as well as UI Toolkit, are supported.\n\nIf you are using other UI systems, they will work as long as they use meshes and MeshRenderers or draw to a RenderTexture, which is then placed on a mesh.\n\nSupport for spatial computing on Apple platforms will be coming soon in beta based on Unity 2022.\n\nHowever, you can start getting your content ready today.\n\nIf you're starting a new project, use Unity 2022 or later.\n\nIf you have an existing project, start upgrading to 2022.\n\nIf you have handwritten shaders in your project, start converting them to Shader Graph.\n\nConsider adopting the Universal Render Pipeline.\n\nWhile the built-in graphics pipeline is supported, all future improvements will be on the Universal pipeline.\n\nIf you haven't yet, start using the Input System package.\n\nMixed-mode input is supported, but platform events are only delivered through the Input System.\n\nFinally, start thinking about how you can bring an existing app or game to spatial computing, or what new experience you'd like to create.\n\nConsider whether your idea fits in the shared space to give people more flexibility, or if your app needs the power of a full space.\n\nTo get more information about Unity's support for this platform, and to sign up for early beta access, please visit unity.com/spatial.\n\nI'm excited to see all the amazing things that you'll create with Unity and this new device.\n\nJohn: Unity is an amazing way to hit the ground running and build immersive apps.\n\nAnd it works great with RealityKit on this new platform.\n\nYou can get started today preparing your projects.\n\nIf you want to create a fully immersive experience with Unity, I recommend the session \"Bring your Unity VR app to a fully immersive space.\" And don't miss \"Build great games for spatial computing\" to get an overview of game development technologies for this platform.\n\nWe can't wait to see what you create.\n\nVladimir: Thanks for watching.\n\n♪",
    "segments": []
  },
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10088/5/77EBB91E-9B3E-41DC-AF08-623919A9F182/downloads/wwdc2023-10088_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10088/5/77EBB91E-9B3E-41DC-AF08-623919A9F182/downloads/wwdc2023-10088_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10093",
      "year": "2023",
      "title": "Bring your Unity VR app to a fully immersive space",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10093"
    },
    {
      "id": "10096",
      "year": "2023",
      "title": "Build great games for spatial computing",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10096"
    },
    {
      "id": "10260",
      "year": "2023",
      "title": "Get started with building apps for spatial computing",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10260"
    }
  ],
  "extractedAt": "2025-07-18T10:30:19.716Z"
}