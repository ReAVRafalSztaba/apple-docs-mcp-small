{
  "id": "315",
  "year": "2025",
  "url": "https://developer.apple.com/videos/play/wwdc2025/315/",
  "title": "Get started with MLX for Apple silicon",
  "speakers": [],
  "duration": "",
  "topics": [
    "Machine Learning & AI"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "Hi, I’m Awni. Today I’m excited to introduce you to MLX. MLX is an open source array framework purpose-built for Apple Silicon. It’s a flexible tool that can be used for basic numerical computations all the way to running the largest scale frontier machine learning models on Apple devices. If you want to generate text with large language models, generate images, audio, or even video with the latest models, MLX is for you. You can also use it to train, fine-tune, or otherwise customize your machine learning models directly on your Mac. I’ll start by telling you a bit more about what MLX is and where it’s useful. I’ll also cover the basics of using MLX in Python, including installation and basic array operations. After that, I’ll tell you about some of the key features that set MLX apart from other frameworks.\n\nNext, I’ll walk through some of the tools MLX has to make your machine learning workloads as fast as possible on Apple Silicon.\n\nIn the last section, I’ll give you a brief look at MLX Swift and show you how to get started using it.\n\nLet's dive into an introduction to MLX. MLX was built from the ground up to be fast on Apple Silicon, where it can run on the CPU or accelerated on the GPU. You can use MLX for a variety of applications, from small-scale numerical computing to large-scale machine learning.\n\nIt’s designed to be easy to use and flexible, without compromising speed and efficiency.\n\nMLX has a core API which closely follows NumPy. You can often use it as a drop-in replacement to accelerate most numerical computations. MLX also has all the tools you need for machine learning, including automatic differentiation and higher level libraries. These higher level APIs are similar to PyTorch and JAX. If you’re coming from any of those frameworks, MLX will be familiar and even easier to get started with.\n\nYou can use MLX for more advanced machine learning directly on device.\n\nFor example, MLX is used by LM Studio, a popular application for generating text with large language models directly on your Mac.\n\nYou can use the MLX LM package, which is built on top of MLX, to generate text and fine-tune language models up to hundreds of billions of parameters in size. To learn more about that, check out the session, “Explore large language models on Apple silicon with MLX”. MLX has a fully-featured Python API, which is useful for rapid prototyping. It also has an API in Swift, which includes all the high-level packages for building and optimizing neural networks.\n\nMLX also has APIs in C++ and C. You can use MLX in any of these languages to run the latest machine learning models on Apple Silicon, including Mac, iPhone, iPad, and Vision Pro.\n\nAll of the MLX software is open source under a permissive MIT license. The core software is available on GitHub, along with several examples and packages built using the Python and Swift APIs. MLX also has an active community of model creators on Hugging Face. Many of the latest models are already in the MLX community Hugging Face organization, and new models are uploaded every day.\n\nThe easiest way to get started with MLX in Python is to install it from PyPi. You only need to run one line in your terminal, pip3 install mlx.\n\nMLX is easy to use. To start performing computations on arrays, simply open up a Python file and import MLX. Then you can make some arrays and do basic operations on them. For example, here we’re adding two integer arrays.\n\nYou can also easily inspect information about an array, such as its shape and data type.\n\nAs I said before, the MLX Python API is similar to NumPy. The operations usually have the same names and signatures and behave the same way. If you are coming from NumPy or a similar framework, MLX will be familiar and easy to get started with. Now that you know what MLX is, where it’s useful, and some of the basics, let’s learn about the key features. These include unified memory, lazy evaluation, function transformations, and some higher-level packages for building and optimizing neural networks.\n\nMLX is designed to take advantage of the best of Apple Silicon. This includes a new programming model specific to unified memory.\n\nMost systems commonly used for machine learning have a discrete GPU with separate memory. Apple Silicon, on the other hand, has a unified memory architecture. This means that the CPU and the GPU share the same physical memory.\n\nTo work with unified memory, MLX is different from what you may be used to in traditional frameworks.\n\nIn traditional frameworks, computation follows data. If the array is in CPU memory, the computation happens on the CPU. If the array is in GPU memory, the computation happens on the GPU.\n\nIn MLX, arrays are allocated in unified memory. You never need to copy them anywhere to  use them on any of the supported devices.\n\nInstead, to run an operation on a device, you specify the device to the operation itself.\n\nFor example, here we’re adding a and b on the GPU and multiplying a and b on the CPU.\n\nThese operations can even run in parallel, and MLX will automatically manage dependencies when they exist. Another key feature of MLX is lazy computation. To make MLX as efficient as possible, Especially for very large computations, MLX has a lazy execution engine.\n\nWhen an operation like an addition on two arrays is called, no actual computation happens. Instead, a computation graph is built, like the one you see here.\n\nThe array C is not yet computed. It only gets computed if you actually need to use it. For example, printing C or converting C out of MLX and into a Python list will force it to be computed.\n\nYou can explicitly force the graph to be evaluated using mx.eval.\n\nLazy computation has several benefits. By decoupling the building and execution of the computation graph, MLX can do transformations and optimizations on the graph before computing the results. Also, with lazy computation, you only ever pay for what you use. Function transformations are another key feature of MLX. They elevate it from a fast array framework to a much more powerful tool for training and improving the performance of machine learning models.\n\nFunction transformations are transformations which take functions as input and return new functions as output.\n\nMLX has several function transformations. They typically fall into one of two categories. Transformations for automatic differentiation are transformations to optimize the compute graph. For example, you can use a function transformation to automatically compute the gradient of any function in MLX.\n\nSuppose you have a basic function which computes the sine of its input. To take the gradient of this function, you can use the mx.grad function transformation.\n\nThe output of mx.grad is a new function, which, when you call it on an array, gives you the derivative. Function transformations are arbitrarily composable. You can take the second derivative by simply using mx.grad on the output of mx.grad.\n\nThe result is another function, which, when called on an array, gives you the second derivative of the sign.\n\nMLX also has two higher-level packages to make building and training neural networks easy.\n\nThe first is mlx.nn, a modular library used to build neural networks.\n\nThe second is mlx.optimizers, a library of common optimization algorithms. The two packages can be used standalone, but also seamlessly integrate with one another.\n\nThe mlx.nn package has all the functionality you need to build neural networks. nn.module is the primary base class that all layers and containers inherit from. It exposes helpful methods for accessing parameters, loading parameters, saving parameters, and more.\n\nThe nn library also comes with a set of standard off-the-shelf layers, like nn.Linear, but you can also build your own layers by inheriting from nn.Module.\n\nCommonly used loss functions and initialization methods are also included in the nn.losses and nn.init subpackages.\n\nLet’s take a look at how you can build a simple multilayer neural network with mlx.nn.\n\nThe first step is to make a custom class that inherits from nn.Module. In this case, we’ll use a simple one hidden layer neural network. We create the linear layers inside the initialization method of the module. Then we implement the call function, which computes the output of the module on the given input. The call function calls the first linear layer, applies the relu activation function, and then calls the second linear layer. While MLX is designed and optimized for Apple Silicon’s unified memory architecture, to make it easy to get started with, the higher level neural network package is also designed to be similar to commonly used machine learning frameworks, like PyTorch.\n\nLet’s compare the MLX model implementation to the same implementation in PyTorch. They're almost identical, with only two small differences in the function which computes the output.\n\nIf you've built models in PyTorch before, then switching to MLX should be very straightforward. Now that you’ve seen most of the core features of MLX, let’s talk about how to use it to make your machine learning workloads as fast and efficient as possible. I'll start by showing you how to compile functions to speed them up. Then I’ll tell you about the mx.fast sub-package, which has off-the-shelf fast implementations of common machine learning operations, and an API for adding your own custom Metal Kernels.\n\nAfter that, I’ll show you how to use quantization to reduce memory use and speed up model execution. Lastly, I'll show you how to use MLX to distribute a computation across many machines.\n\nAlmost every realistic computation MLX will consist of functions which perform several operations on arrays. A simple way to make functions like that faster is with the mx.compile function transformation.\n\nSuppose you have a function which does a few element-wise operations, like the GELU activation function shown here.\n\nThe computation graph for this function has several nodes in it. Each of these nodes corresponds to a separate GPU Kernel launch under the hood.\n\nCompiling the graph uses all of these separate Kernels into single fused Kernel. This can save memory bandwidth and graph execution overhead and result in a much more efficient computation.\n\nUsing mx.compile is as easy as decorating the function with the mx.compile function transformation.\n\nCompilation often works well, but for more complex operations, including some common operations in machine learning, it’s likely they can run faster using the mx.fast sub package. For example, many of the core building blocks of a transformer model use operations in mx.fast. These include the positional encodings, normalization layers, and the scale dot product attention. The operations in mx.fast are more specialized, but highly tuned to be as fast as possible for both training and inference. They are also highly configurable, so they can accelerate many variations of a given computation. For example, the scale dot product attention operation can take an optional mask as input. The mask can be an additive mask, a Boolean mask, or a string indicating the mask type. Let’s take a closer look at RMS norm, which is one of the operations in mx.fast.\n\nRMS norm is used in nearly every modern transformer-based large language model.\n\nA basic implementation using MLX operations results in a large computation graph.\n\nInstead, we can replace the entire implementation with a single call to mx.fast.rms_norm. The code is simpler, the computation graph only has a single node, and the computation itself will be much faster.\n\nMLX has an API for adding custom Metal Kernels for cases where your function could benefit from a more customized implementation, and it’s not already in mx.fast.\n\nYou write the custom Kernel and MLX handles all the rest, including just-in-time compilation and execution. These Kernels are written in Metal, which is a language and API developed by Apple to run functions on Apple GPUs.\n\nYou build the Kernel by passing in a source string of Metal code, as well as some information about the inputs and outputs.\n\nYou call the Kernel by specifying the grid size and the shapes and types of the output. MLX treats the Kernel call the same way as any other operation. It creates a node in the computation graph and is lazily evaluated.\n\nAnother tool in our toolkit for making your machine learning workloads faster is quantization.\n\nLarge models need lots of memory and lots of memory bandwidth to be fast. In many cases, the precision you need for training is much higher than what you actually need for inference to get the same or almost as good quality. Reducing the precision lets you fit larger models in memory and run them faster.\n\nIf your model is in 32-bit floating point precision, you can use bfloat16 or float16 as a first step to reduce the memory requirements by half. When 16-bits is too many, MLX has built-in routines for quantizing arrays to be even smaller and doing operations with them.\n\nFor example, you can quantize using 4-bits per element to shrink memory requirements even further.\n\nTo quantize a matrix, use mx.quantize. You specify the number of bits to use per element and the group size. The group size determines the number of elements in the quantized matrix that get a shared scale and bias value. The smaller the bits and the larger the group size, the smaller and faster the result. MLX has several options for bits and group sizes to give you as much flexibility as possible.\n\nYou can multiply any un-quantized input vector or matrix by your quantized matrix using mx.quantized_matmul.\n\nUse mx.dequantize to recover an approximation to the original input.\n\nMLX.mn also has a handy utility to quantize a module in a single command. Let’s say you have a model that is a stack of an embedding layer followed by several linear layers.\n\nyou can quantize the model with nn.quantize. The quantize command also takes an optional callback to let you have more fine-grained control over which layers to quantize and what precision to use for a given layer.\n\nWhen generating text with large language models, quantization dramatically reduces the memory use and increases the tokens generated per second.\n\nIn some cases, one machine is simply not enough. For example, you might want to generate text with a large model that doesn’t fit in the memory of a single machine.\n\nOr you might be fine tuning or evaluating a model on a large data set, both of which are easy to parallelize and can be much faster using multiple machines.\n\nMLX comes out of the box with the ability to run arbitrary computations on multiple machines. The machines can be connected over ethernet or Thunderbolt.\n\nYou use the mx.distributed sub-package to distribute computations across multiple machines.\n\nMX distributed is mostly a set of communication operations.\n\nFor example, all_Sum adds the input arrays across all machines. The output of all_Sum is the summed up input, which is the same for each machine.\n\nLet’s take a closer look at how to sum an array over multiple machines. Initialize the distributed backend using mx.distributed.init. This step is optional, but it’s what you call if you need access to the communication group.\n\nThe communication group has useful information attached to it, like the total number of processes and the current process index.\n\nThen make an array with a single value on each process and call mx.distributed.all_sum to sum the arrays across all processes.\n\nMLX has a handy launcher for running an MLX program on multiple machines. To run a program on 4 machines, use mlx.launch with the 4 host IP addresses. Everything I’ve gone over so far has been using MLX in Python.\n\nIn many cases, you may prefer the ease and flexibility of Python. In other cases, you may prefer Swift. For that reason, MLX has a fully featured API in Swift.\n\nIt is built on top of Metal and works great across macOS, iOS, iPadOS, visionOS, and more.\n\nGetting started with MLX in Swift is as easy as adding it as a package to your Xcode project. Click on the project and then click on the plus sign in the package dependencies tab.\n\nThen enter the URL for the MLX Swift GitHub repository and click the add package button.\n\nThat’s all it takes to start building with MLX Swift.\n\nTo make it as easy as possible to transition between Python and Swift, the APIs between the two languages are intentionally similar. Here’s a side-by-side comparison of a Python code snippet we saw earlier with its MLX Swift counterpart.\n\nmaking arrays, performing operations on them, and inspecting metadata about the array is almost the same in Swift as in Python. All of the core features we went through using MLX in Python, as well as the optimizations we discussed in the accelerating MLX section, apply equally to MLX Swift. We've gone through a broad overview of many of the key features of MLX. To learn more about the framework, check out the MLX website, which has links to the documentation, examples, and more. Both the Python and Swift APIs have an examples repo, which contain many examples of common machine learning use cases, including language model training and generation, image generation, speech recognition, and many more. These examples are a great way to learn more about MLX, and good starting points to build with MLX in your own project. Thank you for watching, and we’re excited to see what you build with MLX.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "3:48",
      "title": "Basics",
      "language": "swift",
      "code": "import mlx.core as mx\n\n# Make an array\na = mx.array([1, 2, 3])\n\n# Make another array\nb = mx.array([4, 5, 6])\n\n# Do an operation\nc = a + b\n\n# Access information about the array\nshape = c.shape\ndtype = c.dtype\n\nprint(f\"Result c: {c}\")\nprint(f\"Shape: {shape}\")\nprint(f\"Data type: {dtype}\")"
    },
    {
      "timestamp": "5:31",
      "title": "Unified memory",
      "language": "swift",
      "code": "import mlx.core as mx\n\na = mx.array([1, 2, 3])\nb = mx.array([4, 5, 6])\n\nc = mx.add(a, b, stream=mx.gpu)\nd = mx.multiply(a, b, stream=mx.cpu)\n\nprint(f\"c computed on the GPU: {c}\")\nprint(f\"d computed on the CPU: {d}\")"
    },
    {
      "timestamp": "6:20",
      "title": "Lazy computation",
      "language": "swift",
      "code": "import mlx.core as mx\n\n# Make an array\na = mx.array([1, 2, 3])\n\n# Make another array\nb = mx.array([4, 5, 6])\n\n# Do an operation\nc = a + b\n\n# Evaluates c before printing it\nprint(c)\n\n# Also evaluates c\nc_list = c.tolist()\n\n# Also evaluates c\nmx.eval(c)\n\nprint(f\"Evaluate c by converting to list: {c_list}\")\nprint(f\"Evaluate c using print: {c}\")\nprint(f\"Evaluate c using mx.eval(): {c}\")"
    },
    {
      "timestamp": "7:32",
      "title": "Function transformation",
      "language": "swift",
      "code": "import mlx.core as mx\n\ndef sin(x):\n    return mx.sin(x)\n\ndfdx = mx.grad(sin)\n\ndef sin(x):\n    return mx.sin(x)\n\nd2fdx2 = mx.grad(mx.grad(mx.sin))\n\n# Computes the second derivative of sine at 1.0\nd2fdx2(mx.array(1.0))"
    },
    {
      "timestamp": "9:16",
      "title": "Neural Networks in MLX",
      "language": "swift",
      "code": "import mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\n\nclass MLP(nn.Module):\n    \"\"\"A simple MLP.\"\"\"\n\n    def __init__(self, dim, h_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, h_dim)\n        self.linear2 = nn.Linear(h_dim, dim)\n\n    def __call__(self, x):\n        x = self.linear1(x)\n        x = nn.relu(x)\n        x = self.linear2(x)\n        return x"
    },
    {
      "timestamp": "9:57",
      "title": "MLX and PyTorch",
      "language": "swift",
      "code": "# MLX version\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\n\nclass MLP(nn.Module):\n    \"\"\"A simple MLP.\"\"\"\n\n    def __init__(self, dim, h_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, h_dim)\n        self.linear2 = nn.Linear(h_dim, dim)\n\n    def __call__(self, x):\n        x = self.linear1(x)\n        x = nn.relu(x)\n        x = self.linear2(x)\n        return x\n\n# PyTorch version\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLP(nn.Module):\n    \"\"\"A simple MLP.\"\"\"\n\n    def __init__(self, dim, h_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, h_dim)\n        self.linear2 = nn.Linear(h_dim, dim)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = x.relu()\n        x = self.linear2(x)\n        return x"
    },
    {
      "timestamp": "11:35",
      "title": "Compiling MLX functions",
      "language": "swift",
      "code": "import mlx.core as mx\nimport math\n\ndef gelu(x):\n    return x * (1 + mx.erf(x / math.sqrt(2))) / 2\n\n@mx.compile\ndef compiled_gelu(x):\n    return x * (1 + mx.erf(x / math.sqrt(2))) / 2\n\nx = mx.random.normal(shape=(4,))\n\nout = gelu(x)\ncompiled_out = compiled_gelu(x)\nprint(f\"gelu:          {out}\")\nprint(f\"compiled gelu: {compiled_out}\")"
    },
    {
      "timestamp": "12:32",
      "title": "MLX Fast package",
      "language": "swift",
      "code": "import mlx.core as mx\nimport time\n\ndef rms_norm(x, weight, eps=1e-5):\n    y = x.astype(mx.float32)\n    y = y * mx.rsqrt(mx.mean(\n        mx.square(y), \n        axis=-1, \n        keepdims=True,\n    ) + eps)\n    return (weight * y).astype(x.dtype)\n\nbatch_size = 8192\nfeature_dim = 4096\niterations = 1000\n\nx = mx.random.normal([batch_size, feature_dim])\nweight = mx.ones(feature_dim)\nbias = mx.zeros(feature_dim)\n\nstart_time = time.perf_counter()\nfor _ in range(iterations):\n    y = rms_norm(x, weight, eps=1e-5)\n    mx.eval(y)\nrms_norm_time = time.perf_counter() - start_time\nprint(f\"rms_norm execution: {gelu_time:0.4f} sec\")\n\nstart_time = time.perf_counter()\nfor _ in range(iterations):\n    mx.eval(mx.fast.rms_norm(x, weight, eps=1e-5))\nfast_rms_norm_time = time.perf_counter() - start_time\nprint(f\"mx.fast.rms_norm execution: {compiled_gelu_time:0.4f} sec\")\n\nprint(f\"mx.fast.rms_norm speedup: {rms_norm_time/fast_rms_norm_time:0.2f}x\")"
    },
    {
      "timestamp": "13:30",
      "title": "Custom Metal kernel",
      "language": "swift",
      "code": "import mlx.core as mx\n\n# Build the kernel\nsource = \"\"\"\n    uint elem = thread_position_in_grid.x;\n    out[elem] = metal::exp(inp[elem]);\n\"\"\"\nkernel = mx.fast.metal_kernel(\n    name=\"myexp\",\n    input_names=[\"inp\"],\n    output_names=[\"out\"],\n    source=source,\n)\n\n# Call the kernel on a sample input\nx = mx.array([1.0, 2.0, 3.0])\nout = kernel(\n    inputs=[x],\n    grid=(x.size, 1, 1),\n    threadgroup=(256, 1, 1),\n    output_shapes=[x.shape],\n    output_dtypes=[x.dtype],\n)[0]\nprint(out)"
    },
    {
      "timestamp": "14:41",
      "title": "Quantization",
      "language": "swift",
      "code": "import mlx.core as mx\n\nx = mx.random.normal([1024])\nweight = mx.random.normal([1024, 1024])\n\nquantized_weight, scales, biases = mx.quantize(\n        weight, bits=4, group_size=32,\n)\n\ny = mx.quantized_matmul(\n    x,\n    quantized_weight,\n    scales=scales,\n    biases=biases,\n    bits=4,\n    group_size=32,\n)\n\nw_orig = mx.dequantize(\n    quantized_weight,\n    scales=scales,\n    biases=biases,\n    bits=4,\n    group_size=32,\n)"
    },
    {
      "timestamp": "15:23",
      "title": "Quantized models",
      "language": "swift",
      "code": "import mlx.nn as nn\n\nmodel = nn.Sequential(\n    nn.Embedding(100, 32),\n    nn.Linear(32, 32),\n    nn.Linear(32, 32),\n    nn.Linear(32, 1),\n)\n\nprint(model)\n\nnn.quantize(\n    model,\n    bits=4,\n    group_size=32,\n)\n\nprint(model)"
    },
    {
      "timestamp": "16:50",
      "title": "Distributed",
      "language": "swift",
      "code": "import mlx.core as mx\n\ngroup = mx.distributed.init()\n\nworld_size = group.size()\nrank = group.rank()\n\nx = mx.array([1.0])\n\nx_sum = mx.distributed.all_sum(x)\n\nprint(x_sum)"
    },
    {
      "timestamp": "17:20",
      "title": "Distributed launcher",
      "language": "swift",
      "code": "mlx.launch --hosts ip1, ip2, ip3, ip4 my_script.py"
    },
    {
      "timestamp": "18:20",
      "title": "MLX Swift",
      "language": "swift",
      "code": "// Swift\nimport MLX\n\n// Make an array\nlet a = MLXArray([1, 2, 3])\n\n// Make another array\nlet b = MLXArray([1, 2, 3])\n\n// Do an operation\nlet c = a + b\n\n// Access information about the array\nlet shape = c.shape\nlet dtype = c.dtype\n\n// Print results\nprint(\"a: \\(a)\")\nprint(\"b: \\(b)\")\nprint(\"c = a + b: \\(c)\")\nprint(\"shape: \\(shape)\")\nprint(\"dtype: \\(dtype)\")"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/315/4/7321e1ec-e5c2-45a5-a0ea-5e84afc61e0b/downloads/wwdc2025-315_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/315/4/7321e1ec-e5c2-45a5-a0ea-5e84afc61e0b/downloads/wwdc2025-315_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "298",
      "year": "2025",
      "title": "Explore large language models on Apple silicon with MLX",
      "url": "https://developer.apple.com/videos/play/wwdc2025/298"
    }
  ],
  "extractedAt": "2025-07-18T10:38:59.584Z"
}