{
  "id": "276",
  "year": "2025",
  "url": "https://developer.apple.com/videos/play/wwdc2025/276/",
  "title": "What’s new in BNNS Graph",
  "speakers": [],
  "duration": "",
  "topics": [
    "Machine Learning & AI"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "Hi, my name is Simon Gladman, and I work for the Vector and Numerics Group here at Apple. Our group provides a suite of libraries for high-performance, energy-efficient computation on the CPU. These libraries support, among other things, image and signal processing, linear algebra, and what I’ll be talking about today, machine learning.\n\nBasic Neural Network Subroutines, or BNNS, is our machine learning library. It allows you to add CPU-based inference to your app, and is perfectly suited to real-time and low-latency use cases, such as audio processing. You might use it to implement functionalities such as: separating audio to isolate or remove vocals, segmenting audio into different regions based on content, or applying timbre transfer to make one instrument sound like another.\n\nBut BNNS is also perfectly suited to working with other data, such as images. So if your app requires high performance inference, BNNS is for you.\n\nLast year we introduced BNNSGraph, an exciting new API that made BNNS faster, more energy efficient and far far easier to work with. And if you recall we demonstrated BNNSGraph by creating this Bitcrusher audio unit that shows just how easy it is to add your own effects to Logic Pro and Garage Band.\n\nThis year, we’re giving BNNSGraph an even better programming interface that allows you to leverage the power of Swift to create small models for inference and graphs of operations for pre- and post-processing. Today, I’ll be talking about our new addition to BNNSGraph, BNNSGraphBuilder. I’ll start today’s session with a quick recap of BNNSGraph. What it is, how it can optimize machine learning and AI models and how you can adopt it. Then I’ll introduce BNNSGraphBuilder. I’ll summarize the super easy workflow you need to implement it and as part of this introduction, I’ll demonstrate how to write a simple graph. After introducing BNNSGraphBuilder, I run through three demonstrations. Firstly preprocessing an image using the Graph Builder. Then using the new API to postprocess data generated by an ML model. And finally I’ll update last year’s Bitcrusher demonstration to build the graph of operations in Swift. And now, without further ado, let’s get started. We recommend BNNSGraph for audio and other latency-sensitive use cases because it gives you control over allocating memory and multithreading. Both of these may incur a context switch into kernel code and lead to defeat of real-time deadlines.\n\nBefore last year, you would build BNNS based networks using discrete layers such as convolution, matrix multiplication or activation.\n\nIf you wanted to use BNNS to implement an existing model, you’d have to code each layer as a BNNS primitive and write the code for all of the intermediate tensors. BNNSGraph takes an entire graph that consists of multiple layers and the data flow between those layers, and consumes it as a single graph object.\n\nFor you, this means you don’t have to write the code for each individual layer. Furthermore, you and your users will benefit from faster performance and better energy efficiency.\n\nBecause BNNSGraph is aware of the entire model, it is able to perform a number of optimizations that were not possible before. These optimizations reduce execution time, memory usage and energy cost. And even better, they come for free. Let’s use this small section of a model to explore some of these optimizations. One of the optimizations is a mathematical transformation. For example, reordering a slice operation so that BNNS only needs to compute the subset of elements in that slice. Another optimization is layer fusion. For example, fusing a convolution layer and an activation layer into a single operation. And copy elision avoids copying the data in a slice by referencing that subset of data instead. And by ensuring tensors share memory where possible, BNNSGraph optimizes memory usage and eliminates unnecessary allocations.\n\nFinally, BNNSGraph’s weight repacking optimizations can repack weights to provide better cache locality. You don’t need to write any code to benefit from these optimizations. They happen just like that. To create a graph using the file-based API that we introduced last year, you start off with a CoreML Package. Xcode automatically compiles the package to an mlmodelc file and then you write the code that builds a graph from the mlmodelc file. The last step is to create a context to wrap the graph and it’s that context that performs inference. This approach is still the best way to integrate existing PyTorch models into your project. However, for small models or graphs of operations, you might benefit from a more immediate workflow. What if you could define the individual elements of your graph in Swift? Well, this year we’re introducing a new API that does just that BNNSGraphBuilder.\n\nThe new BNNSGraphBuilder API enables you to write graphs of operations using the familiar Swift language. You can write pre- and post-processing routines or small machine learning models directly in Swift. You create a context from that Swift code with a single function call and no intermediate steps. Writing your graphs in Swift and inline with your other code brings some immediate benefits. You use a familiar language and your graph is type-checked when your Swift project is compiled. You can also share values between Swift and your graph that are known at runtime but before you generate the graph. For example, if you know a tensor’s shape at runtime before you initialise the graph, you can pass the shape directly into the graph code, and benefit from the better performance of static sizes over flexible sizes.\n\nBNNSGraphBuilder also allows you to query intermediate tensors for properties such as shape and data type, and this helps you to debug graphs. And type tensors allows Xcode to autocomplete and reduces the chance of runtime errors. Let’s look at the new syntax, type checking, new methods and operators, and how to query intermediate tensors, all features that were not previously available.\n\nThis year we’ve added a new type method to the BNNSGraph object, named makeContext. It is this new method that converts your Swift code to a reusable context that you use to execute your graph. You only make the context once, typically while your app is starting up. Then your app executes the context when required, and it benefits from the optimizations that treating the graph holistically brings. Now for some live coding to see makeContext in action.\n\nThis method accepts a closure, and it’s that closure that you use to define the series of operations that constitute your graph.\n\nYour graph will typically accept one or more arguments. Here, I’ll use the closure’s builder parameter to specify the two arguments, x and y.\n\nThe parameters are both eight element vectors of float values. In this example the graph performs an element-wise multiplication on the two input arguments, and writes the result to the output argument that I have named product. My graph also calculates the mean value of the product and writes that value to a second output argument I have named mean, and finally, the code returns those two output arguments.\n\nTo aid debugging, we can print out details of the intermediate and output tensors that BNNSGraphBuilder generates.\n\nIn this example, I’ve printed the shape of the mean to ensure it’s got a shape of one, that is, it only contains one element. Once the context is created, you can query it to derive the arguments and their positions. Here, the code queries the context’s argument names and creates an array of tensors that represent the graph’s outputs and inputs. BNNSGraph orders argument names so that the outputs are first and the inputs follow.\n\nThe BNNSGraphBuilder API provides a rich set of functions for initializing arguments from different data sources, and either copying or referencing data. In this example, the code initializes the inputs from Swift arrays.\n\nWith the input and output arguments allocated, calling execute function executes the graph and populates the two outputs with the results.\n\nAnd we can print out the results by running the function.\n\nWe’re not limited to just multiplying and calculating averages though. Let’s take a quick tour around BNNSGraphBuilder and explore some of the other operations that the new API provides. Here’s just a tiny selection of the primitives included in BNNSGraphBuilder. For example, matrix multiplication and convolution, reduction operations, gather and scatter operations, and operations such as padding, reshape and transpose. Our new API supports many operations as simple, Swift operators. So we have arithmetic operators, multiplication, addition, subtraction and division. We also have element-wise comparison operators including equality, less than, and greater than. And to complete the set, we have element-wise logical operators. OK, that’s a quick introduction to the API itself. Let’s take a deep dive into two features of the GraphBuilder API that will be very familiar Swift developers, starting with strong typing.\n\nStrong typing helps you catch errors at compile time that otherwise may happen at runtime. The BNNSGraphBuilder API ensures that the data type for tensors is correct for a given operation. Let's look at this in action. In this example, the graph returns the element-wise results of floating-point base values raised to the power of integer exponents.\n\nIt’s able to perform this calculation by casting the integer exponents to FP16.\n\nAttempting to perform this without the cast would prevent the make context method from compiling. The graph also masks the result by zeroing out elements where the values in mask 0 are less than the corresponding values in mask 1.\n\nBecause the tensor elements that the comparison generates are Boolean, the graph performs another cast in order to multiply the results by the condition. Once again, without the cast operation, the make context method wouldn’t compile. And it’s always best to catch this type of error at compile time and before your app is in the hands of your users. That's strong typing in action. Let’s now take a look at BNNSGraphBuilder’s approach to slicing, that is selecting parts with tensor. And this too will be very familiar to Swift developers.\n\nA slice is effectively a window onto a specific part of a tensor. For example, a slice may be a single column or row of a matrix. The great thing about BNNSGraph is that it treats slices as references to existing data, without the overhead of copying that data or allocating more memory. Slicing tensors is a common operation. Our new GraphBuilder API allows you to specify slices of a tensor as the source or destination operations.\n\nFor example, you might want to crop an image to a region of interest before passing it to a machine learning model. Let’s look at how easy it is to use the GraphBuilder API to select a square region from the centre of this photograph of a squirrel enjoying its lunch.\n\nWe’ll define two vImage pixel buffers. Pixel buffers store in images pixel data, dimensions, bit depth and number of channels. The first buffer, source, contains a photograph of the squirrel. The second pixel buffer, Destination, will contain the square crop. Each pixel buffer is three channels, red, green and blue, and each channel is 32-bit floating point format. You can learn more about vImage pixel buffers in our vImage documentation.\n\nThe horizontal and vertical margins ensure that the 640 by 640 crop is centered on the source image. And here’s a graph that uses the slice operation to perform the crop. First, we’ll define the source as an argument and specify the height and width as flexible sizes.\n\nPassing a negative value, minus 1 in this case, tells BNNSGraph that those dimensions might be any value. The final value in the shape 3 specifies that the image contains three channels: red, green and blue.\n\nThe BNNSGraphBuilder API uses Swift subscripts to perform a slice operation. In this example, the code uses the new SliceRange structure. The start indices for both the vertical and horizontal dimensions are the corresponding margin values. Setting the end indices as the negative margin value indicates that the end index is the end value of that dimension minus the margin.\n\nIn this example we don’t want to crop along the channel dimension, and the code specifies fillAll to ensure that we include all three channels.\n\nThis year we’re also introducing a new method for vImage pixel buffers. The withBNNSTensor method allows you to create a temporary BNNSTensor that shares memory and properties such as size and channel count with the pixel buffer. As the code here demonstrates, the new method makes passing images to and from your graph super easy. And because memory is shared, there’s no copying or allocating, and that means you get great performance when working with images.\n\nAfter the execute function method returns, the destination pixel buffer contains a cropped image. And we can double check our squirrel is safely cropped by creating a core graphics image of the result. In addition to the new GraphBuilder slice range structure, the tensor slicing API supports all the Swift range types. So, whatever your slicing needs, we have you covered. And that is a brief overview of slicing.\n\nNow we’ve taken a look at BNNSGraphBuilder and some of its features, let's dive into some use cases.\n\nBNNSGraphBuilder is a great API to construct graphs of operations for pre- and post-processing data that you pass to or receive from ML models. One pre-processing technique is to threshold an image. That is to convert a continuous tone image to a binary image that contains just black or white pixels. Let’s take a look at how we can implement this using the GraphBuilder.\n\nBefore we start writing the graph, we’ll again define some vImage pixel buffers. The first stores the source image, and the second is the destination that receives the thresholded image. Both of these pixel buffers are single channel, 16-bit floating point format.\n\nThe first step in the graph is to define the source, which represents the input image. The code passes negative values for the image size to specify the graph supports any image size, but does however specify the FP16 data type that matches the pixel buffers. Calculating the average value of the continuous grayscale pixels is simply a matter of calling the mean method. The element wise is greater than operator populates the thresholded tensor with ones for corresponding pixels of the value greater than the average pixel value, and zeros otherwise.\n\nAnd finally, the graph casts the Boolean ones and zeros to the bit-depth of the destination pixel buffer. We’ll use the withBNNSTensor method again to pass the two image buffers to the context and execute the function to generate the thresholded image.\n\nAfter passing this continuous-toned grayscale image of a pair of pelicans in flight to the graph We get this thresholded image where all the pixels are either black or white.\n\nAnother use case for the BNNSGraphBuilder API is post-processing the results of a machine learning model.\n\nFor example, let’s say we want to apply a softmax function followed by a topK operation to the results of an ML model. Here, the post-process function creates a small graph on the fly. The graph declares an input argument based on the function’s input tensor. It then applies a softmax operation to the input argument, and then calculates its topK values and indices. Note that the k parameter the code passes to the topK function is actually defined outside of the make context closure. Lastly, the graph returns the topK results. After defining the graph, the function declares the tensors that store the results, and passes the output and input tensors to the context. Finally, the makeArray method converts the topK values and indices to Swift arrays and returns those.\n\nThere we have some examples of pre- and post-processing data with the GraphBuilder API. Now, let’s take a look at last year’s Bitcrusher demonstration and update the Swift piece to use BNNSGraphBuilder. This code demonstrates how BNNSGraph makes adding ML-based audio effects super easy.\n\nLast year, we demonstrated BNNSGraph by incorporating it into an Audio Unit extension app. This app added a real-time Bitcrusher effect to an audio signal, and the demonstration used Swift to apply the same effect to a sine wave to display a visual representation of the effect in the user interface. For example, given this sine wave, the graph is able to reduce the resolution to add distortion to the sound, and the visual representation shows the sine wave as a discrete series of steps rather than a continuous wave. We also included saturation gain to add a richness to the sound. Last year’s demonstration used a CoreML package that was based on PyTorch code. So, let’s take last year’s code and compare it against the same functionality written in Swift using the new GraphBuilder API. Here’s the PyTorch on the left and the new Swift code on the right. One immediate difference is that Swift uses let and var to define intermediate tensors, so you’re free to decide if these are immutable or mutable. You don’t need any extra imports to access the full range of available operations. Furthermore, operations such as, in this case tanh, are methods on tensors rather than free functions.\n\nElement-wise arithmetic operators are the same with the two different approaches.\n\nAnother difference though is that the make context closure can return more than one output tensor, so you always return an array. The Swift GraphBuilder allows you to define input arguments inside the make context closure. If you have an existing PyTorch model, we still recommend using that code and the existing file-based API. But, for new projects, our new GraphBuilder API allows you to write graphs using Swift, and as this comparison demonstrates, with a similar structure to the same set of operations written using PyTorch. Furthermore, we can demonstrate BNNSGraph’s 16-bit support, and how, in this example, it is significantly faster than the FP32 operations. Here’s our Swift code again, but this time using a type alias to specify the precision.\n\nChanging the type alias changes the graph to use FP16 rather than FP32. In this example, FP16 is much faster than the FP32 equivalent.\n\nTo wrap up, BNNSGraphBuilder provides an easy to use Swift API that allows you to write high-performance and energy-efficient models and graphs-of-operations. It’s great for real-time and latency-sensitive use cases, but its user-friendly programming interface means it can be used for a wide range of applications. Before I go, please allow me to suggest that you head over to our documentation page where you’ll find plenty of reference material that discusses BNNSGraph and BNNSGraphBuilder. Also, be sure to check out our video from last year, where I talk about the file-based API and using BNNSGraph in C.\n\nMany thanks for your time today and best wishes.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "8:31",
      "title": "Introduction to BNNSGraphBuilder",
      "language": "swift",
      "code": "import Accelerate\n\n\n\nfunc demo() throws {\n\n    let context = try BNNSGraph.makeContext {\n        builder in\n     \n        let x = builder.argument(name: \"x\",\n                                 dataType: Float.self,\n                                 shape: [8])\n        let y = builder.argument(name: \"y\",\n                                 dataType: Float.self,\n                                 shape: [8])\n        \n        let product = x * y\n        let mean = product.mean(axes: [0], keepDimensions: true)\n        \n        // Prints \"shape: [1] | stride: [1]\".\n        print(\"mean\", mean)\n        \n        return [ product, mean]\n    }\n    \n    var args = context.argumentNames().map {\n        name in\n        return context.tensor(argument: name,\n                              fillKnownDynamicShapes: false)!\n    }\n    \n    // Output arguments\n    args[0].allocate(as: Float.self, count: 8)\n    args[1].allocate(as: Float.self, count: 1)\n    \n    // Input arguments\n    args[2].allocate(\n        initializingFrom: [1, 2, 3, 4, 5, 6, 7, 8] as [Float])\n    args[3].allocate(\n        initializingFrom: [8, 7, 6, 5, 4, 3, 2, 1] as [Float])\n\n        try context.executeFunction(arguments: &args)\n    \n    // [8.0, 14.0, 18.0, 20.0, 20.0, 18.0, 14.0, 8.0]\n    print(args[0].makeArray(of: Float.self))\n    \n    // [15.0]\n    print(args[1].makeArray(of: Float.self))\n    \n    args.forEach {\n        $0.deallocate()\n    }\n}"
    },
    {
      "timestamp": "12:04",
      "title": "Strong typing",
      "language": "swift",
      "code": "// Performs `result = mask0 .< mask1 ? bases.pow(exponents) : 0\n\nlet context = try BNNSGraph.makeContext {\n    builder in\n    \n    let mask0 = builder.argument(dataType: Float16.self,\n                                 shape: [-1])\n    let mask1 = builder.argument(dataType: Float16.self,\n                                 shape: [-1])\n    \n    let bases = builder.argument(dataType: Float16.self,\n                                 shape: [-1])\n    let exponents = builder.argument(dataType: Int32.self,\n                                     shape: [-1])\n    \n    // `mask` contains Boolean values.\n    let mask = mask0 .< mask1\n    \n    // Cast integer exponents to FP16.\n    var result = bases.pow(y: exponents.cast(to: Float16.self))\n    result = result * mask.cast(to: Float16.self)\n    \n    return [result]\n}"
    },
    {
      "timestamp": "14:15",
      "title": "Slicing",
      "language": "swift",
      "code": "let srcImage = #imageLiteral(resourceName: \"squirrel.jpeg\").cgImage(\n    forProposedRect: nil,\n    context: nil,\n    hints: nil)!\n\nvar cgImageFormat = vImage_CGImageFormat(\n    bitsPerComponent: 32,\n    bitsPerPixel: 32 * 3,\n    colorSpace: CGColorSpaceCreateDeviceRGB(),\n    bitmapInfo: CGBitmapInfo(alpha: .none,\n                             component: .float,\n                             byteOrder: .order32Host))!\n\nlet source = try vImage.PixelBuffer(cgImage: srcImage,\n                                    cgImageFormat: &cgImageFormat,\n                                    pixelFormat: vImage.InterleavedFx3.self)\n\nlet cropSize = 640\nlet horizontalMargin = (source.width - cropSize) / 2\nlet verticalMargin = (source.height - cropSize) / 2\n\nlet destination = vImage.PixelBuffer(size: .init(width: cropSize,\n                                                 height: cropSize),\n                                     pixelFormat: vImage.InterleavedFx3.self)\n\nlet context = try BNNSGraph.makeContext {\n    builder in\n    \n    let src = builder.argument(name: \"source\",\n                               dataType: Float.self,\n                               shape: [ -1, -1, 3])\n    \n    let result = src [\n        BNNSGraph.Builder.SliceRange(startIndex: verticalMargin,\n                                     endIndex: -verticalMargin),\n        BNNSGraph.Builder.SliceRange(startIndex: horizontalMargin,\n                                     endIndex: -horizontalMargin),\n        BNNSGraph.Builder.SliceRange.fillAll\n    ]\n    \n    return [result]\n}\n\nsource.withBNNSTensor { src in\n    destination.withBNNSTensor { dst in\n        \n        var args = [dst, src]\n        \n        print(src)\n        print(dst)\n        \n        try! context.executeFunction(arguments: &args)\n    }\n}\n\nlet result = destination.makeCGImage(cgImageFormat: cgImageFormat)"
    },
    {
      "timestamp": "17:31",
      "title": "Preprocessing by thresholding on mean",
      "language": "swift",
      "code": "let srcImage = #imageLiteral(resourceName: \"birds.jpeg\").cgImage(\n    forProposedRect: nil,\n    context: nil,\n    hints: nil)!\n\nvar cgImageFormat = vImage_CGImageFormat(\n    bitsPerComponent: 16,\n    bitsPerPixel: 16,\n    colorSpace: CGColorSpaceCreateDeviceGray(),\n    bitmapInfo: CGBitmapInfo(rawValue: CGBitmapInfo.byteOrder16Little.rawValue |\n                             CGBitmapInfo.floatComponents.rawValue |\n                             CGImageAlphaInfo.none.rawValue))!\n\nlet source = try! vImage.PixelBuffer<vImage.Planar16F>(cgImage: srcImage,\n                                                       cgImageFormat: &cgImageFormat)\nlet destination = vImage.PixelBuffer<vImage.Planar16F>(size: source.size)\n\nlet context = try BNNSGraph.makeContext {\n    builder in\n    \n    let src = builder.argument(name: \"source\",\n                               dataType: Float16.self,\n                               shape: [-1, -1, 1])\n    \n    let mean = src.mean(axes: [0, 1], keepDimensions: false)\n    \n    let thresholded = src .> mean\n    \n    let result = thresholded.cast(to: Float16.self)\n    \n    return [result]\n}\n\nsource.withBNNSTensor { src in\n    destination.withBNNSTensor { dst in\n        \n        var args = [dst, src]\n        \n        try! context.executeFunction(arguments: &args)\n    }\n}\n\nlet result = destination.makeCGImage(cgImageFormat: cgImageFormat)"
    },
    {
      "timestamp": "19:04",
      "title": "Postprocessing",
      "language": "swift",
      "code": "func postProcess(result: BNNSTensor, k: Int) throws -> ([Float32], [Int32]) {\n    \n    let context = try BNNSGraph.makeContext {\n        builder in\n        \n        let x = builder.argument(dataType: Float32.self,\n                                 shape: [-1])\n        \n        let softmax = x.softmax(axis: 1)\n        \n        let topk = softmax.topK(k, axis: 1, findLargest: true)\n        \n        return [topk.values, topk.indices]\n    }\n    \n    let indices = context.allocateTensor(argument: context.argumentNames()[0],\n                                         fillKnownDynamicShapes: false)!\n    let values = context.allocateTensor(argument: context.argumentNames()[1],\n                                        fillKnownDynamicShapes: false)!\n    \n    var arguments = [values, indices, result]\n    \n    try context.executeFunction(arguments: &arguments)\n    \n    return (values.makeArray(of: Float32.self), indices.makeArray(of: Int32.self))\n}"
    },
    {
      "timestamp": "21:03",
      "title": "Bitcrusher in PyTorch",
      "language": "swift",
      "code": "import coremltools as ct\nfrom coremltools.converters.mil import Builder as mb\nfrom coremltools.converters.mil.mil import (\n    get_new_symbol\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BitcrusherModel(nn.Module):\n    def __init__(self):\n        super(BitcrusherModel, self).__init__()\n\n    def forward(self, source, resolution, saturationGain, dryWet):\n        # saturation\n        destination = source * saturationGain\n        destination = F.tanh(destination)\n\n        # quantization\n        destination = destination * resolution\n        destination = torch.round(destination)\n        destination = destination / resolution\n\n        # mix\n        destination = destination * dryWet\n        destination = 1.0 - dryWet\n        source = source * dryWet\n        \n        destination = destination + source\n        \n        return destination"
    },
    {
      "timestamp": "21:03",
      "title": "Bitcrusher in Swift",
      "language": "swift",
      "code": "typealias BITCRUSHER_PRECISION = Float16\n    \nlet context = try! BNNSGraph.makeContext {\n    builder in\n    \n    var source = builder.argument(name: \"source\",\n                                  dataType: BITCRUSHER_PRECISION.self,\n                                  shape: [sampleCount, 1, 1])\n    \n    let resolution = builder.argument(name: \"resolution\",\n                                      dataType: BITCRUSHER_PRECISION.self,\n                                      shape: [1, 1, 1])\n    \n    let saturationGain = builder.argument(name: \"saturationGain\",\n                                          dataType: BITCRUSHER_PRECISION.self,\n                                          shape: [1, 1, 1])\n    \n    var dryWet = builder.argument(name: \"dryWet\",\n                                  dataType: BITCRUSHER_PRECISION.self,\n                                  shape: [1, 1, 1])\n    \n    // saturation\n    var destination = source * saturationGain\n    destination = destination.tanh()\n    \n    // quantization\n    \n    destination = destination * resolution\n    destination = destination.round()\n    destination = destination / resolution\n    \n    // mix\n    destination = destination * dryWet\n    dryWet = BITCRUSHER_PRECISION(1) - dryWet\n    source = source * dryWet\n    \n    destination = destination + source\n    \n    return [destination]\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "BNNS",
        "url": "https://developer.apple.com/documentation/Accelerate/bnns-library"
      },
      {
        "title": "Supporting real-time ML inference on the CPU",
        "url": "https://developer.apple.com/documentation/Accelerate/supporting-real-time-ml-inference-on-the-cpu"
      },
      {
        "title": "vImage.PixelBuffer",
        "url": "https://developer.apple.com/documentation/Accelerate/vImage/PixelBuffer"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/276/4/68973e7e-a4db-44eb-9c9f-f90f3dde4a75/downloads/wwdc2025-276_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2025/276/4/68973e7e-a4db-44eb-9c9f-f90f3dde4a75/downloads/wwdc2025-276_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "360",
      "year": "2025",
      "title": "Discover machine learning & AI frameworks on Apple platforms",
      "url": "https://developer.apple.com/videos/play/wwdc2025/360"
    },
    {
      "id": "10211",
      "year": "2024",
      "title": "Support real-time ML inference on the CPU",
      "url": "https://developer.apple.com/videos/play/wwdc2024/10211"
    }
  ],
  "extractedAt": "2025-07-18T10:38:52.008Z"
}