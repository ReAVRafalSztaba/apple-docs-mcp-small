{
  "id": "10653",
  "year": "2020",
  "url": "https://developer.apple.com/videos/play/wwdc2020/10653/",
  "title": "Detect Body and Hand Pose with Vision",
  "speakers": [],
  "duration": "",
  "topics": [
    "Machine Learning & AI"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "â™ª Hello and welcome to WWDC.\n\nBrett Keating: Hello everyone. My name is Brett Keating.\n\nI hope you're all enjoying WWDC 2020.\n\nLet's continue your journey and talk about some new API's in Vision this year.\n\nLet's hear all about how to obtain body and hand pose using the Vision framework.\n\nOne of the major themes of the Vision framework is how it can be used to help you understand people in your visual data.\n\nBack when Vision framework was first introduced, it came with a new face detector that was based on deep learning technology.\n\nAnd since that time, it had been improved upon with a second revision that could detect upside down faces.\n\nAlso arriving with the debut of Vision was face landmark detection, which has also seen improvements with new revisions.\n\nLast year we started giving you a new, richer set of landmarks that infer pupil locations.\n\nAlso new last year is human torso detection.\n\nAnd now I'm excited to show you what's new in the People theme for Vision framework this year.\n\nWe will be providing hand pose and we will be providing human body pose from Vision framework.\n\nLet's begin by talking about hand pose.\n\nHand pose has so many exciting possible applications.\n\nI hope you will agree and find amazing ways to take advantage of it.\n\nLook at how well it's working on this video of a child playing the ukulele.\n\nLet's go through a few examples of what can be done with hand pose.\n\nPerhaps your mission as a developer is to rid the world of selfie sticks.\n\nThe Vision framework can now help with hand pose estimation.\n\nIf you develop your app to look for specific gestures to trigger a photo capture, you can create an experience like the one my colleague Xiaoxia Sun has done here.\n\nMaybe you'd like to be able to overlay fun graphics on hands like many developers have already done with faces.\n\nIn your app you can look at the hand poses Vision gives you and if you write code to classify them, you can overlay emojis or any other kind of graphic that you choose based on the gestures you find.\n\nSo how do you use Vision for this? As we've promised since the start, all of our algorithm requests generally follow the same pattern as the others.\n\nThe first step is to create a request handler.\n\nHere we are using the ImageRequestHandler.\n\nThe next step is to create the request.\n\nIn this case, use a VNDetectHumanHandPoseRequest.\n\nThe next step is to provide the request to the handler via a call to performRequests.\n\nOnce that completes successfully, you will have your observations in the requests results property.\n\nIn this case, VNRecognizedPointsObservation are returned.\n\nThe observations contain locations for all the found landmarks for the hand.\n\nThese are given in new classes meant to represent 2D points.\n\nThese classes form an inheritance hierarchy.\n\nIf the algorithm you're using only returns a location, you'll be given a VNPoint.\n\nA VNPoint contains a CGPoint location, and the X and Y coordinates of the locations can be accessed directly if desired as well.\n\nThe coordinates use the same lower left origin as other Vision algorithms and are also returned to normalized coordinates relative to the pixel dimensions of your image.\n\nIf the algorithm you're using also has a confidence associated with it, you will get VNDetectedPoint objects.\n\nFinally, if the algorithm is also labelling the points, you will get VNRecognizedPoint objects.\n\nFor hand pose, VNRecognizedPoint objects are returned.\n\nHere's how you access these points from a hand pose observation.\n\nFirst you will request a dictionary of the landmarks by calling .recognizedPoints (forGroupKey: on the observation.\n\nI will go into more detail about group keys in a minute but know that if you want all the points, you will use VNRecognizedPointGroupKeyAll.\n\nWe provide other VNRecognizedPointGroupKeys, such as for only those points that are part of the index finger, and you can access the landmarks by iterating over them.\n\nOr as in this example, access a particular one for the tip of the index finger by specifying it's VNRecognizedPointKey.\n\nHere I am showing you a quick overview of the hand landmarks that are returned.\n\nThere are four for each finger and thumb and one for the wrist, for a total of twenty-one hand landmarks.\n\nAs I just mentioned, we have a new type this year called VNRecognizedPointGroupKey.\n\nEach of the hand landmarks belong to at least one of these groups.\n\nHere is the definition for the group key for the index finger and here are the landmarks that are contained in that group displayed visually on the hand.\n\nAlso, in the code example shown before, I showed you one of the VNRecognizedPointKey that you can use to retrieve the tip of the index finger.\n\nLet's see what this looks like for the remaining parts of the finger.\n\nGoing down towards the hand, the first joint is the distal interphalangeal joint, which we abbreviate as DIP.\n\nNotice that the VNRecognizedPointKey type uses this abbreviation to distinguish itself.\n\nContinuing along, the next joint is the proximal interphalangeal joint, which we abbreviate as PIP.\n\nFinally, at the base of the fingers is the metacarpophalangeal joint, which for fingers we abbreviate as MCP.\n\nAll these four points I mentioned are retrievable by the index fingers group key.\n\nThe pattern repeats for each of the fingers.\n\nAs an example, here is the group key and associated landmark keys for the ring finger.\n\nThe thumb is a bit different.\n\nThe thumb also has a tip.\n\nThe first joint is the interphalangeal joint which we abbreviate as IP.\n\nThe next joint is the metacarpophalangeal joint which for thumbs we abbreviate as MP.\n\nThe next joint for the thumb is the carpometacarpal joint, abbreviated as CMC.\n\nBelow, for reference, the corresponding group keys and landmark keys for the thumb are shown to contrast against what we provide for fingers.\n\nAnd then there is the base of the wrist, which also has it's own landmark.\n\nThe wrist landmark falls on the center of the wrist and is not part of any group except for the all group.\n\nIn other words, it's not part of any finger or thumb group.\n\nCombined with the landmarks for the fingers and thumbs, this forms the set of landmarks we identify for hands.\n\nNow let me show you our sample application for hand pose.\n\nSo with this sample, I'm using my hand to draw on the screen.\n\nWhen my finger and thumb are close together, I'll start drawing.\n\nAnd here I'm using that to write the word hello.\n\nAnd that's it! So let's see what that looks like in the code.\n\nHere I'm going to start in the CameraViewController in our capture output where we are receiving CMSampleBuffers from the camera stream.\n\nThe first thing we do is we create a VNImageRequestHandler using that sample buffer.\n\nThen we use that handler to perform our request.\n\nOur request is a VNDetectHumanHandPoseRequest.\n\nIf we find a hand, we'll be getting an observation back and from that observation we can get the thumb points and the index finger points by using their VNRecognizedPointGroupKey by calling recognizedPoints (forGroupKey.\n\nWith those collections, we can look for the fingertip and the thumb tip points, and we do that here.\n\nWe ignore any low confidence points and then at the end of this section, we convert the points from Vision coordinates to AVfoundation coordinates.\n\nThe one thing I want to draw your attention to up here is that we have set up a separate queue to process these points.\n\nSo let's go into processPoints.\n\nHere we're just checking to see if nothing's going on; if so, we reset.\n\nOtherwise, we convert our thumb and index points intoAVfoundation coordinates here.\n\nThen we process these points.\n\nWe have another class called gestureProcessor, and in that we call processPointsPair.\n\nSo here in processPointsPair, we're looking at the distance between the tip of the index finger and the tip of the thumb.\n\nIf the distance is less than a threshold, then we start accumulating evidence for whether or not we are in a pinch state or a possible pinch state.\n\nThe threshold that we have is 40, and how much evidence we require is three frames of the same pinch state.\n\nSo once we've collected three frames of the pinch state, we move from a possible pinch state to a pinch state.\n\nWe do the same thing for the apart state.\n\nIf the fingers are not meeting the threshold, we consider them apart.\n\nUntil we accumulate enough evidence, we are in the possible apart state.\n\nOnce we have enough evidence, we transition to the apart state.\n\nGoing back to the CameraViewController file, these state changes are handled in handleGestureStateChange.\n\nHere we're looking at which case we're in.\n\nIf we're in a possible pinch or a possible apart state, we want to keep track of the points that we have found in order to decide later if we want to draw them or not.\n\nAnd we collect those here.\n\nIf ultimately we end up in the pinch state, then we go ahead and we draw those points.\n\nOnce we've drawn them, we can remove them from our evidence buffer and then go ahead and continue drawing.\n\nIf we end up in the apart state, we will not draw those points; we will still remove all of those points from our evidence buffer.\n\nBut then we will update the path with the final point and indicate that with the boolean isLastPointsPair set to true.\n\nOne last thing I'd like to mention in the sample is that we have also set up a gesture handler that looks for a double tap in order to clear the screen.\n\nThat is how you use our sample for hand pose to use Visual framework to draw on the screen with your hand.\n\nVision provides a couple additional things in the API that you should know about which are meant to deal with the fact that there could be many hands in the scene.\n\nPerhaps you're only interested in the one or two biggest hands in the scene and don't want results returned for any smaller hands found in the background.\n\nYou may control this by specifying maximumHandCount on the request.\n\nThe default is two.\n\nSo if you do want more than two, it's important to adjust this parameter.\n\nSuppose you want all the detected hands which aren't too blurry or occluded.\n\nYou may set this as high as you want but know that setting this parameter to a large number will have a latency impact because the pose will be generated for every detected hand.\n\nIf the parameter is set to a lower number, pose will not be computed for any hands detected beyond the maximum requested, which can help with performance.\n\nIt is therefore recommended that you tune this parameter for your application needs with performance in mind.\n\nWhile using hand pose in Vision, it might help to take advantage of the VNTrackObjectRequest.\n\nVNTrackObjectRequest is potentially useful in hand analysis for two reasons.\n\nFirst, if all you want to do is track the location of the hands and care less about the pose, you may use a hand pose request to find the hands and then use VNTrackObjectRequest from that point forward to know where the hands are moving.\n\nSecond, if you want to be more robust about which hand is which, a tracking request may help you.\n\nThe vision tracker is good at maintaining object identifiers as things move off screen or become occluded temporarily.\n\nFor more information about the object tracker, please have a look at our WWDC session on the matter from 2018.\n\nI'm sure you're completely sold on hand pose by now and can't wait to start implementing the apps you've already begun dreaming up.\n\nBut before you do, there are a few accuracy considerations to keep in mind.\n\nHands near the edges of the screen will be partially occluded, and hand pose is not guaranteed to work well in those situations.\n\nAlso, the algorithm may have difficulty with hands that are parallel to the camera viewing direction, like those in the image of this karate chopper.\n\nHands covered by gloves may also be difficult at times.\n\nFinally, the algorithm will sometimes detect feet as hands.\n\nIt is good to keep all of these caveats in mind when designing your app, which may include instructions to your users to make the best of the experience you plan to provide them.\n\nAnd that is hand pose.\n\nMoving on, let's now discuss human body pose in Vision.\n\nNew this year, Vision is providing you with the capability to analyze many people at once for body pose, as shown in this video.\n\nSo, like we did for hand pose, let's go over some exciting ideas for apps you can develop with body pose.\n\nHow about taking better action shots? If you have the body pose available, you can identify interesting parts of the action like the apex of this jump.\n\nThat is probably the most interesting shot of the sequence.\n\nOr creating stromotion shots.\n\nHere we use human body pose to find the frames where the basketball player doesn't overlap with himself and we can create this cool looking image.\n\nHaving a look at body pose for work and safety applications might be interesting to your application domain.\n\nPerhaps it can help with training for proper ergonomics.\n\nHuman body pose can be used as part of another new feature this year as well: action classification through CreateML.\n\nMaybe you'd like to create some fitness apps that can classify an athlete as performing the action you expect, like this man performing jumping jacks.\n\nOr perhaps you want to know if these kids are having any success in attempting what is properly considered dancing.\n\nThis technology was used in the Action and Vision application.\n\nSo check out the session on that for a more in-depth look on how to bring everything together into a really cool app.\n\nTo analyze images for human body pose, the flow is very similar to that of hand pose.\n\nAs with any use of Vision, the first thing to do is to create a request handler.\n\nThe next step is to create a VNDetectHumanBodyPoseRequest, then use the request handler to perform the requests.\n\nThe difference between this example and the hand pose example is we use the word \"body\" instead of \"hand\" and that's it.\n\nLooking at the landmark points is also analogous to what is done for a hand pose.\n\nGetting all the points is done exactly the same way by calling the recognizedPoints(forGroupKey with VNRecognizedPointGroupKeyAll or you can get a specific group of landmarks, as we do for the left arm here.\n\nAnd then you may request a particular landmark by key, as we do here to get the left wrist landmark.\n\nLet's go over the landmarks for the human body.\n\nThere are VNRecognizedPointGroupKeys for each group, and here we start with the face.\n\nThe VNRecognizedPointKey values for the face include the nose, the left and right eye, and the left and right ear.\n\nLet's have a look at the right arm group next.\n\nNote that this is the subject's right arm, not the one on the right side of the image.\n\nThere are three landmarks in this group for the shoulder, the elbow, and the wrist.\n\nThe subject's left arm also has a group for the shoulder, elbow, and wrist, with associated keys listed here.\n\nThe torso is next.\n\nNote that it also contains the shoulders.\n\nSo then, the shoulder landmarks appear in more then one group.\n\nAlso included is a neck point between the shoulders, the left and right hip joints, and a root joint between the two hip joints.\n\nThe subject's right leg comes next, and note that the hip joints appear in both the torso group and each leg group.\n\nAlso in the leg group are the knee and ankle landmarks.\n\nAnd finally, same with the left leg: a hip, knee, and ankle landmark.\n\nThere are some limitations you ought to be aware of regarding body pose in Vision.\n\nIf the people on the scene are bent over or upside down, the body pose algorithm will not perform as well.\n\nAlso the pose might not be determinable due to obstructive flowing clothing.\n\nAlso as you may have noticed in the dancing example, if one person is partially occluding another in the view, it is possible for the algorithm to get confused.\n\nSimilar to hand pose, the results may get worse if the subject is close to the edges of the screen.\n\nAnd finally, the same considerations that applied to hand pose for tracking also apply to body pose.\n\nAs you may be aware, Vision is not the first framework in our SDKs to offer body pose analysis.\n\nSince last year, ARKit has been providing body pose to developers within the context of an AR session.\n\nHere we contrast that division and when it makes sense to use one versus the other.\n\nYou will get the same set of landmarks either way, but Vision provides a confidence value per point while ARKit does not.\n\nIn general, the Vision framework is capable of being used on still images or camera feeds.\n\nSo just like anything else Vision provides, human body pose from Vision can be used offline to analyze whole libraries of images.\n\nARKit's solution is designed for live motion capture applications.\n\nAlso due to its specific use case, ARKit body pose can only be used with a rear-facing camera from within an AR session on supported iOS and iPadOS devices.\n\nVision's API can be used on all supported platforms, except the watch.\n\nThe idea behind providing this API through Vision is precisely to make available Apple's body pose technology outside of an ARKit session.\n\nHowever, for most ARKit use cases, especially motion capture, you should be using ARKit to get body pose information.\n\nAs mentioned earlier in this discussion, you can use body pose in combination with CreateML for action classification.\n\nLet's go over a couple tips for how to best train your action classifier with body pose data.\n\nIf you use videos directly for training, you should understand that Vision will be used to generate body poses on your behalf.\n\nIf there is more than one subject in the video, then by default the largest one will be used.\n\nTo avoid having this default behavior applied to your training videos, it is best to ensure only the subject of interest is in the frame.\n\nYou can do this by cropping your training videos so that only a single actor is present.\n\nIf you are not using videos, you may also use ML MultiArray buffers retrieved from Vision's keypoints MultiArray method.\n\nWith this method, you exactly control which body poses are used for training.\n\nSince Vision is used during training, it should also be used for inference.\n\nAttempting to use ARKit body pose as an input to a model trained with Vision body poses will produce undefined results.\n\nFinally, please be aware that once you get to the point of performing inference, you will need to pay attention to latency on older devices.\n\nWhile the latest and greatest devices might be able to keep up with the camera stream while performing inference for action classification, older devices will not.\n\nSuppose this is your image sequence.\n\nYou will still want to retrieve body pose for every frame because the classifiers will be expecting the body poses to be sampled at that rate.\n\nBut do be mindful of holding onto camera buffers for too long.\n\nOtherwise, you may starve the stream.\n\nBut your application will perform better, especially on older model devices, if you space out the classification inferences.\n\nRecognizing an action within a fraction of a second is pretty quick for most use cases, so doing inferences only a few times per second should be OK.\n\nThe Action and Vision sample is a great example of how to use Vision and CreateML to classify human actions.\n\nHere is a short demonstration clip of the Action and Vision sample in action.\n\nIn this clip, we see my colleague Frank tossing some beanbags at a cornhole board.\n\nHe tosses the bags using a few different throwing techniques which are classified in the app using CreateML.\n\nSo let's have a look at the code in the Action and Vision sample to see how this is done.\n\nWe'll start in the GameViewController.\n\nHere in the cameraViewController, we are getting our CMSampleBuffers from the camera, and we provide those to an image request handler here.\n\nLater on we use that image request handler to perform our request.\n\nHere it is a VNDetectHumanBodyPoseRequest.\n\nIf we get a result, then we call humanBoundingBox.\n\nLets go into humanBoundingBox to see what this is doing.\n\nIn humanBoundingBox we're trying to find the human bounding box around the person.\n\nWe'll start by getting all the points out of the observation by calling recognizePoints forGroupKey all.\n\nThen we can iterate over all the points here.\n\nWe look here to see if the point has enough confidence and if it does we add it to our normalizedBoundingBox.\n\nHere we're doing something where we're trying to find the body joints for the right-hand side of the body so we can overlay them on the screen.\n\nBut since we're talking about action classification, I'll skip that for now.\n\nLater on we're going to be storing these observations.\n\nLet's look at what's happening in storeObservation and why we're doing it.\n\nIn storeObservation, we have a buffer of observations that we call poseObservations.\n\nWhat we're going to do is use that as a ring buffer.\n\nIf this buffer is full, we will remove the first one.\n\nAnd either way, we're going to append the next one.\n\nWe use this later on when a throw is detected.\n\nA throw is detected when the trajectory analysis detects a throw, and then we call this function getLastThrowType to find out what kind of throw was made.\n\nWe're going to use our actionClassifier for this but we need to put all of our observations into the correct format.\n\nSo we will call prepareInputWithObservations in order to do that.\n\nLet's look at what that function is doing.\n\nIn prepareInputWithObservations, we see that we need 60 observations and we need to put them in a multiArrayBuffer of type MLMultiArray.\n\nHere in this loop, we iterate over all the observations that we have in our buffer, and for each observation we call keypointsMultiArray and we put that into our buffer.\n\nIf we don't have 60, then we'll pad with zeros.\n\nNotice the MLMultiArray shape and dataType that's expected.\n\nAnd then we concatenate those and return them.\n\nGoing back to getLastThrowType, we provide that input into PlayerActionClassifierInput, which is our ML feature provider.\n\nOnce we have that, we provide it to our action classifier by calling prediction with the input.\n\nThen once we have the output , we look over all the probabilities to see which of the possibilities are the most likely.\n\nAnd then we return the throw type that corresponds to that maximum probability.\n\nAnd that is how you do action classification using body pose from Vision framework.\n\nNow that you've had a closer look at the code in the Action and Vision application for body pose, congratulations! Along with everything else you've learned in this session, you are prepared to go create amazing applications.\n\nApplications that do more when it comes to analyzing people with computer vision with the Vision framework.\n\nAnd that is hand and body pose in Vision.\n\nHave a great time continuing\nyour WWDC journey.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "7:07",
      "title": "HandPoseCameraViewController",
      "language": "swift",
      "code": "extension CameraViewController: AVCaptureVideoDataOutputSampleBufferDelegate {\n    public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n        var thumbTip: CGPoint?\n        var indexTip: CGPoint?\n        \n        defer {\n            DispatchQueue.main.sync {\n                self.processPoints(thumbTip: thumbTip, indexTip: indexTip)\n            }\n        }\n\n        let handler = VNImageRequestHandler(cmSampleBuffer: sampleBuffer, orientation: .up, options: [:])\n        do {\n            // Perform VNDetectHumanHandPoseRequest\n            try handler.perform([handPoseRequest])\n            // Continue only when a hand was detected in the frame.\n            // Since we set the maximumHandCount property of the request to 1, there will be at most one observation.\n            guard let observation = handPoseRequest.results?.first as? VNRecognizedPointsObservation else {\n                return\n            }\n            // Get points for thumb and index finger.\n            let thumbPoints = try observation.recognizedPoints(forGroupKey: .handLandmarkRegionKeyThumb)\n            let indexFingerPoints = try observation.recognizedPoints(forGroupKey: .handLandmarkRegionKeyIndexFinger)\n            // Look for tip points.\n            guard let thumbTipPoint = thumbPoints[.handLandmarkKeyThumbTIP], let indexTipPoint = indexFingerPoints[.handLandmarkKeyIndexTIP] else {\n                return\n            }\n            // Ignore low confidence points.\n            guard thumbTipPoint.confidence > 0.3 && indexTipPoint.confidence > 0.3 else {\n                return\n            }\n            // Convert points from Vision coordinates to AVFoundation coordinates.\n            thumbTip = CGPoint(x: thumbTipPoint.location.x, y: 1 - thumbTipPoint.location.y)\n            indexTip = CGPoint(x: indexTipPoint.location.x, y: 1 - indexTipPoint.location.y)\n        } catch {\n            cameraFeedSession?.stopRunning()\n            let error = AppError.visionError(error: error)\n            DispatchQueue.main.async {\n                error.displayInViewController(self)\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "8:29",
      "title": "HandPoseProcessPointsPair",
      "language": "swift",
      "code": "init(pinchMaxDistance: CGFloat = 40, evidenceCounterStateTrigger: Int = 3) {\n        self.pinchMaxDistance = pinchMaxDistance\n        self.evidenceCounterStateTrigger = evidenceCounterStateTrigger\n    }\n    \n    func reset() {\n        state = .unknown\n        pinchEvidenceCounter = 0\n        apartEvidenceCounter = 0\n    }\n    \n    func processPointsPair(_ pointsPair: PointsPair) {\n        lastProcessedPointsPair = pointsPair\n        let distance = pointsPair.indexTip.distance(from: pointsPair.thumbTip)\n        if distance < pinchMaxDistance {\n            // Keep accumulating evidence for pinch state.\n            pinchEvidenceCounter += 1\n            apartEvidenceCounter = 0\n            // Set new state based on evidence amount.\n            state = (pinchEvidenceCounter >= evidenceCounterStateTrigger) ? .pinched : .possiblePinch\n        } else {\n            // Keep accumulating evidence for apart state.\n            apartEvidenceCounter += 1\n            pinchEvidenceCounter = 0\n            // Set new state based on evidence amount.\n            state = (apartEvidenceCounter >= evidenceCounterStateTrigger) ? .apart : .possibleApart\n        }\n    }"
    },
    {
      "timestamp": "9:25",
      "title": "HandPoseHandleGestureStateChange",
      "language": "swift",
      "code": "private func handleGestureStateChange(state: HandGestureProcessor.State) {\n        let pointsPair = gestureProcessor.lastProcessedPointsPair\n        var tipsColor: UIColor\n        switch state {\n        case .possiblePinch, .possibleApart:\n            // We are in one of the \"possible\": states, meaning there is not enough evidence yet to determine\n            // if we want to draw or not. For now, collect points in the evidence buffer, so we can add them\n            // to a drawing path when required.\n            evidenceBuffer.append(pointsPair)\n            tipsColor = .orange\n        case .pinched:\n            // We have enough evidence to draw. Draw the points collected in the evidence buffer, if any.\n            for bufferedPoints in evidenceBuffer {\n                updatePath(with: bufferedPoints, isLastPointsPair: false)\n            }\n            // Clear the evidence buffer.\n            evidenceBuffer.removeAll()\n            // Finally, draw current point\n            updatePath(with: pointsPair, isLastPointsPair: false)\n            tipsColor = .green\n        case .apart, .unknown:\n            // We have enough evidence to not draw. Discard any evidence buffer points.\n            evidenceBuffer.removeAll()\n            // And draw the last segment of our draw path.\n            updatePath(with: pointsPair, isLastPointsPair: true)\n            tipsColor = .red\n        }\n        cameraView.showPoints([pointsPair.thumbTip, pointsPair.indexTip], color: tipsColor)\n    }"
    },
    {
      "timestamp": "10:15",
      "title": "HandPoseHandleGesture",
      "language": "swift",
      "code": "@IBAction func handleGesture(_ gesture: UITapGestureRecognizer) {\n        guard gesture.state == .ended else {\n            return\n        }\n        evidenceBuffer.removeAll()\n        drawPath.removeAllPoints()\n        drawOverlay.path = drawPath.cgPath\n    }"
    },
    {
      "timestamp": "20:48",
      "title": "ActionVisionGameViewController",
      "language": "swift",
      "code": "extension GameViewController: CameraViewControllerOutputDelegate {\n    func cameraViewController(_ controller: CameraViewController, didReceiveBuffer buffer: CMSampleBuffer, orientation: CGImagePropertyOrientation) {\n        let visionHandler = VNImageRequestHandler(cmSampleBuffer: buffer, orientation: orientation, options: [:])\n        if self.gameManager.stateMachine.currentState is GameManager.TrackThrowsState {\n            DispatchQueue.main.async {\n                // Get the frame of rendered view\n                let normalizedFrame = CGRect(x: 0, y: 0, width: 1, height: 1)\n                self.jointSegmentView.frame = controller.viewRectForVisionRect(normalizedFrame)\n                self.trajectoryView.frame = controller.viewRectForVisionRect(normalizedFrame)\n            }\n            // Perform the trajectory request in a separate dispatch queue\n            trajectoryQueue.async {\n                self.setUpDetectTrajectoriesRequest()\n                do {\n                    if let trajectoryRequest = self.detectTrajectoryRequest {\n                        try visionHandler.perform([trajectoryRequest])\n                    }\n                } catch {\n                    AppError.display(error, inViewController: self)\n                }\n            }\n        }\n        // Run bodypose request for additional GameConstants.maxPostReleasePoseObservations frames after the first trajectory observation is detected\n        if !(self.trajectoryView.inFlight && self.trajectoryInFlightPoseObservations >= GameConstants.maxTrajectoryInFlightPoseObservations) {\n            do {\n                try visionHandler.perform([detectPlayerRequest])\n                if let result = detectPlayerRequest.results?.first as? VNRecognizedPointsObservation {\n                    let box = humanBoundingBox(for: result)\n                    let boxView = playerBoundingBox\n                    DispatchQueue.main.async {\n                        let horizontalInset = CGFloat(-20.0)\n                        let verticalInset = CGFloat(-20.0)\n                        let viewRect = controller.viewRectForVisionRect(box).insetBy(dx: horizontalInset, dy: verticalInset)\n                        self.updateBoundingBox(boxView, withRect: viewRect)\n                        if !self.playerDetected && !boxView.isHidden {\n                            self.gameStatusLabel.alpha = 0\n                            self.resetTrajectoryRegions()\n                            self.gameManager.stateMachine.enter(GameManager.DetectedPlayerState.self)\n                        }\n                    }\n                }\n            } catch {\n                AppError.display(error, inViewController: self)\n            }\n        } else {\n            // Hide player bounding box\n            DispatchQueue.main.async {\n                if !self.playerBoundingBox.isHidden {\n                    self.playerBoundingBox.isHidden = true\n                    self.jointSegmentView.resetView()\n                }\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "21:19",
      "title": "ActionVisionHumanBoundingBox",
      "language": "swift",
      "code": "func humanBoundingBox(for observation: VNRecognizedPointsObservation) -> CGRect {\n        var box = CGRect.zero\n        // Process body points only if the confidence is high\n        guard observation.confidence > 0.6 else {\n            return box\n        }\n        var normalizedBoundingBox = CGRect.null\n        guard let points = try? observation.recognizedPoints(forGroupKey: .all) else {\n            return box\n        }\n        for (_, point) in points {\n            // Only use point if human pose joint was detected reliably\n            guard point.confidence > 0.1 else { continue }\n            normalizedBoundingBox = normalizedBoundingBox.union(CGRect(origin: point.location, size: .zero))\n        }\n        if !normalizedBoundingBox.isNull {\n            box = normalizedBoundingBox\n        }\n        // Fetch body joints from the observation and overlay them on the player\n        DispatchQueue.main.async {\n            let joints = getBodyJointsFor(observation: observation)\n            self.jointSegmentView.joints = joints\n        }\n        // Store the body pose observation in playerStats when the game is in TrackThrowsState\n        // We will use these observations for action classification once the throw is complete\n        if gameManager.stateMachine.currentState is GameManager.TrackThrowsState {\n            playerStats.storeObservation(observation)\n            if trajectoryView.inFlight {\n                trajectoryInFlightPoseObservations += 1\n            }\n        }\n        return box\n    }"
    },
    {
      "timestamp": "21:58",
      "title": "ActionVisionStoreObservation",
      "language": "swift",
      "code": "mutating func storeObservation(_ observation: VNRecognizedPointsObservation) {\n        if poseObservations.count >= GameConstants.maxPoseObservations {\n            poseObservations.removeFirst()\n        }\n        poseObservations.append(observation)\n    }"
    },
    {
      "timestamp": "22:21",
      "title": "ActionVisionGetLastThrowType",
      "language": "swift",
      "code": "mutating func getLastThrowType() -> ThrowType {\n        let actionClassifier = PlayerActionClassifier().model\n        guard let poseMultiArray = prepareInputWithObservations(poseObservations) else {\n            return ThrowType.none\n        }\n        let input = PlayerActionClassifierInput(input: poseMultiArray)\n        guard let predictions = try? actionClassifier.prediction(from: input),\n            let output = predictions.featureValue(for: \"output\")?.multiArrayValue,\n                let outputBuffer = try? UnsafeBufferPointer<Float32>(output) else {\n            return ThrowType.none\n        }\n        let probabilities = Array(outputBuffer)\n        guard let maxConfidence = probabilities.prefix(3).max(), let maxIndex = probabilities.firstIndex(of: maxConfidence) else {\n            return ThrowType.none\n        }\n        let throwTypes = ThrowType.allCases\n        return throwTypes[maxIndex]\n    }"
    },
    {
      "timestamp": "22:42",
      "title": "ActionVisionPrepareInputWithObservations",
      "language": "swift",
      "code": "func prepareInputWithObservations(_ observations: [VNRecognizedPointsObservation]) -> MLMultiArray? {\n    let numAvailableFrames = observations.count\n    let observationsNeeded = 60\n    var multiArrayBuffer = [MLMultiArray]()\n\n    // swiftlint:disable identifier_name\n    for f in 0 ..< min(numAvailableFrames, observationsNeeded) {\n        let pose = observations[f]\n        do {\n            let oneFrameMultiArray = try pose.keypointsMultiArray()\n            multiArrayBuffer.append(oneFrameMultiArray)\n        } catch {\n            continue\n        }\n    }\n    \n    // If poseWindow does not have enough frames (60) yet, we need to pad 0s\n    if numAvailableFrames < observationsNeeded {\n        for _ in 0 ..< (observationsNeeded - numAvailableFrames) {\n            do {\n                let oneFrameMultiArray = try MLMultiArray(shape: [1, 3, 18], dataType: .double)\n                try resetMultiArray(oneFrameMultiArray)\n                multiArrayBuffer.append(oneFrameMultiArray)\n            } catch {\n                continue\n            }\n        }\n    }\n    return MLMultiArray(concatenating: [MLMultiArray](multiArrayBuffer), axis: 0, dataType: MLMultiArrayDataType.double)\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Detecting Hand Poses with Vision",
        "url": "https://developer.apple.com/documentation/Vision/detecting-hand-poses-with-vision"
      },
      {
        "title": "Vision",
        "url": "https://developer.apple.com/documentation/Vision"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10653/8/E739CC44-25A9-46B9-8E40-1788530C5785/wwdc2020_10653_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10653/8/E739CC44-25A9-46B9-8E40-1788530C5785/wwdc2020_10653_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "111241",
      "year": "2023",
      "title": "Explore 3D body pose and person segmentation in Vision",
      "url": "https://developer.apple.com/videos/play/wwdc2023/111241"
    },
    {
      "id": "10304",
      "year": "2023",
      "title": "Integrate with motorized iPhone stands using DockKit",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10304"
    },
    {
      "id": "10039",
      "year": "2021",
      "title": "Classify hand poses and actions with Create ML",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10039"
    },
    {
      "id": "10040",
      "year": "2021",
      "title": "Detect people, faces, and poses using Vision",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10040"
    },
    {
      "id": "10043",
      "year": "2020",
      "title": "Build an Action Classifier with Create ML",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10043"
    },
    {
      "id": "10673",
      "year": "2020",
      "title": "Explore Computer Vision APIs",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10673"
    },
    {
      "id": "10099",
      "year": "2020",
      "title": "Explore the Action & Vision app",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10099"
    }
  ],
  "extractedAt": "2025-07-18T10:40:44.865Z"
}