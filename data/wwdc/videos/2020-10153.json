{
  "id": "10153",
  "year": "2020",
  "url": "https://developer.apple.com/videos/play/wwdc2020/10153/",
  "title": "Get models on device using Core ML Converters",
  "speakers": [],
  "duration": "",
  "topics": [
    "Machine Learning & AI"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "Hello and welcome to WWDC.\n\nHi, my name is Aseem, and I'm from the Core ML team. In this session, I want to share with you a few exciting new developments in Core ML converters.\n\nWe have been working hard on improving the experience of converting models to Core ML and have significantly updated our conversion tools.\n\nLet's first start by looking, though, at why Core ML is such a great solution for integrating machine learning into your apps. Since Core ML was launched in 2017, our mission has always been about making it as easy as possible to deploy machine learning models into your application to create a wide range of compelling experiences.\n\nWith Core ML, the same model can be conveniently deployed to all types of Apple devices, and the best compatibility and performance across OS and device generations is guaranteed. Core ML models seamlessly leverage all the hardware acceleration available on the device, be it the CPU, the GPU or the Apple Neural Engine, which is specifically designed for accelerating neural networks.\n\nIn addition, with each new release, you get the best of Apple's ecosystem. For instance, this year we are introducing Core ML model deployment to make it really easy to update your models. Furthermore, Core ML models can now be encrypted.\n\nFor more details, please check out our session on \"Model Deployment and Security with Core ML.\" So really, the starting point to unlock all this awesomeness is the Core ML model. And how it can be created is the topic of this session.\n\nA variety of machine learning models, ranging from deep learning to tree based, can be expressed in Core ML. Of course, one of the best sources is the Create ML app, but you can also easily create an ML model starting from your favorite framework using the Core ML Tools Python package.\n\nOver time, as the ML ecosystem grows, the Core ML converters continue to extend support to more frameworks.\n\nThis year, we have some exciting announcements regarding our support for neural network libraries.\n\nSo far we have supported the conversion of neural network models from one of these frameworks. This year, we have focused on the two most commonly used libraries by the deep learning community, which are PyTorch and TensorFlow.\n\nLets look at TensorFlow first.\n\nSo far, if you wanted to convert a TensorFlow model to Core ML, you would have to additionally install tfcoreml and use its API, which internally depended on the Core ML Tools package. Well, this has changed, and now all you need is Core ML Tools. We have now fully integrated TensorFlow conversion within Core ML Tools. We are also very excited to announce a much expanded support for TensorFlow 2. TensorFlow 1 has been supported for a while now through tfcoreml, and we had added support for TF 2 convolutional models last year.\n\nThis year, we significantly expanded to include dynamic models such as LSTMs, transformers, et cetera.\n\nThe new converter supports all the different formats in which a TensorFlow model can be exported.\n\nLet's look at conversion from PyTorch now.\n\nSo far, the way to do this is to use the PyTorch export tool to produce an ONNX model and then use ONNX Core ML to get to ML model. However, many times the first export step may fail since ONNX is an open standard that evolves independently, so it may lack a newly added feature to PyTorch, or the torch exporter hasn't been updated, or maybe it has a bug. Well, now this extra dependency has been removed since we have implemented a new PyTorch converter.\n\nIt's now a one-step process starting from a torch_script_model.\n\nYou may have noticed that the API to invoke the PyTorch converter is exactly the same as was used with TensorFlow. That's because we have redesigned the API to keep a single call to invoke all converters. It works, irrespective of which source framework the model needs to be converted from.\n\nWith these changes, Core ML Tools is now the one-stop shop for converting models from TensorFlow and PyTorch.\n\nAnd it's not just the API that has changed. We haven't simply added two new converter paths. Instead, we have undertaken a major effort to redesign the converter architecture to significantly improve the experience and code quality. So, we have moved from having separate converter pipelines, that were built when different converters were added at different points of time, to a single converter stack with maximum code reuse. And to achieve this consolidation, we have introduced a new in-memory representation called Model Intermediate Language, or MIL for short.\n\nMIL has been designed to streamline the conversion process and make it easy to add support for new frameworks. It unifies the stack by providing a common interface to capture information from different frameworks.\n\nIt has a set of operations, optimization passes and a model builder API.\n\nAs an end user, you generally don't interact with MIL, but it can be really useful in certain scenarios.\n\nWe will revisit MIL a bit later in this talk. Let's first see a few examples of using the new converters.\n\nLet's start with a simple image classifier example to get familiar with the new unified API. Let me switch to the Jupyter notebook for that. Here I am in a Jupyter notebook, and I've already imported Core ML Tools. Let's convert from TensorFlow 2 first, which I've imported as well.\n\nI'll grab a model from the TensorFlow 2 model zoo.\n\nI'm using a MobileNet model, which is a popular convolutional model for image classification. Let's load it.\n\nAnd let's convert it.\n\nTo do that I simply type ct.convert...\n\nand provide to it the TensorFlow model object and hit enter.\n\nThe converter automatically detects the type of the model, its input shapes, outputs, et cetera, and continues to convert it through MIL.\n\nOkay, it's done. That was easy! Why don't we try it again? This time let's use a model from PyTorch.\n\nFor that, let me go ahead and import torch and torchvision. And I'll grab a model this time from torchvision, the mobilenet v2 model.\n\nNow, we need a TorchScript model to convert to Core ML, which can be obtained by either scripting or tracing. I'm gonna use tracing here.\n\nTracing can be done by using functions provided by PyTorch. Let's see how.\n\nWe first get the model in inference mode by using eval. And then invoke the jit.trace method, which needs an example input to work. Let's hit enter. So, we have the traced model, and now we can use Core ML Tools to convert it.\n\nSo, I'll again type ct.convert. And this time, I provided the traced model. Now, one more thing. Generally, the information about input shape is not present in the TorchScript model, but that's required for conversion. So let's give it to the converter. And this can be done using the inputs argument to which I will provide the type and shape of the input.\n\nAnd I can do that by using the TensorType class, which takes a shaping.\n\nThat's it. Let's hit enter.\n\nWe see the familiar steps of conversion to MIL, a few optimization passes and finally conversion to ML model. And that's done. Let's check out the model interface by printing the ML model object.\n\nLet's see. So, we see that there is an input called input.1, and it's of type multiArray since we provided a TensorType here. And there is an output called 1648. Hmm. That is a bit odd. What really happened there is that that's the name of the output tensor in the torch model, and the converter just automatically picked it up from there.\n\nWell, it's easy to rename it to something more meaningful. Let's see how.\n\nSo I use the rename feature utility from Core ML Tools to rename my inputs and outputs to whatever I want. In this case, I use a placeholder name. So let's hit enter and then again print the ML model object. Nice. So we see that the name of inputs and outputs has been updated to the names that I provided.\n\nLet me do one last conversion now, this time from TensorFlow 1. For that, let me move to a different notebook, where I already set up my TensorFlow 1 environment. I also pre-downloaded the mobilenet TensorFlow 1 model, which is in this protobuf pb format. Along with this model came the labels.txt file, which contains the names of the classes that this model was trained on. So let's convert this model.\n\nWe'll invoke our familiar convert API, and now we can provide the protobuf pb file to it.\n\nNow, this will work. But let's make a nice Core ML model this time by doing a couple of extra things. So, earlier we had used the TensorType, but since this model really works on an image, you know, it would be nice to let the converter know about it. And I can do that by using the ct.ImageType class, to which I'll provide a couple of pre-processing parameters. Bias...\n\nfor each channel in an RGB image and scale.\n\nThis will normalize the image as expected by the mobilenet model.\n\nAnother thing that I'll change is that since this model performs classification, you know, probably it's a good idea to generate a classifier Core ML model. And this can be done by using the ClassifierConfig class, which takes the labels.txt file as is. Isn't this great? Let's hit enter.\n\nAnd we are done. Let's go ahead and save this model to disk. Before that though, I'll add some useful metadata regarding license and author. And then I'll go ahead and save the model by typing mlmodel.save.\n\nI'll name my model as mobilenet.mlmodel.\n\nAnd now I can see that it's on disk. Let's check out this model in Finder. So we see the model is here. Let me click to open it. It automatically opens in Xcode. This year, we have updated our Xcode UI. For classifiers now, the class labels are visible right here. As we can see, this model has about 1,000 classes. There is also a new tab called Preview which is really convenient. And I really like it. We can simply drag and drop a few images in here, and it will automatically run our model on these images and display the predictions. As we can see here, our model seems to be doing quite well on these images. So that wraps up the conversion API demo. Let's recap. We invoked the conversion function with different model types, and it just worked. Now let's try converting a slightly more complex model. For that, I'd like to invite my colleague Gitesh, who will convert a model that's used in translating audio to text.\n\nThanks, Aseem. Hi, I am Gitesh, an engineer in the Core ML team. In this demo, I will illustrate the automatic handling of flexible shapes and related capabilities of the new Core ML Tools conversion API. And I will demonstrate these using an Automatic Speech Recognition task. In this task, the input is a speech audio file, and the output is the text transcription of it. There are many approaches to Automatic Speech Recognition. The system that I use in my example consists of three stages.\n\nThere are pre- and post-processing stages and a neural network model in the middle to do the heavy lifting.\n\nPre-processing involves extracting the mel spectrum, also called MFCCs, from the raw audio file. These MFCCs are fed to the neural network model, which returns a character level time series of probability distributions.\n\nThose are then post-processed by a CTC decoder to produce the final transcription.\n\nPre- and post-processing stages employ standard techniques which can be easily implemented. Therefore, my focus will be on converting this model in the center. I use a pre-trained TensorFlow model called DeepSpeech.\n\nAt a high level, this model uses an LSTM and a few dense layers stacked on top of each other. And such an architecture is quite common for seq2seq models. Now, let's jump right into the Jupyter notebook to convert this model to Core ML and try it out on a few audio samples. We start with importing some packages. I found the pre-trained weights for DeepSpeech model on this GitHub repository from Mozilla and have already downloaded those weights and a script to export the TensorFlow 1 model from that repository. Let's run the script.\n\nWe now have a frozen TensorFlow graph in the protobuf format. Let's look into the outputs of this graph. And for that, I have already written some inspection utilities.\n\nSo, this model has four outputs, and this first one, called \"mfccs,\" represents the output of the pre-processing stage. Which means the exported TensorFlow graph contains not just the DeepSpeech model, but also the pre-processing sub-graph.\n\nLet's strip off this pre-processing component by providing the remaining three output names to the unified converter function. And with this information, let's call the Core ML converter.\n\nNice! The conversion is successful. Now let's run this converted model on an audio sample. First, we load and play an audio file.\n\nOnce upon a time, there was an exploding chicken. He met up with a golden tiger, and together, they walked through the green forest. The end. Next, we pre-process it. For the full pipeline to work in this notebook, I have already constructed these pre- and post-processing functions using code in the DeepSpeech repository.\n\nSo, this pre-processing has transformed the audio file into a tensor object of this shape, and the shape can be viewed as one audio file, pre-processed into 636 sequences, each of width 19 and containing 26 coefficients. The number of these sequences change with the length of the audio. For this 12 seconds audio file, we have 636 sequences. Let's now inspect the input shapes that the model expects.\n\nSo, we see that the first input of this model has almost the correct shape. The only difference is that it can process 16 sequences at a time. Therefore, I will write a loop to break the input features into chunks and feed each segment to the model one by one. I've written that bit of code already. Let me paste it here. You don't need to follow all this code. Basically, we break the pre-processed feature into slices of size 16 and run prediction on each slice, with some state management, inside a loop. Let's run it.\n\nNice. The transcription looks pretty accurate. Now, everything looks good, but wouldn't it be great if we could run the prediction on the entire pre-processed feature in just one go? Well, it's possible, and we would need a dynamic TensorFlow model for that. Let's rerun the same script from the DeepSpeech repository to obtain a dynamic graph. This time, we provide an additional flag called \"n_steps.\" This flag corresponds to sequence length and had a default value of 16. But now we set it to -1, which means that the sequence length can take any positive value.\n\nWe have a new TensorFlow model. Let's convert it.\n\nGreat. Conversion is done. Let's inspect how this model is different from the previous one.\n\nWell, one difference I see is that this Core ML model can work on inputs of arbitrary sequence length. And the difference lies not just in the shapes. Under the hood, this dynamic Core ML model is much more complicated than the previous static one. It has lots of dynamic operations such as get shape, dynamic reshape, et cetera. However, our experience of converting it was exactly the same. The converter handled it with just the same amount of ease as before. Let's now validate the model on the same audio file.\n\nThis time, we don't need the loop and can directly feed the entire input features to the model. Let's run it.\n\nGreat. Transcription looks perfect again. Let's recap what we saw in this demo. So, we worked with two variants of the DeepSpeech model.\n\nOn a static TensorFlow graph, the converter produced a Core ML model with inputs of fixed shape.\n\nAnd with the dynamic variant, we obtained a Core ML model which could accept inputs of any sequence length. Converter handled both cases transparently...\n\nand without making any change to the conversion call. One thing I did not get the chance to show in the demo-- we can start with a dynamic TensorFlow graph and get a static Core ML model. Let's see how we can do it.\n\nFirst, we define a Type description object...\n\nwith name of the input...\n\nand its shape.\n\nThen we feed this object to the conversion API.\n\nThat's all. Under the hood, the type and value inference propagates this shape information to remove all the unnecessary dynamic operations. Therefore, static models are likely to be more performant while the dynamic ones are definitely more flexible.\n\nWhich one to use depends on the requirements of your application. At this point, we have seen few examples of successful conversion to Core ML. However, in some cases, we might encounter an unsupported op error. In fact, I recently hit this issue. Let me show that to you. So, I was exploring this library for natural language models called \"transformers.\" And a recent model called T5 had caught my attention. Let's convert it. First, we load the pre-trained model from the library. Since the returned object is an instance of tf.keras model...\n\nwe can pass it directly into the Core ML converter. Let's do that.\n\nAnd here we see this unsupported op error for the operation \"Einsum.\" I will now hand it back to Aseem, who will go through a few approaches to handle this issue, and then we will come back to convert this model. We recognize that hitting this error is a challenge in an ever-evolving machine learning space as new ops are regularly added to TensorFlow or PyTorch, or you may be using a custom-built op yourself.\n\nWhat to do in this case? Well, one option is to use a Core ML custom layer, which allows you to accompany the ML model with your own swift implementation of the op. This is great, but in many cases, it may be possible to take another easier approach. You can use what we call the \"composite op,\" which does not require writing additional Swift code since it can keep everything bundled in the ML model file.\n\nA composite op is built from existing MIL ops. Let's dig a little bit into what MIL is and how we can construct a composite op using it. We developed the model intermediate language to unify the converter stack. If we expand to look at the internals, this stack consists of three sections-- the frontends, the intermediate MIL portion and the backend. For each source framework, there is a separate frontend, which captures a framework specific representation. After that, an MIL program is built, at which point the representation becomes source agnostic. A lot of common optimization passes such as operator fusions, dead code elimination, constant propagation, et cetera, happen here, after which the graph is serialized to the protobuf ML Model format. Another way to look at the same picture is that each source framework has its own dialect which is converted to MIL, as a consolidation point, to ML model. This is one way of going to the MIL format which is what the converter does. But there is another way to directly write out an MIL program using the builder API. MIL is a stand-alone language that can be used to directly express a neural network model. And it is quite similar in its API to the ones many of you are already quite familiar with, whether you are a TensorFlow 2 or a PyTorch user. Let's check out this builder API. Here is how we can start writing an MIL program in Python. We import the builder and define the input by specifying its shape, which is 1, 100, 100, 3 in this case. We can print a description of the program by simply calling print. In the description below, we see that the type of the input was inferred to be Float32, which is the default type. Now let's add the first op. So, we added a ReLu op with this simple syntax. Let's add another op, a transpose op this time. A great thing about the MIL builder is that it instantly performs type and shape inference. We can see here that the shape of the output of the transpose has been updated correctly in the description below. Let's add a reduction operation on the last two axes. And we see, as expected, the shape of the tensor is 1, 3 now. Let's add one last op. Finally, the program returns the output of the log op. So, we see that the API to define a network in MIL is quite straightforward. Let's now see how it can be used to implement a composite op and bypass the unsupported op error. Let me hand it back over to Gitesh to illustrate this. So, we were converting the T5 model and had come across an unsupported op error for Einsum. I read the TensorFlow documentation on it and found that it refers to Einstein summation notation.\n\nLots of operations like reduce_sum, transpose, trace, et cetera, can be expressed in this notation using a string. For this particular conversion, let's focus on the notation that this model uses.\n\nLooking at the error trace, we see that this model uses Einsum with this notation, which translates to following mathematical expression. This might look complicated, but, effectively, it is just a batched matrix multiplication with a transpose on the second input. And that's great since MIL supports this operation directly. Let's now write a composite op. First, we import MIL Builder and a decorator.\n\nThen we define a function with the same name as TensorFlow operation, which is Einsum in this case. Next, we decorate this function to register it with the converter.\n\nThis would ensure that the right function would be invoked whenever Einsum operation is encountered during the conversion.\n\nAnd finally, we grab the inputs and define a MatMul operation using MIL Builder. That's all. Let's call the Core ML converter again.\n\nConversion is completed. To verify if it is successful, let's print the ML model.\n\nPerfect! To recap, while converting T5 model, we had hit an unsupported op error for Einsum. In general, Einsum is a complicated operation and can represent a variety of tensor operations, but we did not have to worry about all the possible cases. We just handled the particular parameterization needed for this model, and that was easily implemented using composite ops. To summarize, we have packed Core ML Tools with many new features such as powerful type inference, user-friendly APIs, et cetera, which make Core ML converters easier to use and more extensible. To read more about these features, visit our new documentation that contains several examples including the demos for this session. To wrap up, we have announced the new PyTorch converter and enhanced support for TensorFlow 2. These are available through the new unified API and made possible via MIL.\n\nWe would like to invite all of you to try them out and help us make Core ML Tools even better with your feedback. Thank you. [chimes]",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "2:58",
      "title": "TensorFlow conversion using tfcoreml",
      "language": "swift",
      "code": "# pip install tfcoreml\n# pip install coremltools\n\nimport tfcoreml\nmlmodel = tfcoreml.convert(tf_model, mlmodel_path=\"/tmp/model.mlmodel\")"
    },
    {
      "timestamp": "3:16",
      "title": "New TensorFlow model conversion",
      "language": "swift",
      "code": "# pip install coremltools\n\nimport coremltools as ct\nmlmodel = ct.convert(tf_model)"
    },
    {
      "timestamp": "3:57",
      "title": "ONNX conversion to Core ML",
      "language": "swift",
      "code": "# pip install onnx-coreml\n# pip install coremltools\n\nimport onnx_coreml\nonnx_model = torch.export(torch_model)\nmlmodel = onnx_coreml.convert(onnx_model)"
    },
    {
      "timestamp": "4:28",
      "title": "New PyTorch model conversion",
      "language": "swift",
      "code": "# pip install coremltools\n\nimport coremltools as ct \nmlmodel = ct.convert(torch_script_model)"
    },
    {
      "timestamp": "4:52",
      "title": "Unified conversion API",
      "language": "swift",
      "code": "import coremltools as ct\n\nmodel = ct.convert(\n  source_model # TF1, TF2, or PyTorch model\n)"
    },
    {
      "timestamp": "6:42",
      "title": "Demo 1: TF2 conversion",
      "language": "swift",
      "code": "import coremltools as ct \nimport tensorflow as tf\n\ntf_model = tf.keras.applications.MobileNet()\nmlmodel = ct.convert(tf_model)"
    },
    {
      "timestamp": "7:41",
      "title": "Demo 1: Pytorch conversion",
      "language": "swift",
      "code": "import coremltools as ct \nimport torch\nimport torchvision \n\ntorch_model = torchvision.models.mobilenet_v2()\n\ntorch_model.eval()\nexample_input = torch.rand(1, 3, 256, 256)\ntraced_model = torch.jit.trace(torch_model, example_input)\n\nmlmodel = ct.convert(traced_model,\n                    inputs=[ct.TensorType(shape=example_input.shape)])\n\nprint(mlmodel)\n\nspec = mlmodel.get_spec()\nct.utils.rename_feature(spec, \"input.1\", \"myInputName\")\nct.utils.rename_feature(spec, \"1648\", \"myOutputName\")\nmlmodel = ct.models.MLModel(spec)\n\nprint(mlmodel)"
    },
    {
      "timestamp": "10:37",
      "title": "Demo 1 : TF1 conversion",
      "language": "swift",
      "code": "import coremltools as ct \nimport tensorflow as tf\n\nmlmodel = ct.convert(\"mobilenet_frozen_graph.pb\",\n                    inputs=[ct.ImageType(bias=[-1,-1,-1], scale=1/127.0)],\n                    classifier_config=ct.ClassifierConfig(\"labels.txt\"))\n\nmlmodel.short_description = 'An image classifier'\nmlmodel.license = 'Apache 2.0'\nmlmodel.author = \"Original Paper: A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, \"\\\n                 \"T. Weyand, M. Andreetto, H. Adam\"\n\nmlmodel.save(\"mobilenet.mlmodel\")"
    },
    {
      "timestamp": "13:33",
      "title": "Demo 1 Recap: Using coremltools convert",
      "language": "swift",
      "code": "import coremltools as ct\nmlmodel = ct.convert(\"./tf1_inception_model.pb\")\nmlmodel = ct.convert(\"./tf2_inception_model.h5\")\nmlmodel = ct.convert(torch_model, inputs=[ct.TensorType(shape=example_input.shape)])"
    },
    {
      "timestamp": "15:45",
      "title": "Converting a Deep Speech model",
      "language": "swift",
      "code": "import numpy as np\nimport IPython.display as ipd\n\nimport coremltools as ct\n\n### Pretrained models and chekpoints are available on the repository: \nhttps://github.com/mozilla/DeepSpeech\n\n!python DeepSpeech.py --export_dir /tmp --checkpoint_dir ./deepspeech-0.7.1-checkpoint --alphabet_config_path=alphabet.txt --scorer_path=kenlm.scorer >/dev/null 2>&1\n\nls /tmp/*.pb\n\ntf_model = \"/tmp/output_graph.pb\"\n\nfrom demo_utils import inspect_tf_outputs\n\ninspect_tf_outputs(tf_model)\n\noutputs = [\"logits\", \"new_state_c\", \"new_state_h\"]\n\nmlmodel = ct.convert(tf_model, outputs=outputs)\n\naudiofile = \"./audio_sample_16bit_mono_16khz.wav\"\n\nipd.Audio(audiofile) \n\nfrom demo_utils import preprocessing, postprocessing\n\nmfccs = preprocessing(audiofile)\n\nmfccs.shape\n\nfrom demo_utils import inspect_inputs\n\ninspect_inputs(mlmodel, tf_model)\n\nstart = 0 \nstep = 16\n\nmax_time_steps = mfccs.shape[1]\n\nlogits_sequence = []\n\ninput_dict = {}\n\ninput_dict[\"input_lengths\"]  = np.array([step]).astype(np.float32)\n\ninput_dict[\"previous_state_c\"] = np.zeros([1, 2048]).astype(np.float32) # Initializing cell state \ninput_dict[\"previous_state_h\"] = np.zeros([1, 2048]).astype(np.float32) # Initializing hidden state \n\n\nprint(\"Transcription: \\n\")\n\nwhile (start + step) < max_time_steps:\n    input_dict[\"input_node\"] = mfccs[:, start:(start + step), :, :]\n    \n    # Evaluation\n    preds = mlmodel.predict(input_dict)\n    \n    \n    start += step\n    logits_sequence.append(preds[\"logits\"])\n\n    \n    # Updating states\n    input_dict[\"previous_state_c\"] = preds[\"new_state_c\"]\n    input_dict[\"previous_state_h\"] = preds[\"new_state_h\"]\n    \n    \n    # Decoding\n    probs = np.concatenate(logits_sequence)\n    transcription = postprocessing(probs)\n    print(transcription[0][1], end=\"\\r\", flush=True)\n\n!python DeepSpeech.py --n_steps -1 --export_dir /tmp --checkpoint_dir ./deepspeech-0.7.1-checkpoint --alphabet_config_path=alphabet.txt --scorer_path=kenlm.scorer >/dev/null 2>&1\n\nmlmodel = ct.convert(tf_model, outputs=outputs)\n\ninspect_inputs(mlmodel,tf_model)\n\ninput_dict = {}\n\ninput_dict[\"input_node\"] = mfccs\n\ninput_dict[\"input_lengths\"] = np.array([mfccs.shape[1]]).astype(np.float32)\ninput_dict[\"previous_state_c\"] = np.zeros([1, 2048]).astype(np.float32) # Initializing cell state \ninput_dict[\"previous_state_h\"] = np.zeros([1, 2048]).astype(np.float32) # Initializing hidden state \n\nprobs = mlmodel.predict(input_dict)[\"logits\"]\n\ntranscription = postprocessing(probs)\n\nprint(transcription[0][1])"
    },
    {
      "timestamp": "21:52",
      "title": "Deep Speech Demo Recap: Convert with input type",
      "language": "swift",
      "code": "import coremltools as ct\n\ninput = ct.TensorType(name=\"input_node\", shape=(1, 16, 19, 26))\nmodel = ct.convert(tf_model, outputs=outputs, inputs=[input])"
    },
    {
      "timestamp": "26:26",
      "title": "MIL Builder API sample",
      "language": "swift",
      "code": "from coremltools.converters.mil import Builder as mb\n\n@mb.program(input_specs=[mb.TensorSpec(shape=(1, 100, 100, 3))])\ndef prog(x):\n    x = mb.relu(x=x)\n    x = mb.transpose(x=x, perm=[0, 3, 1, 2])\n    x = mb.reduce_mean(x=x, axes=[2, 3], keep_dims=False)\n    x = mb.log(x=x)\n    return x"
    },
    {
      "timestamp": "28:20",
      "title": "Converting with composite ops",
      "language": "swift",
      "code": "import coremltools as ct\n\nfrom transformers import TFT5Model\n\nmodel = TFT5Model.from_pretrained('t5-small')\n\nmlmodel = ct.convert(model)\n\n# Einsum Notation\n\n $$ \\Large \"bnqd,bnkd \\rightarrow bnqk\" $$\n\n$$ \\large C(b, n, q, k) = \\sum_d A(b, n, q, d) \\times  B(b, n, k, d) $$\n\n$$ \\Large C = AB^{T}$$\n\nfrom coremltools.converters.mil import Builder as mb\n\nfrom coremltools.converters.mil import register_tf_op\n\n@register_tf_op\ndef Einsum(context, node):\n\n\t\tassert node.attr['equation'] == 'bnqd,bnkd->bnqk'\n\n    a = context[node.inputs[0]]\n    b = context[node.inputs[1]]\n\n    x = mb.matmul(x=a, y=b, transpose_x=False, transpose_y=True, name=node.name)\n\n    context.add(node.name, x)\n\nmlmodel = ct.convert(model)\n\nprint(mlmodel)"
    },
    {
      "timestamp": "29:50",
      "title": "Recap: Custom operation",
      "language": "swift",
      "code": "@register_tf_op\ndef Einsum(context, node):\n    assert node.attr['equation'] == 'bnqd,bnkd->bnqk'\n\n    a = context[node.inputs[0]]\n    b = context[node.inputs[1]]\n\n    x = mb.matmul(x=a, y=b, transpose_x=False, transpose_y=True, name=node.name)\n    \n    context.add(node.name, x)"
    },
    {
      "timestamp": "29:50",
      "title": "Deep Speech demo utilities",
      "language": "swift",
      "code": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.python.ops import gen_audio_ops as contrib_audio\n\n\nfrom deepspeech_training.util.text import Alphabet\nfrom ds_ctcdecoder import ctc_beam_search_decoder, Scorer\n\n\n## Preprocessing + Postprocessing functions are constructed using code in DeepSpeech repository: https://github.com/mozilla/DeepSpeech\n\naudio_window_samples = 512\naudio_step_samples = 320\nn_input  = 26\naudio_sample_rate = 16000\ncontext = 9\n\nlm_alpha = 0.931289039105002\nlm_beta  = 1.1834137581510284\nscorer_path = \"./kenlm.scorer\"\n\nbeam_width = 1024\ncutoff_prob = 1.0\ncutoff_top_n = 300\n\nalphabet = Alphabet(\"./alphabet.txt\")\n\nscorer = Scorer(lm_alpha, lm_beta, scorer_path, alphabet)\n\n\ndef audiofile_to_features(wav_filename):\n    \n    samples = tf.io.read_file(wav_filename)\n    \n    decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n    \n    spectrogram = contrib_audio.audio_spectrogram(decoded.audio,\n                                                  window_size=audio_window_samples,\n                                                  stride=audio_step_samples,\n                                                  magnitude_squared=True)\n        \n    mfccs = contrib_audio.mfcc(spectrogram = spectrogram,\n                               sample_rate = decoded.sample_rate,\n                               dct_coefficient_count=n_input,\n                               upper_frequency_limit=audio_sample_rate/2)\n    \n    mfccs = tf.reshape(mfccs, [-1, n_input])\n\n    return mfccs, tf.shape(input=mfccs)[0]\n\n\n\ndef create_overlapping_windows(batch_x):\n    \n    batch_size = tf.shape(input=batch_x)[0]\n    window_width = 2 * context + 1\n    num_channels = n_input\n\n    eye_filter = tf.constant(np.eye(window_width * num_channels)\n                               .reshape(window_width, num_channels, window_width * num_channels), tf.float32) \n    \n    # Create overlapping windows\n    batch_x = tf.nn.conv1d(input=batch_x, filters=eye_filter, stride=1, padding='SAME')\n\n    batch_x = tf.reshape(batch_x, [batch_size, -1, window_width, num_channels])\n\n    return batch_x\n\n\nsess = tf.Session(graph=tf.Graph())\n\nwith sess.graph.as_default() as g:\n    path = tf.placeholder(tf.string)\n    _features, _ = audiofile_to_features(path)\n    _features = tf.expand_dims(_features, 0)\n    _features = create_overlapping_windows(_features)\n\n    \n\ndef preprocessing(input_file_path):\n    return _features.eval(session=sess, feed_dict={path: input_file_path})\n\n\n\n\ndef postprocessing(logits):\n    logits = np.squeeze(logits)\n\n    decoded = ctc_beam_search_decoder(logits, alphabet, beam_width,\n                                      scorer=scorer, cutoff_prob=cutoff_prob,\n                                      cutoff_top_n=cutoff_top_n)\n\n    return decoded\n\n\n\ndef inspect_tf_outputs(path):\n    \n    with open(path, 'rb') as f:\n        serialized = f.read()\n    gdef = tf.GraphDef()\n    gdef.ParseFromString(serialized)\n\n    with tf.Graph().as_default() as g:\n        tf.import_graph_def(gdef, name=\"\")\n\n    output_nodes = []\n    for op in g.get_operations():\n    \n        if op.type == \"Const\":\n            continue\n        \n        if all([len(g.get_tensor_by_name(tensor.name).consumers()) == 0 for tensor in op.outputs]):\n            \n            output_nodes.append(op.name)\n\n    return output_nodes\n\n\ndef inspect_inputs(mlmodel, tfmodel):\n    \n    names = []\n    ranks = []\n    shapes = []\n\n    spec = mlmodel.get_spec()\n\n    with open(tfmodel, 'rb') as f:\n        serialized = f.read()\n    gdef = tf.GraphDef()\n    gdef.ParseFromString(serialized)\n\n    with tf.Graph().as_default() as g:\n        tf.import_graph_def(gdef, name=\"\")\n\n    for tensor in spec.description.input:\n        name = tensor.name\n        shape = tensor.type.multiArrayType.shape\n\n        if tensor.type.multiArrayType.shapeRange:\n            for dim, size in enumerate(tensor.type.multiArrayType.shapeRange.sizeRanges):\n                if size.upperBound == -1:\n                    shape[dim] = -1\n                elif size.lowerBound < size.upperBound:\n                    shape[dim] = -1\n                elif size.lowerBound == size.upperBound:\n                    assert shape[dim] == size.lowerBound\n                else:\n                    raise TypeError(\"Invalid shape range\")\n\n        coreml_shape = tuple(None if i == -1 else i for i in shape)\n\n        tf_shape = tuple(g.get_tensor_by_name(name + \":0\").shape.as_list())\n\n        shapes.append({\"Core ML shape\": coreml_shape, \"TF shape\": tf_shape})\n        names.append(name)\n        ranks.append(len(coreml_shape))\n\n\n    columns = [shapes[i] for i in np.argsort(ranks)[::-1]]\n    indices = [names[i] for i in np.argsort(ranks)[::-1]]\n\n    return pd.DataFrame(columns, index= indices)"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Core ML",
        "url": "https://developer.apple.com/documentation/CoreML"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10153/3/710F0174-7EF1-4E55-9E84-7699484D2B1F/wwdc2020_10153_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10153/3/710F0174-7EF1-4E55-9E84-7699484D2B1F/wwdc2020_10153_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10047",
      "year": "2023",
      "title": "Use Core ML Tools for machine learning model compression",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10047"
    },
    {
      "id": "10017",
      "year": "2022",
      "title": "Explore the machine learning development experience",
      "url": "https://developer.apple.com/videos/play/wwdc2022/10017"
    },
    {
      "id": "10038",
      "year": "2021",
      "title": "Tune your Core ML models",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10038"
    },
    {
      "id": "10152",
      "year": "2020",
      "title": "Use model deployment and security with Core ML",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10152"
    }
  ],
  "extractedAt": "2025-07-18T10:40:57.903Z"
}