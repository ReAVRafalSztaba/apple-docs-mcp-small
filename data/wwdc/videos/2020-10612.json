{
  "id": "10612",
  "year": "2020",
  "url": "https://developer.apple.com/videos/play/wwdc2020/10612/",
  "title": "What's new in RealityKit",
  "speakers": [],
  "duration": "",
  "topics": [
    "Graphics & Games"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "â™ª Hello, and welcome to WWDC.\n\nMy name's Saad and I'm from the RealityKit team.\n\nLast year we released RealityKit, an AR-focused 3D engine.\n\nAnd since then, we've been working hard to add many new features.\n\nToday, I'm excited to tell you about some of them.\n\nTo start off, we have video materials.\n\nAnd video materials allow you to use videos as materials in RealityKit.\n\nSecond, is scene understanding using the brand new LiDAR sensor.\n\nThanks to ARKit processing data from this new sensor, we're able to bring the real world into your virtual one.\n\nAs a result, we have a huge set of features that allow you to make your virtual content interact with the real world.\n\nNext is improved rendering debugging.\n\nWith the rendering being such a huge pipeline, we've added the ability to inspect various properties related to the rendering of your entity.\n\nAnd the next two updates are related to ARKit 4 integration.\n\nARKit has extended its Face Tracking support to work on more devices.\n\nThis means that now face anchors will also work on more devices.\n\nAnd ARKit has also added location anchors.\n\nLocation anchors allow you to place AR content at specific locations in the real world using RealityKit.\n\nWith these new features in mind, I want to now show you an experience that you can now create using RealityKit.\n\nThis experience is built upon a problem we face as developers every day.\n\nWhen we code, what do we get? We get bugs like this one.\n\nAnd this is quite a simple looking bug.\n\nAnd in real life, bugs are never this simple.\n\nSo let's add some complexity to it using our new video materials feature.\n\nThe bug now has a glowing body as well as glittering in its eyes.\n\nSo what does a bug normally do? Well, it hides.\n\nAs the bug runs to hide, the tree occludes the bug.\n\nBut it can't hide forever.\n\nYou know you'll fix it.\n\nBut until then, you know it's going to try to run away from you.\n\nAnd using scene understanding, we can implement this logic.\n\nOnce you finally catch it, you want to make sure you fix it for good.\n\nSo let's pick it up and crush it into the pavement.\n\nAnd because of scene understanding, the bug disintegrates when it collides against the real world.\n\nAnd that's it for the experience.\n\nNow let's take a deep dive into these new features and see how you can use them.\n\nWe'll start with video materials.\n\nWe saw from the experience that video materials were used to give the bug a glowing effect as well as glittering in its eyes.\n\nNow, let's have a look at the video.\n\nOn the left, you can see the video associated with the texture on the bug.\n\nYou can see how the eyes map on and you can also see how we're able to share one portion of the texture for both eyes.\n\nYou can also see how the body is mapped on as the globe pulses in the texture, you can see it pulse through the body.\n\nSo in a nutshell, video materials allow you to have textures that change over time.\n\nThey get these textures from a video, and you can use this for a lot of things.\n\nYou can use it to simulate the glow effect you saw.\n\nYou can also use it to provide video instructions.\n\nSimply create a plane and map a video onto it.\n\nYou can even combine ARKit image anchors with video materials to bring images to life.\n\nBut that's not all.\n\nVideo materials also play audio, spatialized audio.\n\nWhen you apply a video material onto an entity, that entity becomes a spatialized audio source for your video.\n\nSpatialized audio is when the sound source acts as if it's emitted from a specific location.\n\nIn this case, from the entity.\n\nThis helps build a more immersive experience.\n\nNow, because all of this is done under the hood when you apply the material, you don't have to do extra work like synchronization or doing your own manual audio playback.\n\nSo now that you know what they are, how do we use them? The flow is quite simple.\n\nFirst, we have to load the video.\n\nRealityKit leverages the power of AVFoundation's AVPlayer to use as a video source.\n\nYou load the video using AVFoundation and create an AVPlayer object.\n\nOnce you have an AVPlayer object, you can then use it to create a video material.\n\nThis video material is just like any other RealityKit material.\n\nYou can assign it on any entity that you want to use it on.\n\nSo let's see how these steps map to code.\n\nFirst, we use AVFoundation to load the video.\n\nNext we create a video material using an AVPlayer object and assign it to the bug entity.\n\nOnce we've assigned the material, we can then play the video.\n\nNow, video can be controlled as you would through AVPlayer.\n\nAnd this is quite a simple example of video playback.\n\nHowever, since RealityKit's video material uses AVPlayer you get all of the great functionality and features that AVFoundation brings.\n\nFor example, you can use AVPlayer to directly play, pause, and seek inside your media.\n\nThis allows you to use a video atlas instead of having one video per texture.\n\nYou can use AVPlayer properties to trigger state transitions in your app.\n\nFor example, when your video finishes you can use this to go to the next stage of your app.\n\nYou can use AVPlayerLooper to play one video in a loop and you can use AVQueuePlayer to sequentially play a queue of videos.\n\nAnd lastly, you can even play remote media served using HTTP Live streaming.\n\nIf you want to learn more, see the advances in AVFoundation session in WWDC 2016 for more information.\n\nThis concludes video materials.\n\nTo summarize, video materials allow you to use videos as a texture source and spatial audio source.\n\nLet's move on to our next big feature: scene understanding.\n\nScene understanding has one main goal.\n\nIts goal is to make virtual content interact with the real world.\n\nTo achieve this, we want to bring everything in the real world into your virtual one.\n\nSo let's see how we can do this.\n\nWe start in the ARView.\n\nThe ARView has a list of settings related to the real world under the environment struct.\n\nThese settings configure the background image, environment based lighting, as well as spatial audio options.\n\nAnd we consider the real world to be part of your environment.\n\nAs a result, we've added a new scene understanding option set and this new option set lets you configure how the real world interacts with your virtual.\n\nIt has four options.\n\nFirst, we have occlusion.\n\nThis means real-world objects occlude virtual objects.\n\nSecond, is receives lighting.\n\nThis allows virtual objects to cast shadows on real-world surfaces.\n\nThird, is physics.\n\nThis enables the act of virtual objects physically interacting with the real world.\n\nAnd last but not least, we have collision.\n\nThis enables the generation of collision events, as well as the ability to ray-cast against the real world.\n\nNow one thing to know; receives lighting automatically turns on occlusion, and likewise physics automatically turns on collision.\n\nNow let's have a look at these options in a bit more detail.\n\nWe'll start with occlusion.\n\nSo on the left without occlusion, you can see that the bug is always visible.\n\nOn the right with occlusion, the bug is hidden behind the tree.\n\nSo you can see how occlusion helps with the realism, by hiding virtual objects behind real ones.\n\nTo use occlusion, simply add .occlusion into your scene understanding options.\n\nOur next option was receives lighting.\n\nOn the left, without receives lighting, the bug looks like it's floating because of the lack of shadows.\n\nAnd now on the right with receives lighting, the bug casts the shadow and looks grounded.\n\nTo use shadows, like occlusion, simply add .receivesLighting into your option set.\n\nThese new shadows are similar to the shadows that you're used to seeing on horizontally anchored objects.\n\nHowever, because we're using real-world surfaces you no longer have to anchor these objects to horizontal planes.\n\nThis means that any entity that you have in the virtual world will cast a shadow onto the real world.\n\nIt's important to note though that the shadows imitate a light shining straight down.\n\nThis means you won't see shadows on walls.\n\nAnd for those shadows, you still have to anchor your entity vertically.\n\nWe can now move on to our third option: physics.\n\nPreviously, for AR experiences you were just limited to using planes or various primitives to represent your world.\n\nThis is unrealistic, because the real world doesn't conform to these shapes.\n\nNow, because of scene understanding physics you can see that when the bug disintegrates, the small pieces bounce off the steps and scatter all around.\n\nTo use physics, you add the .physics option into your scene understanding option set.\n\nThere's a couple of specifics you should know, however.\n\nFirst, we consider real-world objects to be static with infinite mass.\n\nThis means that they're not movable as you'd expect in real life.\n\nSecond, these meshes are constantly updating.\n\nThis means that you should never expect objects to stay still, especially on non-planer surfaces.\n\nThird, the reconstructed mesh is limited to regions where the user has scanned.\n\nThis means that if the user has never scanned the floor, there will be no floor in your scene.\n\nAnd as a result, objects will fall right through.\n\nYou should design your experience to make sure that the user has scanned the room before starting.\n\nAnd fourth, the mesh is an approximation of the real world.\n\nWhile the occlusion mask is very accurate, the mesh for physics is less so.\n\nAs a result, don't expect the mesh to have super crisp edges as you can see in the image on the right.\n\nLastly, if you do use physics, collaborative sessions are not supported.\n\nCollaborative sessions will still work, just with the other scene understanding options.\n\nLet's now look at our last option, collision.\n\nCollision is a bigger topic.\n\nBut before we get into that, to use collision, similar to the previous three options, just insert .collision into your scene understanding options.\n\nNow the reason collision is a bigger topic is because it has two use cases.\n\nThe first is ray-casting.\n\nRay-casting is a super powerful tool.\n\nYou can use it to do a lot of things such as pathfinding, initial object placement, line of sight testing, and keeping things on meshes.\n\nThe next use case is collision events.\n\nWe want to do some action when we detect a collision between a real-world object and a virtual one.\n\nSo let's recap.\n\nLet's see how we use the two use cases in our experience.\n\nLet's start by looking at ray-casting.\n\nYou can see how the bug stays on the tree and finds different points to visit.\n\nRay-casting allows us to keep the bug on the tree by ray-casting from the body towards the tree, to find the closest contact point.\n\nAnd ray-casting is also used to find different points for the bug to visit.\n\nAnd let's look at the next case, collision events.\n\nBy using collision events, we were able to detect when the bug is tossed and hits the real world.\n\nWhen it does, we make it disintegrate.\n\nSo now that we've seen these two use cases and how they helped build the experience, there's still one more thing you need to know about using them.\n\nRay-casting returns a list of entities and collision events happen between two entities.\n\nThis means we need an entity, an entity that corresponds to a real-world object.\n\nAs a result, we're introducing the scene understanding entity.\n\nA scene understanding entity is just an entity, an entity that consists of various components.\n\nIt contains a transform component, collision component, and a physics component.\n\nAnd these are automatically added, based on your scene understanding options.\n\nThere's still one more component, and that is the scene understanding component.\n\nThe scene understanding component is unique to a scene understanding entity.\n\nIt is what makes a scene understanding entity, a scene understanding entity.\n\nWhen looking for a real-world object in a collision event or ray-casting results, you simply need to find an entity with this component.\n\nAnd similarly, with this new component we have a new HasSceneUnderstanding trait.\n\nAll scene understanding entities will conform to this trait.\n\nIt's really important to realize that these entities are created and managed by RealityKit.\n\nYou should consider these entities read-only and not modify properties on their components.\n\nDoing so can result in undefined behavior.\n\nSo now that we have scene understanding entities in our tool box and we know how to find them, let's take a look at how we can use them in code for the collision use cases we saw.\n\nWe'll start with ray-casting.\n\nIn this sample, we'll look at implementing some simple object avoidance for a bug entity.\n\nWe start by figuring out where the bug is looking and get a corresponding ray.\n\nOnce we have this ray, we can then do a RealityKit ray-cast.\n\nThe ray-cast returns a list containing both virtual and real-world entities.\n\nWe only want the real-world entities.\n\nThus, we filter using the HasSceneUnderstanding trait.\n\nAnd with these filtered results, we know that the first entity into the results is the closest real world object.\n\nThen finally, we can do some simple object avoidance by looking at the distance to the nearest object and taking an action.\n\nMaybe we go right, maybe we go left.\n\nSo now let's move on to another code example, a code example using collision events.\n\nIn this sample, we'll look at implementing something similar to the bug disintegration we saw in the experience.\n\nWe start by subscribing to collision events between all entities.\n\nWhen we get a callback for a collision, we need to figure out if it was with a real-world object.\n\nWe need to see if either entity is a scene understanding entity by using the HasSceneUnderstanding trait.\n\nIf neither of these entities conform to this trait, then, well, we know we don't have a real-world collision.\n\nIf, however, one entity does conform to this trait, then we know we have a real-world collision and that the other entity is the bug.\n\nWe can then make the bug disintegrate by doing an asset swap.\n\nNow this leads to one question that I want to discuss.\n\nWe can react when objects collide against the real world, but what if we don't want them to collide at all? And to do this, we need to use collision filters and collision filters need collision groups.\n\nIf we want objects not to collide with the real world, we need to filter them out, using the collision group for the real world.\n\nAnd as a result, we're introducing a new collision group called scene understanding for real-world objects.\n\nTo use this group, set your collision filter to include or not include scene understanding to filter the collision appropriately.\n\nAnd because RealityKit manages scene understanding entities, it automatically sets the collision groups on them.\n\nSo you don't have to manually do this.\n\nThat covers all of the collision related aspects of scene understanding.\n\nUsing all of the scene understanding features I talked about, you can now create a great app.\n\nHowever, once you start building your app, you might run into some issues and then you're not sure if it's the mesh that's the problem or your logic.\n\nSo to help with this, we've added the ability to visualize the mesh.\n\nLet's take a look.\n\nHere you can see a video with the debug mesh visualization turned on.\n\nThis shows us the raw, real-world mesh.\n\nThe mesh is color coded by the distance away from the camera.\n\nYou can also see how this mesh is constantly updating, and the reconstructed mesh is an approximation of the real world.\n\nYou can see how the mesh is not crisp at the edges of the stairs.\n\nTo enable this visualization, add the .showSceneUnderstanding option into the ARViews debug options.\n\nThe colors you saw in the video are color coded using the chart below.\n\nYou can see that the color varies by distance and past five meters, everything is colored white.\n\nAnd it's important to note that this option shows you the raw, real-world mesh.\n\nIf you want to see the physics mesh, then you just turn on the regular physics debug view that's already available.\n\nAnd that covers everything scene understanding.\n\nAnd as you can tell, there was a lot of cool stuff you can do with scene understanding.\n\nAnd so, I want to summarize some key takeaways.\n\nThe first is the goal of scene understanding.\n\nThe goal of scene understanding is to make your virtual content interact with the real world.\n\nAnd one set of these interactions is through occlusion and shadows.\n\nReal-world objects will occlude and receive shadows from virtual ones.\n\nAnother set of interactions is through physics and collision.\n\nPhysics lets objects physically interact with the real world.\n\nCollision, on the other hand, lets you know when objects physically collide.\n\nIt also enables you to ray-cast against the real world.\n\nReal-world objects have corresponding scene understanding entities.\n\nThese entities can be identified using the HasSceneUnderstanding trait.\n\nIt's important to remember that these entities are created and managed by RealityKit and they should be considered read-only.\n\nAnd lastly, we have debug mesh visualization.\n\nThis allows you to view the raw, real-world mesh.\n\nAnd that's it for scene understanding.\n\nWe can now move on to our next major feature: improved rendering debugging.\n\nRendering is a huge pipeline with lots of small components.\n\nThis includes model loading, material setup, scene lighting, and much more.\n\nAnd to help debug rendering-related issues, we've added the ability to inspect various properties related to the rendering of your entity.\n\nLet me show you an example of what I mean by looking at our bug.\n\nLet's have a look at its base color texture.\n\nHow about its normal map or its texture coordinates? So we can see these properties, but how did they actually help us? They can help us because you can look at normals and texture coordinates to ensure that your model was loaded correctly.\n\nThis is especially important if you find a model off the Internet and are having issues with its rendering.\n\nMaybe the model was just bad.\n\nIf you're using a simple material on an entity and setting the base color, roughness or metallic parameter and things don't look right, you can inspect those parameters to verify that they are set correctly.\n\nAnd finally, you can use PBR-related outputs, such as diffuse lighting received or specular lighting received, to know how much to tweak your material parameters.\n\nNow, to visualize these properties, we've added a new component: the debug model component.\n\nTo enable the visualization of a property, simply choose a property, create a debug model component using the property, and then assign it to your entity.\n\nAnd you can choose from a huge list of properties.\n\nCurrently we have 16.\n\nThese can be grouped as vertex attributes, material parameters, and PBR outputs.\n\nFinally, one last thing to note: the visualization only applies to the targeted entity and is not inherited by its children.\n\nUSDZ files may have multiple entities with a varying hierarchy.\n\nAs a result, you need to add the component to each and every entity that you want to inspect.\n\nAnd that's it.\n\nHopefully with this component, you're able to iterate and debug rendering problems much faster.\n\nThat covers everything related to improved rendering debugging.\n\nWe've also covered all of the features that are RealityKit specific.\n\nLet's move on to our next section: integration with ARKit 4.\n\nARKit 4 has many updates this year.\n\nThere are two updates that relate to RealityKit.\n\nThe first is Face Tracking.\n\nSupport for Face Tracking is now extended to devices without a TrueDepth camera, as long as they have an A12 processor or later.\n\nThis includes new devices, such as the iPhone SE.\n\nAs a RealityKit developer, if you were using face anchors before, whether you created them using code or Reality Composer, your applications should now work without any changes.\n\nThe second ARKit feature is location anchors.\n\nLocation anchors let you create anchors using real-world coordinates.\n\nARKit takes these coordinates and converts into locations relative to your device.\n\nThis means that you can now place AR content at specific locations in the real world.\n\nAs a RealityKit developer to use location anchors, you create an anchor entity using a location anchor.\n\nAny virtual content that you anchor under this entity will show at this specific location.\n\nAnd creating an anchor entity is also super simple since ARKit's new ARGeoAnchor class is just a subclass of ARAnchor, you can call the already existing anchor initializer to create an anchor entity.\n\nNow, one thing to note is that ARKit also introduced a new ARGeo tracking configuration for location anchors.\n\nAnd since this is not a world tracking configuration, you need to manually configure and start a location anchor session in the ARView.\n\nThis also means certain features -- like scene understanding that I just talked about -- will not work.\n\nIf you want to learn more, I suggest checking out the Introducing ARKit 4 talk.\n\nIt will go over how to set up your session, how to create anchors and how to use them with RealityKit.\n\nAnd that summarizes the main things you need to know about ARKit 4 and its integration with RealityKit for iOS14.\n\nThis also concludes all of the features that I want to talk to you about this year.\n\nAnd there were a lot of them.\n\nSo I want to summarize everything we've learned so far.\n\nWe started with video materials.\n\nVideo materials allow you to use videos as a texture and spatial audio source on your entity.\n\nThis can be used for a lot of things, such as sprite lighting effects or instructional videos.\n\nNext, thanks to scene understanding using the brand new LiDAR sensor, we're able to bring the real world into your virtual one through many ways, such as occlusion, shadows and physics.\n\nYou can now leverage ray-casting to implement smart character behavior to have it react with the real world.\n\nAnd you can also use collision events to implement collision responses when your objects collide with the real world.\n\nAnd then our next update was an improved ability to debug rendering problems.\n\nWith our new debug model component, you can inspect any rendering related property on your entity, such as vertex attributes, material parameters and PBR related outputs.\n\nWe then had ARKit related integrations.\n\nARKit extended Face Tracking support on devices without TrueDepth camera.\n\nThis also means face anchors will work on a lot more devices like the new iPhone SE, without requiring any code changes.\n\nAnd finally, ARKit added location anchors.\n\nLocation anchors allow you to place AR content at specific locations in the real world using RealityKit.\n\nAnd that concludes my talk.\n\nThank you for your time, and I can't wait to see what incredible content you will make using these new features.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "4:52",
      "title": "Loading a video material",
      "language": "swift",
      "code": "// Use AVFoundation to load a video\nlet asset = AVURLAsset(url: Bundle.main.url(forResource: \"glow\", withExtension: \"mp4\")!)\nlet playerItem = AVPlayerItem(asset: asset)\n\n// Create a Material and assign it to your model entity...\nlet player = AVPlayer()\nbugEntity.materials = [VideoMaterial(player: player)]\n\n// Tell the player to load and play\nplayer.replaceCurrentItem(with: playerItem)\nplayer.play()"
    },
    {
      "timestamp": "13:58",
      "title": "Implementing object avoidance with scene understanding",
      "language": "swift",
      "code": "// Get the position and forward direction of the bug in world space\nlet bugOrigin = bug.position(relativeTo: nil)\nlet bugForward = bug.convert(direction: [0, 0, 1], relativeTo: nil)\n\n// Perform a raycast \nlet collisionResults = arView.scene.raycast(origin: bugOrigin, direction: bugForward)\n\n// Get all hits against a Scene Understanding Entity\nlet filteredResults = collisionResults.filter { $0.entity as? HasSceneUnderstanding }\n\n// Pick the closest one and get the collision point\nguard let closestCollisionPoint = filteredResults.first?.position else {\n\treturn\n}\n\nif length(bugOrigin - closestCollisionPoint) < safeDistance {\n  // Avoid obstacle too close to objectâ€™s forward\n}"
    },
    {
      "timestamp": "14:48",
      "title": "Using collision events with a scene understanding entity",
      "language": "swift",
      "code": "// Subscribe to all collision events\narView.scene.subscribe(to: CollisionEvents.Began.self) { event in\n    // Get any entity if it conforms to HasSceneUnderstanding\n    guard let sceneUnderstandingEntity = (event.entityA as? HasSceneUnderstanding) \n                                      ?? (event.entityB as? HasSceneUnderstanding)   \n    else { \n       // Did not collide with real world    \n       return \n    } \n    // The bug entity is the one that is not the scene understanding entity\n    let bugEntity = (sceneUnderstandingEntity == event.entityA)\n                   ? event.entityB : event.entityA \n\n   // Disintegrate the bug entity\n   â€¦\n}"
    },
    {
      "timestamp": "16:00",
      "title": "Real world collision filtering",
      "language": "swift",
      "code": "// Only collide with real world\nentity.collision?.filter.mask = [.sceneUnderstanding]\n\n// Never collide with real world\nentity.collision?.filter.mask = CollisionGroup.all.subtracting(.sceneUnderstanding)"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Creating a game with scene understanding",
        "url": "https://developer.apple.com/documentation/realitykit/creating_a_game_with_scene_understanding"
      },
      {
        "title": "RealityKit",
        "url": "https://developer.apple.com/documentation/RealityKit"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10612/5/1B6C5C51-471E-4D93-9198-4D6B9AAE7D89/wwdc2020_10612_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10612/5/1B6C5C51-471E-4D93-9198-4D6B9AAE7D89/wwdc2020_10612_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10074",
      "year": "2021",
      "title": "Dive into RealityKit 2",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10074"
    },
    {
      "id": "10075",
      "year": "2021",
      "title": "Explore advanced rendering with RealityKit 2",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10075"
    },
    {
      "id": "10611",
      "year": "2020",
      "title": "Explore ARKit 4",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10611"
    },
    {
      "id": "10601",
      "year": "2020",
      "title": "The artistâ€™s AR toolkit",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10601"
    },
    {
      "id": "603",
      "year": "2019",
      "title": "Introducing RealityKit and Reality Composer",
      "url": "https://developer.apple.com/videos/play/wwdc2019/603"
    }
  ],
  "extractedAt": "2025-07-18T10:33:32.596Z"
}