{
  "id": "10089",
  "year": "2023",
  "url": "https://developer.apple.com/videos/play/wwdc2023/10089/",
  "title": "Discover Metal for immersive apps",
  "speakers": [],
  "duration": "",
  "topics": [
    "Graphics & Games"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "♪ Mellow instrumental hip-hop ♪ ♪ Hey, I'm Pau Sastre Miguel, a Software Engineer at Apple.\n\nToday I'm going to talk about how to create immersive experiences with Metal on xrOS.\n\nThis year, with the launch of xrOS, you will be able to create immersive experiences with familiar technologies in the Apple ecosystem.\n\nWith RealityKit you will be able to create experiences that blend your virtual content with the real world.\n\nOn the other hand, if your application will take the user into a fully immersive experience, xrOS also enables you to completely replace the real world content with your own virtual content.\n\nWhen creating fully immersive experiences, you have a few choices when it comes to your rendering method.\n\nYou can still use RealityKit, or if you prefer, you can choose Metal and ARKit APIs.\n\nRecRoom is a great example of an application that provides a fully immersive experience using CompositorServices to create a rendering session, Metal APIs to render frames, and ARKit to get world and hand tracking.\n\nThey were able to bring support to all these technologies thanks to the Unity editor.\n\nIf you'd like to write your own engine, the CompositorServices API gives you access to Metal rendering on xrOS.\n\nYou can combine it with ARKit, which adds world tracking and hand tracking, to create a fully immersive experience.\n\nCompositorServices is the key to configure your engine to work on xrOS.\n\nI'll show you how to setup the render loop and then how to render one frame.\n\nFinally, I'll show you how to use ARKit to make your experience interactive.\n\nStart with the architecture of an xrOS app.\n\nYou'll get the most out of today's session if you have previous experience with Metal APIs and Metal rendering techniques.\n\nIf you haven't used Metal before, check out the code samples and documentation over in developer.apple.com/Metal.\n\nWhen you create immersive experiences on xrOS with Metal, you'll start with SwiftUI to create the application and the rendering session.\n\nAfter creating the rendering session, you can switch to a language you might be more familiar with, like C or C++, to define the inner parts of the engine.\n\nYou start by creating a type that conforms to the SwiftUI app protocol.\n\nTo conform to this protocol, you will define the list of scenes in your app.\n\nOn xrOS, there are three main scenes types.\n\nThe window type provides an experience similar to 2D platforms like macOS.\n\nThe volume type renders content within its bounds and it coexists in the Shared Space with other applications.\n\nAnd ImmersiveSpace allows you to render content anywhere.\n\nWhenever you render fully immersive experiences with Metal, you will choose the ImmersiveSpace type.\n\nImmersiveSpace is a new SwiftUI Scene type available on xrOS.\n\nIt serves as the container for fully immersive experiences.\n\nTo learn how to use ImmersiveSpace, check out the session \"Go beyond the window with SwiftUI.\" When you create an ImmersiveSpace scene, your application provides the content by using a type that conforms to the ImmersiveSpaceContent protocol.\n\nOften, when creating content for an ImmersiveSpace Scene, applications will use RealityKit.\n\nIt uses CoreAnimation and MaterialX under the hood.\n\nBut if instead, you'd like to use the power of Metal to render the content of your application, you've got another option.\n\nThe CompositorServices API uses Metal and ARKit to provide immersive rendering capabilities to your application.\n\nThe new CompositorServices API, introduced in xrOS, provides a Metal rendering interface to be able to render the contents of an ImmersiveSpace.\n\nWith CompositorServices, applications can render directly into the compositor server.\n\nIt has a low IPC overhead to minimize latency, and it is built from the ground up to support both C and Swift APIs.\n\nWhen using CompositorServices, the ImmersiveSpaceContent is called CompositorLayer.\n\nTo create a CompositorLayer you will need to provide two parameters.\n\nThe first one is the CompositorLayerConfiguration protocol.\n\nThis protocol defines the behavior and capabilities of your rendering session.\n\nThe second is the LayerRenderer.\n\nThis is the interface to the layer rendering session.\n\nYour application will use this object to schedule and render new frames.\n\nWhen writing an immersive experience with Metal, start by defining the app type.\n\nAs the scene type, use ImmersiveSpace.\n\nFor the content type, use a CompositorLayer.\n\nOnce the CompositorLayer is ready to render content, the system will call the application with the instance of the rendering session.\n\nHere is a good place to create the instance of your custom engine.\n\nNow that you have the engine instance, you can create the render thread and run the render loop by calling start.\n\nOne thing to take into account when defining the scene list in your application, is that by default SwiftUI creates a window scene, even if the first scene in your app is an ImmersiveSpace.\n\nTo change that default behavior, you can modify the info plist of your app.\n\nYou can add the key UIApplicationPreferred DefaultSceneSessionRole to your application scene manifest to change the default scene type of your application.\n\nIf you are using a space with a Compositor SpaceContent, you will use CPSceneSessionRole ImmersiveSpaceApplication.\n\nAfter setting up the application, and before getting in the render loop, you'll tell CompositorServices how to configure the LayerRenderer.\n\nTo provide a configuration to the CompositorLayer, you will create a new type that conforms to the CompositorLayerConfiguration protocol.\n\nThis protocol allows you to modify the setup and some of the behavior of the rendering session.\n\nThe CompositorLayerConfiguration provides you two parameters.\n\nThe first one is the layer capabilities.\n\nIt enables you to query what features are available on the device.\n\nUse the capabilities to create a valid configuration.\n\nAnd the second parameter is the LayerRenderer Configuration.\n\nThis type defines the configuration of your rendering session.\n\nWith the configuration, you can define how your engine maps its content into the layer, enable foveated rendering, and define the color management of your pipeline.\n\nNow, I'll talk about how each of these properties will affect your engine.\n\nThe first one is foveated rendering.\n\nThe main goal of this feature is to allow you to render content at a higher pixel-per-degree density without using a bigger texture size.\n\nIn a regular display pipeline, the pixels are distributed linearly in a texture.\n\nxrOS optimizes this workflow by creating a map that defines what regions in a display can use a lower sampling rate.\n\nThis helps reduce the power required to render your frame while maintaining the visual fidelity of the display.\n\nUsing foveation whenever possible is important, as it will result in a better visual experience.\n\nA great way to visualize how foveation affects your rendering pipeline is using Xcode's Metal Debugger.\n\nWith Metal Debugger, you can inspect the target textures and rasterization rate maps being used in the render pipeline.\n\nThis capture shows the contents of the texture without scaling for the rasterization rate map.\n\nYou can notice the different sample rates by focusing in the regions of the texture that are more compressed.\n\nWith the attachment viewer options in Metal Debugger, you can scale the image to visualize the final result that the display will show.\n\nCompositor provides the foveation map using an MTLRasterizationRateMap for each frame.\n\nIt is a good practice to always check if foveation is supported.\n\nThis will change depending on the platform.\n\nFor example, in the xrOS simulator, foveation is not available.\n\nTo enable foveation, you can set isFoveationEnabled on the configuration.\n\nThe second property is LayerRenderer layout.\n\nThis property is one of the most important configurations for your engine.\n\nIt defines how each display from the headset gets mapped into the rendered content of your application.\n\nEach eye first maps into a Metal texture provided by Compositor.\n\nThen Compositor provides the index of which slice to use within that texture.\n\nAnd finally, Compositor provides the viewport to use within that texture slice.\n\nThe LayerRenderer layout lets you choose different mappings between the texture slice and viewport.\n\nWith layered, Compositor will use one texture with two slices and two viewports.\n\nWith dedicated, Compositor will use two textures with one slice and one viewport each.\n\nAnd finally with shared, Compositor will use one texture, one slice, and two different viewports for that slice.\n\nChoosing which layout to use will depend on how you set up your rendering pipeline.\n\nFor example, with layered and shared, you will be able to perform your rendering in one single pass, so you can optimize your rendering pipeline.\n\nWith shared layout, it might be easier to port existing code bases where foveated rendering is not an option.\n\nLayered layout is the optimal layout since it allows you to render your scene in a single pass while still maintaining foveated rendering.\n\nThe last configuration property to discuss is color management.\n\nCompositor expects the content to be rendered with extended linear display P3 color space.\n\nxrOS supports an EDR headroom of 2.0.\n\nThat is two times the SDR range.\n\nBy default, Compositor does not use a pixel format that is HDR renderable, but if your application supports HDR, you can specify rgba16Float in the layer configuration.\n\nIf you want to know more about how to render HDR with EDR, checkout the session \"Explore HDR rendering with EDR.\" To create a custom configuration in your application, start by defining a new type that conforms to the CompositorLayerConfiguration protocol.\n\nTo conform to this protocol, add the makeConfiguration method.\n\nThis method provides the layer capabilities and a configuration you can modify.\n\nTo enable the three properties I mentioned before, first check if foveation is supported.\n\nThen check what layouts are supported in this device.\n\nWith this information, you can set a valid layout in the configuration.\n\nIn some devices like the simulator, where the Compositor only renders one view, layered won't be available.\n\nFor foveation, set it to true if the device supports it.\n\nAnd finally, set the colorFormat to rgba16Float to be able to render HDR content.\n\nReturning to the code that created the Compositor layer, you can now add the configuration type you just created.\n\nNow that the rendering session is configured, you can set up the render loop.\n\nYou'll start by using the LayerRenderer object from the CompositorLayer.\n\nFirst, you'll load the resources and initialize any objects that your engine will need to render frames.\n\nThen check the state of the layer.\n\nIf the layer is paused, wait until the layer is running.\n\nOnce the layer is unblocked from the wait, check the layer state again.\n\nIf the layer is running, you'll be able to render a frame.\n\nAnd once that frame is rendered, check the layer state again before rendering the next frame.\n\nIf the layer state is invalidated, free the resources you created for the render loop.\n\nNow, it's time to define the main function of the render_loop.\n\nUntil now I've been using Swift since the ImmersiveSpace API is only available in Swift.\n\nBut from here I will switch to C to write the render loop.\n\nAs I mentioned, the first step in the render loop is to allocate and initialize all the objects you'll need to render frames.\n\nYou'll do this by calling the setup function in your custom engine.\n\nNext, is the main section of the loop.\n\nThe first step is to check the layerRenderer state.\n\nIf the state is paused, the thread will sleep until the layerRenderer is running.\n\nIf the layer state is running, the engine will render one frame.\n\nAnd finally, if the layer has been invalidated, the render loop will finish.\n\nThe last step of the render_loop function will be to clear any used resources.\n\nNow that the app is going through the render loop, I'll explain how to render one frame.\n\nRendering content in xrOS is always from the point of view of the device.\n\nYou can use ARKit to obtain the device orientation and translation.\n\nARKit is already available on iOS, and now xrOS is introducing a whole new API, which has additional features that can help you create immersive experiences.\n\nWith ARKit, you can add world tracking, hand tracking, and other world sensing capabilities to your application.\n\nThe new ARKit API is also built from the ground up to support C and Swift APIs, which will allow for an easier integration with existing rendering engines.\n\nTo learn more about ARKit on xrOS, check out \"Meet ARKit for spatial computing.\" Within the render loop, it's time to render one frame.\n\nWhen rendering a frame, Compositor defines two main sections.\n\nThe first one is the update.\n\nHere's where you will do any work that is not input-latency critical.\n\nThis can be things like updating the animations in your scene, updating your characters, or gathering inputs in your system like hand skeleton poses.\n\nThe second section of the frame is the submission section.\n\nHere's where you will perform any latency-critical work.\n\nYou'll also render any content that is headset-pose dependent here.\n\nIn order to define the timing for each of those sections, Compositor provides a timing object.\n\nThis diagram defines how the timing affects the different frame sections.\n\nThe CPU and GPU tracks represent the work that is being done by your application.\n\nAnd the Compositor track represents the work done by the Compositor server to display your frame.\n\nThe timing type from Compositor Services defines three main time values.\n\nFirst is the optimal input time.\n\nThat is the best time to query the latency-critical input and start rendering your frame.\n\nSecond is the rendering deadline.\n\nThat is the time by when your CPU and GPU work to render a frame should be finished.\n\nAnd third is presentation time.\n\nThat is the time when your frame will be presented on display.\n\nIn the two sections of your frame, The update section should happen before the optimal input time.\n\nAfter the update, you will wait for the optimal input time before starting the frame submission.\n\nThen you will perform the frame submission, which will submit the render work to the GPU.\n\nIt is important to notice that the CPU and GPU work needs to finish before the rendering deadline, otherwise the Compositor server won't be able to use this frame and will use a previous one instead.\n\nFinally, on the rendering deadline, the Compositor server will composite this frame with the other layers in the system.\n\nBack to the render loop code, it's time to define the render_new_frame function.\n\nIn your engine's render_new_frame function, you will first query a frame from the layerRenderer.\n\nWith the frame object, you will be able to predict the timing information.\n\nUse that timing information to scope the update and submit intervals.\n\nNext, implement the update section.\n\nDefine this section by calling the start and end update on the frame.\n\nInside, you will gather the device inputs and update the contents of the frame.\n\nOnce the update is finished, wait for the optimal input time before starting the submission.\n\nAfter the wait, define the submission section by calling start and end submission.\n\nInside this section, first query the drawable object.\n\nSimilar to CAMetalLayer, the drawable object contains the target texture and the information that you will need to set up the render pipeline.\n\nNow that you have your drawable, you can get the final timing information that Compositor will use to render this frame.\n\nWith the final timing, you can query the ar_pose.\n\nIt is important to set the pose in the drawable since it will be used by the Compositor to perform reprojection on the frame.\n\nHere I'm getting the pose by calling the get_ar_pose function in my engine object.\n\nBut you will need to implement the contents of this function using the ARKit world tracking APIs.\n\nThe last step of the function will be to encode all the GPU work and submit the frame.\n\nInside the submit_frame, use the drawable to render the contents of the frame as usual.\n\nNow that the render loop is rendering frames, it's time to make your immersive experience interactive.\n\nThis video shows how RecRoom using Unity is already taking advantage of the ARKit and Compositor APIs to add interactivity to their application.\n\nThere are two main input sources driving this interaction.\n\nARKit's HandTracking is providing the hand skeleton to render virtual hands.\n\nAnd pinch events from the LayerRenderer are driving the user interactions.\n\nIn order to make the experience interactive, you'll first gather the user input and then apply it to the contents of your scene.\n\nAll this work will happen in the update section of the frame.\n\nThere are two main input sources, the LayerRenderer and the ARKit HandTracking provider.\n\nWith the LayerRenderer, you will get updates every time the application receives a pinch event.\n\nThese updates are exposed in the form of spatial events.\n\nThese events contains three main properties.\n\nThe phase will tell you if the event is active, if it finished, or if it got canceled.\n\nThe selection ray will allow you to determine the content of the scene that had the attention when the event began.\n\nAnd the last event property is the manipulator pose.\n\nThis is the pose of the pinch and gets updated every frame for the duration of the event.\n\nFrom the HandTracking API, you will be able to get skeleton for both the left and right hands.\n\nNow, it's time to add input support in the code.\n\nBefore gathering the input, you will decide if your application is rendering virtual hands or if it uses passthrough hands.\n\nAdd the upperLimbVisibility scene modifier to the ImmersiveSpace to make the passthrough hands visible or hidden.\n\nTo access the spatial events, go back to where you defined the CompositorLayer render handler.\n\nHere, register a block in the layerRenderer to get updates every time there is a new spatial event.\n\nIf you are writing your engine code in C, you'll map the SwiftUI spatial event to a C type.\n\nInside the C code, you can now receive the C event collection.\n\nOne thing to keep in mind when handling the spatial event updates is that the updates get delivered in the main thread.\n\nThis means that you will use some synchronization mechanism when reading and writing the events in your engine.\n\nNow that the events are stored in the engine, it's time to implement the gather input function.\n\nThe first step is to create an object to store the current input state for this frame.\n\nThis input state will store the events that you received from the LayerRenderer.\n\nMake sure that you are accessing your internal storage in a safe way.\n\nAs for the hand skeleton, you can use the hand tracking provider API from ARKit to get the latest hand anchors.\n\nAnd now that your application has input support, you have all the tools at your disposal to create fully immersive experiences on xrOS.\n\nTo recap, with SwiftUI, you will define the application.\n\nWith CompositorServices and Metal, you will set up the render loop and display 3D content.\n\nAnd finally, with ARKit, you will be able to make your experience interactive.\n\nThank you for watching! ♪",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "4:45",
      "title": "App architecture",
      "language": "swift",
      "code": "@main\nstruct MyApp: App {\n    var body: some Scene {\n        ImmersiveSpace {\n            CompositorLayer { layerRenderer in\n                let engine = my_engine_create(layerRenderer)\n                let renderThread = Thread {\n                    my_engine_render_loop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n            }\n        }\n    }\n}"
    },
    {
      "timestamp": "10:32",
      "title": "CompositorLayer Configuration",
      "language": "swift",
      "code": "// CompositorLayer configuration\n\nstruct MyConfiguration: CompositorLayerConfiguration {\n    func makeConfiguration(capabilities: LayerRenderer.Capabilities,\n                           configuration: inout LayerRenderer.Configuration) {\n\n        let supportsFoveation = capabilities.supportsFoveation\n        let supportedLayouts = capabilities.supportedLayouts(options: supportsFoveation ?\n                                                             [.foveationEnabled] : [])\n\n        configuration.layout = supportedLayouts.contains(.layered) ? .layered : .dedicated\n\n        configuration.isFoveationEnabled = supportsFoveation\n\n        // HDR support\n        configuration.colorFormat = .rgba16Float\n   }\n}"
    },
    {
      "timestamp": "12:20",
      "title": "Render loop",
      "language": "swift",
      "code": "void my_engine_render_loop(my_engine *engine) {\n    my_engine_setup_render_pipeline(engine);\n\n    bool is_rendering = true;\n    while (is_rendering) @autoreleasepool {\n        switch (cp_layer_renderer_get_state(engine->layer_renderer)) {\n            case cp_layer_renderer_state_paused:\n                cp_layer_renderer_wait_until_running(engine->layer_renderer);\n                break;\n            case cp_layer_renderer_state_running:\n                my_engine_render_new_frame(engine);\n                break;\n            case cp_layer_renderer_state_invalidated:\n                is_rendering = false;\n                break;\n        }\n    }\n\n    my_engine_invalidate(engine);\n}"
    },
    {
      "timestamp": "15:56",
      "title": "Render new frame",
      "language": "swift",
      "code": "void my_engine_render_new_frame(my_engine *engine) {\n    \n    cp_frame_t frame = cp_layer_renderer_query_next_frame(engine->layer_renderer);\n    if (frame == nullptr) { return; }\n    \n    cp_frame_timing_t timing = cp_frame_predict_timing(frame);\n    if (timing == nullptr) { return; }\n\n    cp_frame_start_update(frame);\n\n    my_input_state input_state = my_engine_gather_inputs(engine, timing);\n    my_engine_update_frame(engine, timing, input_state);\n\n    cp_frame_end_update(frame);\n\n    // Wait until the optimal time for querying the input\n    cp_time_wait_until(cp_frame_timing_get_optimal_input_time(timing));\n\n    cp_frame_start_submission(frame);\n\n    cp_drawable_t drawable = cp_frame_query_drawable(frame);\n    if (drawable == nullptr) { return; }\n\n    cp_frame_timing_t final_timing = cp_drawable_get_frame_timing(drawable);\n    ar_pose_t pose = my_engine_get_ar_pose(engine, final_timing);\n    cp_drawable_set_ar_pose(drawable, pose);\n\n    my_engine_draw_and_submit_frame(engine, frame, drawable);\n\n    cp_frame_end_submission(frame);\n}"
    },
    {
      "timestamp": "18:57",
      "title": "App architecture + input support",
      "language": "swift",
      "code": "@main\nstruct MyApp: App {\n    var body: some Scene {\n        ImmersiveSpace {\n            CompositorLayer(configuration: MyConfiguration()) { layerRenderer in\n                let engine = my_engine_create(layerRenderer)\n                let renderThread = Thread {\n                    my_engine_render_loop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n                layerRenderer.onSpatialEvent = { eventCollection in\n                    var events = eventCollection.map { my_spatial_event($0) }\n                    my_engine_push_spatial_events(engine, &events, events.count)\n                }\n            }\n        }\n        .upperLimbVisibility(.hidden)\n    }\n}"
    },
    {
      "timestamp": "18:57",
      "title": "Push spatial events",
      "language": "swift",
      "code": "void my_engine_push_spatial_events(my_engine *engine,\n                                   my_spatial_event *spatial_event_collection,\n                                   size_t event_count) {\n    os_unfair_lock_lock(&engine->input_event_lock);\n    \n    // Copy events into an internal queue\n    \n    os_unfair_lock_unlock(&engine->input_event_lock);\n}"
    },
    {
      "timestamp": "19:57",
      "title": "Gather inputs",
      "language": "swift",
      "code": "my_input_state my_engine_gather_inputs(my_engine *engine,\n                                       cp_frame_timing_t timing) {\n    my_input_state input_state = my_input_state_create();\n\n    os_unfair_lock_lock(&engine->input_event_lock);\n    input_state.current_pinch_collection = my_engine_pop_spatial_events(engine);\n    os_unfair_lock_unlock(&engine->input_event_lock);\n\n    ar_hand_tracking_provider_get_latest_anchors(engine->hand_tracking_provider,\n                                                 input_state.left_hand,\n                                                 input_state.right_hand);\n\n    return input_state;\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Drawing fully immersive content using Metal",
        "url": "https://developer.apple.com/documentation/CompositorServices/drawing-fully-immersive-content-using-metal"
      },
      {
        "title": "Metal",
        "url": "https://developer.apple.com/documentation/Metal"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10089/5/49D0E645-DBE8-4D7A-9637-C4D744C86894/downloads/wwdc2023-10089_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2023/10089/5/49D0E645-DBE8-4D7A-9637-C4D744C86894/downloads/wwdc2023-10089_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10096",
      "year": "2023",
      "title": "Build great games for spatial computing",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10096"
    },
    {
      "id": "10111",
      "year": "2023",
      "title": "Go beyond the window with SwiftUI",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10111"
    },
    {
      "id": "10082",
      "year": "2023",
      "title": "Meet ARKit for spatial computing",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10082"
    },
    {
      "id": "10161",
      "year": "2021",
      "title": "Explore HDR rendering with EDR",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10161"
    }
  ],
  "extractedAt": "2025-07-18T10:30:33.074Z"
}