{
  "id": "10096",
  "year": "2020",
  "url": "https://developer.apple.com/videos/play/wwdc2020/10096/",
  "title": "Explore Packages and Projects with Xcode Playgrounds",
  "speakers": [],
  "duration": "",
  "topics": [
    "Developer Tools"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "Hello and welcome to WWDC.\n\nWelcome to \"Explore Packages and Projects with Xcode Playgrounds.\" I'm Chris Miles, an engineering manager on the Xcode team.\n\nDevelopers love using Xcode Playgrounds to quickly try out new ideas and to learn and explore APIs and frameworks. Today, I'm excited to tell you about enhancements we've made to Xcode 12 to make Playgrounds work seamlessly with Swift packages and frameworks in your projects. In Xcode 12, Playgrounds is fully integrated with Xcode's modern build system. That integration brings with it a lot of improvements to how Playgrounds works with packages, projects and resources.\n\nFirst, I'll show you how Playgrounds now works well in Swift packages and makes a great option for package documentation containing runnable code.\n\nThen I'm going to demonstrate improvements for Playgrounds importing project targets, like frameworks and package dependencies.\n\nIn the final part of the presentation, I'm going to demonstrate working with resources that need build support, like ML models and asset catalogs, in a playground.\n\nSo, let's get started by looking at how Xcode Playgrounds works with Swift packages.\n\nHere I have a Swift package checked out called NutritionFacts. I'm gonna open it in Xcode by double-clicking \"Package.swift.\" NutritionFacts provides accurate nutritional information for many common types of food. It's a really handy package to use in recipe and food apps.\n\nWhat's great about NutritionFacts is the authors have included rich documentation explaining how to use the API, and, even better, the documentation was written as a playground. Playgrounds are great for documentation, and they now work seamlessly with packages in Xcode 12. So, let's take a look.\n\nIn the navigator, I'm gonna open the Playgrounds folder and select the NutritionFacts playground. Here, we see a description of the package followed by detailed documentation about how to use the API, along with example code snippets. And being a playground, we can run that code. Let's do that now by hitting \"execute\" in the toolbar.\n\nThe playground will execute, and on the right, we see a live view.\n\nThat is a view provided by the package. It shows NutritionFacts in a familiar format.\n\nIn the editor, we see an example of looking up an item by identifier and fetching nutritional information about it. The playground helps us understand the code by showing the result of each statement in the results bar on the right.\n\nAs you can see, Playgrounds in Xcode 12 work seamlessly with packages. This was made possible because of Playgrounds' integration with the modern build system, which knows how to build packages and make them available for importing into playground code. I recommend making use of a playground the next time you need to write documentation or a tutorial for your Swift package. Don't forget to take advantage of the fact that users can run the example code and see live results in the playground. Next, I'd like to show you how Xcode 12 makes it easier for Playgrounds to use framework and package dependencies from a project.\n\nI'm gonna use a project called Fruta, and let's open it in Xcode and run the project in the simulator. Fruta is an iOS and Mac app for browsing and ordering smoothies. You can even purchase smoothie recipes with Apple Pay so you can make your own smoothies at home.\n\nLet's take a look at the app in the simulator. We see a list of some of the smoothies available, and if we select one, we can see an option to buy it with Apple Pay, and scrolling up, we see some of the ingredients that are in the smoothie. Let's select one to see a detailed image, and tap the \"I\" button to see nutritional facts about that ingredient. Now, you may recognize this as the view provided by the NutritionFacts package. This project has a dependency on that package, as you can see in the left in the navigator.\n\nEven as a project dependency, any playgrounds in that package will still be available. Let's take a look by opening the package and opening the NutritionFacts playground that we opened earlier. Here, we see the same content, and, like before, we can execute the playground to see the results.\n\nSo, playgrounds in packages are available whether you open the package directly, like we did at the start, or whether it's part of your project as a package dependency, like it is here. Just remember that, like all files in a package dependency, the playground will be read-only.\n\nThis project also contains a framework called UtilityViews.\n\nLet's take a look at the project editor where we can see the UtilityView's framework target. It is now easier than before to use your frameworks in Playgrounds, as long as they are in the same workspace. Let me show you an example.\n\nIn the navigator, I'm gonna open the Playgrounds folder, and we see a playground called SmoothieLab. Let's open SmoothieLab. Notice some build activity at the top.\n\nThis was due to a new playground option in Xcode 12 called Build Active Scheme. Let me show you by opening the inspector, and here we see in the playground settings the new option at the bottom of the settings. When \"Build Active Scheme\" is enabled, it tells Xcode to automatically build all targets in the currently selected scheme and make any modules importable to the playground.\n\nSo, that's why when we opened the playground, Xcode built all of the targets for the currently selected scheme, and the playground could import the UtilityViews framework and the NutritionFacts package. SmoothieLab is a utility playground that we use to design new smoothies. We can select some raw ingredients, combine them in various quantities and then inspect their nutritional values.\n\nWe can even chart a calorie breakdown using a chart view from the UtilityViews framework.\n\nLet's run the playground to see the calorie breakdown for this smoothie.\n\nIn the live view, we see a chart of the calorie breakdown. Now, we usually like to keep the fat content to be less than about 25%, so let's tap the chart to take a look at the percentages.\n\nIt's about 33% in this case, so perhaps we could lower the amount of peanut butter, but I'll work on that later.\n\nAs you can see, this playground is a handy tool for helping us to combine ingredients to design well-balanced smoothies, at least from a nutritional perspective. We haven't designed an algorithm for calculating how good a smoothie might taste, but I'm fine with that as tasting is my favorite part of smoothie design. Playgrounds can now work seamlessly with project targets, such as frameworks and packages. Xcode will take care of building those targets so that they can be imported into your playground code. Just make sure to enable \"Build Active Scheme.\" The good news is that this option is enabled by default for new playgrounds.\n\nAlso make sure that the module you want to import is either part of the active scheme or a dependency on another target built by that scheme.\n\nAnother great enhancement in Xcode 12 is the availability of full build logs for playgrounds. Let's take a look at the build log for SmoothieLab. We'll switch to the report navigator, and at the top, we see the build log for the SmoothieLab playground. So, let's select it to open the build logs. In the build logs, we see all of the build details for the targets that were built as part of the active scheme, as well as the module that was built to support the playground, containing compiled sources and resources from within the playground. Build logs are invaluable for diagnosing build issues, and we're happy that they're available for playgrounds.\n\nAnother benefit of Playgrounds' integration with Xcode's modern build system, is that all resource types supported by the build system are now supported in Playgrounds. Let me show you an example of using some resources in a playground that require build system support.\n\nReturning to the Fruta project, I'd like to add a feature to Fruta where the user can point their iPhone's camera at their favorite fruit, and Fruta would magically suggest smoothies that match the fruit that it could see. To do this I'd like to use machine learning to perform object detection and image classification.\n\nThere are a number of classifiers available, so I'd like to try one of them with a few sample images to see if it would be suitable for the task, and I'm going to do that using a playground in Xcode. First, let's get some sample fruit images to test with, and I think we happen to have some already available in the Fruta project.\n\nLet's switch back to the simulator and navigate back, and remember we saw the image of some oranges before? And if we dismiss that, we see images of the other ingredients. So, let's find these in the project.\n\nI'm gonna hide the simulator, as we don't need that anymore, expand the Shared folder, and let's take a look in the asset catalog.\n\nWe see an app icon, some color sets, and in the Ingredients folder, we see all the images representing the ingredients that we put in our smoothies. I think this will be a good sample set for testing our machine-learning model. So, let's make a copy of this by Option-dragging this out to the desktop.\n\nAnd now we don't need the project open, so we'll close that, and instead create a new playground with \"File,\" \"New Playground.\" We use an iOS blank playground.\n\nAnd let's call it ML Fruta, and store it on the desktop.\n\nI'm gonna expand the size of the window to give us some more working space. Now, let's drag in the asset catalog into the resources folder of the playground. Notice that Playground compiles the asset catalog for us so that we can access the resources from our code.\n\nWe can do that using UIImage(named.\n\nSpecify the name of the folder within the asset catalog and the name of an image. And now let's execute the playground to see the results.\n\nIn the results, we see the width of the image, and if we QuickLook the image, we see a preview. That looks great. So, we have access to images from that asset catalog. Now we just need a machine-learning model.\n\nI'm gonna switch to Safari where I've been browsing the machine learning section of the Apple Developer website.\n\nIn the Core ML Models page, we see a number of image classification and object detection models. I'd like to try out this model called YOLOv3, which is able to detect multiple objects in a scene and classify each of them. I think that'd be good for pointing the camera at a bowl of fruit and having it identify all the fruit that it could see. So, let's click \"View models,\" and we can download the full-precision model at the top. To save us time, I've already done this, so let's minimize Safari.\n\nAnd I'll drag in the model from the desktop into the resources section of the playground. Xcode will automatically compile the model for us, and while it's doing that, let's select the model to see details about it in the editor. An important detail is the model class up near the top here. It tells us the name of the class that will be automatically generated by the build system for use in our Swift code. So, let's make use of that now.\n\nReturning to the playground, let's add the code to import CoreML. Then I'll call my variable yoloModel, and we'll use the class that was automatically generated for us, YOLOv3, to create a new instance with a default configuration, and we'll fetch the model property.\n\nLet's run the playground to see the results.\n\nWe get back a model description, which we can preview here, and see details about the inputs, outputs and other properties. So, that's great. We now have a machine-learning model, and images to test with, we just need to combine them together. An easy way to do that is using the Vision framework. I'm gonna replace this code and instead import the Vision framework.\n\nI'll define an array with three ingredients to start testing with, and then we can use our yoloModel to create an instance of the Vision framework's CoreMLModel. And with that, we can create a CoreMLRequest that we can use to make machine-learning requests on images.\n\nNow, I'd like to visualize the results of these requests by showing each image, and drawing a rectangle around any objects detected in the image. To help do that, I've got some code that I've used in the past. Let me QuickLook it for you. It defines a couple of structs to hold the results of our object detection and some SwiftUI views that we use to show the images and draw rectangles around any objects detected in the image. We won't go into detail about this code, but we'll make it available on our session website.\n\nSo, after dragging the support code into the sources section of the playground, Xcode will compile that, and now we can make use of the code.\n\nWe'll iterate over the ingredient names, fetch an image for each one from the asset catalog, and then we'll create an image request handler with that image.\n\nWe can use that to perform our machine-learning request. Then we just need to pass the results. Each result is a RecognizedObjectObservation.\n\nWe'll enumerate the observations, we'll fetch the label with the highest confidence and the bounding box of that object within the image, and return that as a RecognizedObject.\n\nFinally, for each image, we'll return the results as an ObjectDetectionResult, and let's preview the results that we get at the bottom.\n\nXcode will iterate over the three images, and then we can preview the results. Indeed we have a result for each image, so let's visualize them.\n\nWe can do that by setting a playground LiveView to the RecognizedObjectVisualizer, and pass it the results. Let's continue running.\n\nOn the right, we see the results of our object detection. The first image we passed in was of a banana, and the orange indicates that a banana was detected with 100% accuracy. So, that's a pretty good result.\n\nThe second image was of three oranges, and we can see that all three oranges were detected with high confidence. So, that worked really well.\n\nThe third image was of a bottle of almond milk, and we can see that the machine-learning model detected a bottle with pretty high confidence, although it didn't classify it specifically as a bottle of milk or almond milk. And that would be because this model has been trained to detect bottles but hasn't been specifically trained to detect bottles of milk or almond milk. And I think that's fine. For what this model's been trained for, it performed really well. I think this would be the model to use for our app. The next step would be to go ahead and train that model even further, with the types of ingredients that we wanna detect for our use case. To find out more about how to train your ML models, I suggest watching the WWDC 2019 session called \"Training Object Detection Models in Create ML.\" So, that was an example of using resources that require build system support in Playgrounds. Let's summarize what I've covered in this presentation. In Xcode 12, Playgrounds is fully integrated with the modern build system.\n\nWhen the new option \"Build Active Scheme\" is enabled, all targets in the currently selected scheme will be automatically built and made importable into playgrounds that are in the same workspace. Any frameworks or Swift packages can then be imported into your playground code.\n\nAll resource types the build system knows about can now be used in Playgrounds. For example, asset catalogs and ML Models, like I demonstrated.\n\nFull playground build logs are now available in the report navigator, making it much easier to diagnose playground build issues.\n\nPlaygrounds are great for documenting packages and frameworks with runnable code. And don't forget that playgrounds are a handy tool for quickly coding up new ideas or writing utility code. I look forward to hearing how playgrounds are able to help you make your packages and projects even better. Thank you for watching.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "8:53",
      "title": "Playgrounds and resources Demo: Part 1",
      "language": "swift",
      "code": "import UIKit\nlet image = UIImage(named: \"ingredient/orange\")"
    },
    {
      "timestamp": "10:18",
      "title": "Playgrounds and resources Demo: Part 2",
      "language": "swift",
      "code": "import CoreML\nlet yoloModel = try YOLOv3(configuration: MLModelConfiguration()).model"
    },
    {
      "timestamp": "10:54",
      "title": "Playgrounds and resources Demo: Part 3",
      "language": "swift",
      "code": "import UIKit\nimport CoreML\nimport Vision\n\nlet ingredientNames = [\n    \"banana\",\n    \"orange\",\n    \"almond-milk\",\n]\n\nlet yoloModel = try YOLOv3(configuration: MLModelConfiguration()).model\nlet model = try VNCoreMLModel(for: yoloModel)\n\nlet request = VNCoreMLRequest(model: model) {_,_ in }"
    },
    {
      "timestamp": "11:24",
      "title": "Recognized Object Visualizer",
      "language": "swift",
      "code": "import Foundation\nimport SwiftUI\nimport UIKit\n\n// MARK: Model\n\n/// The result of object detection on an image.\npublic struct ObjectDetectionResult : Identifiable {\n    public var name: String\n    public var image: UIImage\n    public var id: String\n    public var objects: [RecognizedObject]\n\n    public init(name: String, image: UIImage, id: String, objects: [RecognizedObject]) {\n        self.id = id\n        self.name = name\n        self.image = image\n        self.objects = objects\n    }\n}\n\n/// An object recognized by an image classifier.\npublic struct RecognizedObject : Identifiable {\n    public var id: Int\n    public var label: String\n    public var confidence: Double\n    public var boundingBox: CGRect\n\n    public init(id: Int, label: String, confidence: Double, boundingBox: CGRect) {\n        self.id = id\n        self.label = label\n        self.confidence = confidence\n        self.boundingBox = boundingBox\n    }\n}\n\n// MARK: Views\n\npublic struct RecognizedObjectVisualizer : View {\n    public var results: [ObjectDetectionResult]\n    public var imageSize: CGFloat = 400\n\n    public init(withResults results: [ObjectDetectionResult]) {\n        self.results = results\n    }\n\n    public var body: some View {\n        List(results) { result in\n            Spacer()\n\n            VStack(alignment: .center) {\n                RecognizedObjectsView(\n                    image: result.image,\n                    objects: result.objects\n                )\n                .frame(width: imageSize, height: imageSize)\n\n                Text(result.name.capitalized)\n\n                Spacer(minLength: 20)\n            }\n\n            Spacer()\n        }\n    }\n}\n\nstruct RecognizedObjectsView : View {\n    var image: UIImage\n    var objects: [RecognizedObject]\n\n    var body: some View {\n        GeometryReader { geometry in\n            Image(uiImage: image)\n                .resizable()\n                .overlay(\n                    ZStack {\n                        ForEach(objects) { object in\n                            Rectangle()\n                                .stroke(Color.red)\n                                .shadow(radius: 2.0)\n                                .frame(\n                                    width: object.boundingBox.width * geometry.size.width / image.size.width,\n                                    height: object.boundingBox.height * geometry.size.height / image.size.height\n                                )\n                                .position(\n                                    x: (object.boundingBox.origin.x + object.boundingBox.size.width / 2.0) * geometry.size.width / image.size.width,\n                                    y: geometry.size.height - (object.boundingBox.origin.y + object.boundingBox.size.height / 2.0) * geometry.size.height / image.size.height\n                                )\n                                .overlay(\n                                    Text(\"\\(object.label.capitalized) (\\(String(format: \"%0.0f\", object.confidence * 100.0))%)\")\n                                        .foregroundColor(Color.red)\n                                        .position(\n                                            x: (object.boundingBox.origin.x + object.boundingBox.size.width / 2.0) * geometry.size.width / image.size.width,\n                                            y: geometry.size.height - (object.boundingBox.origin.y - 20.0) * geometry.size.height / image.size.height\n                                        )\n                                )\n                        }\n                    }\n                )\n        }\n    }\n}"
    },
    {
      "timestamp": "11:48",
      "title": "Playgrounds and resources Demo: Part 4",
      "language": "swift",
      "code": "let results = ingredientNames.compactMap { ingredient -> ObjectDetectionResult? in\n\n    guard let image = UIImage(named: \"ingredient/\\(ingredient)\") else { return nil }\n\n    let handler = VNImageRequestHandler(cgImage: image.cgImage!)\n    try? handler.perform([request])\n    let observations = request.results as! [VNRecognizedObjectObservation]\n\n    let detectedObjects = observations.enumerated().map { (index, observation) -> RecognizedObject in\n\n        // Select only the label with the highest confidence.\n        let topLabelObservation = observation.labels[0]\n        let objectBounds = VNImageRectForNormalizedRect(observation.boundingBox, Int(image.size.width), Int(image.size.height))\n\n        return RecognizedObject(id: index, label: topLabelObservation.identifier, confidence: Double(topLabelObservation.confidence), boundingBox: objectBounds)\n    }\n\n    return ObjectDetectionResult(name: ingredient, image: image, id: ingredient, objects: detectedObjects)\n}\n\nresults"
    },
    {
      "timestamp": "12:33",
      "title": "Playgrounds and resources Demo: Part 5",
      "language": "swift",
      "code": "import PlaygroundSupport\n\nPlaygroundPage.current.setLiveView(\n    RecognizedObjectVisualizer(withResults: results)\n        .frame(width: 500, height: 800)\n)"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "Fruta: Building a Feature-Rich App with SwiftUI",
        "url": "https://developer.apple.com/documentation/appclip/fruta_building_a_feature-rich_app_with_swiftui"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10096/4/9D01022D-66DF-4B16-B5B6-B01F57252226/wwdc2020_10096_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10096/3/E26BE90C-7F33-4ADB-9939-505758D60858/wwdc2020_10096_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10250",
      "year": "2023",
      "title": "Prototype with Xcode Playgrounds",
      "url": "https://developer.apple.com/videos/play/wwdc2023/10250"
    },
    {
      "id": "10169",
      "year": "2020",
      "title": "Swift packages: Resources and localization",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10169"
    },
    {
      "id": "424",
      "year": "2019",
      "title": "Training Object Detection Models in Create ML",
      "url": "https://developer.apple.com/videos/play/wwdc2019/424"
    }
  ],
  "extractedAt": "2025-07-18T09:43:46.085Z"
}